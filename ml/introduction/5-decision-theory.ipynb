{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\LN}[1]{\\ln\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\invp}[1]{\\left({#1}\\right)^{-1}}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\ml}[1]{#1_{\\text{ML}}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\mat}[1]{ \\left[ \\begin{matrix} #1 \\end{matrix} \\right] }\n",
    "\\newcommand{\\matp}[1]{ \\left( \\begin{matrix} #1 \\end{matrix} \\right)}\n",
    "\\newcommand{\\mats}[1]{ \\begin{matrix}#1\\end{matrix} }\n",
    "$\n",
    "\n",
    "$\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr} #1 \\end{array}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Ca}{\\mathcal{C}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Ra}{\\mathcal{R}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet bold,\n",
    "$\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\At}{\\Ab^T}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\fb}{\\mathbf{f}}\n",
    "\\newcommand{\\fp}{f^{\\prime}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\Kb}{\\mathbf{K}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lt}{\\Lb^T}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "\\newcommand{\\Ut}{\\Ub^T}\n",
    "\\newcommand{\\vb}{\\mathbf{v}}\n",
    "\\newcommand{\\Vb}{\\mathbf{V}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wt}{\\wb^T}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xt}{\\xb^T}\n",
    "\\newcommand{\\xp}{x^{\\prime}}\n",
    "\\newcommand{\\xbp}{\\xb^{\\prime}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zt}{\\zb^T}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math bold\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\zerob}{\\pmb{0}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the misclassification rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2d case, \n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\text{mistake})\n",
    "&=\n",
    "p(\\xb \\in \\Ra_1, \\Ca_2)\n",
    "+\n",
    "p(\\xb \\in \\Ra_2, \\Ca_1)\n",
    "\\\\\n",
    "&=\n",
    "\\displaystyle \\int_{\\Ra_1} p(\\xb, \\Ca_2) ~d\\xb +\n",
    "\\displaystyle \\int_{\\Ra_2} p(\\xb, \\Ca_1) ~d\\xb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to min this loss, we have to use this rule\n",
    "$$\n",
    "t =\n",
    "\\cases{\n",
    "\\Ca_1 & \\text{if } p(\\xb,\\Ca_1) > p(\\xb,\\Ca_2)\\\\\n",
    "\\Ca_2 & \\text{otherwise}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $p(\\xb,\\Ca_k) = p(\\Ca_k \\mid \\xb) ~p(\\xb)$, the above rule becomes\n",
    "$$\n",
    "t =\n",
    "\\cases{\n",
    "\\Ca_1 & \\text{if } p(\\Ca_1 \\mid \\xb) > p(\\Ca_2 \\mid \\xb)\\\\\n",
    "\\Ca_2 & \\text{otherwise}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in general to min the loss, \n",
    "$$\n",
    "t = \\argmax_{k} p(\\Ca_k \\mid \\xb)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the expected loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\L = \\matp{0 & 1000 \\\\ 1 & 0}\n",
    "$$\n",
    "\n",
    "0 -> cancer  \n",
    "1 -> normal\n",
    "\n",
    "$\\L_{kj}$ : true class is k and we assign j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected loss is given by\n",
    "$$\n",
    "\\E{\\L} = \\sum_k \\sum_j \\int_{\\Ra_j} \\L_{kj} ~p(\\xb, \\Ca_k) ~d\\xb\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reject Option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the output if the posterior is less than a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two stages to everything or three in case of life\n",
    "1. inference stage: use training data to model the posterior prob $p(\\Ca_k \\mid \\xb)$\n",
    "1. decision stage: use these posteriors to make optimal class assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt way  \n",
    "Solve both problems together and find a function that maps input **x** directly to decisions. such functions are called discriminant functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Generative*\n",
    "Solve the inference problem of determining the class-conditional densities $p(\\xb \\mid \\Ca_k)$ for each $\\Ca_k$ individually.\n",
    "\n",
    "separately infer the prior probabilities $p(\\Ca_k)$\n",
    "\n",
    "Then use Bayes' theorem in the form\n",
    "$$\n",
    "p(\\Ca_k \\mid \\xb)\n",
    "=\n",
    "\\frac\n",
    "{p(\\xb \\mid \\Ca_k) ~p(\\Ca_k)}\n",
    "{p(\\xb)}\n",
    "=\n",
    "\\frac\n",
    "{p(\\xb \\mid \\Ca_k) ~p(\\Ca_k)}\n",
    "{\\displaystyle \\sum_k p(\\xb \\mid \\Ca_k) ~p(\\Ca_k)}\n",
    "$$\n",
    "\n",
    "Having found the posterior, we can use it to determine class memberships. such models are called generative models which model the distribution of the inpt as well as the outputs\n",
    "\n",
    "#### Caveats\n",
    "* Needs large training set to compute class-conditional densities to a decent accuracy\n",
    "* can be used to detect outliers \\citeme{Bishop,\n",
    "1994; Tarassenko, 1995}\n",
    "* could be waste since we need only posterior and all that extra information might not be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Discriminative models*  \n",
    "solve the inference problem to find $p(\\Ca_k \\mid \\xb)$ directly skipping the likelihood $p(\\xb \\mid \\Ca_k)$ computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Shitty models* aka *Discriminant functions*\n",
    "\n",
    "find f(**x**) which maps **x** directly onto a class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) vs (a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Risk\n",
    "If the loss matrix keeps changing, trivial to update. (c) would do the same shit all over fucking again\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reject option\n",
    "(a,b) can afford to, but in no way (c) can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compensating for class priors\n",
    "\n",
    "if we have a skewed data set, then the trivial solution of assigning everything to the majority class cant be avoided.\n",
    "\n",
    "need to have a balanced data set with equal number of data points in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Models ??\n",
    "\n",
    "two different features, say, blood tests as well as xray.\n",
    "\n",
    "predictions of these two can be combined easitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\E{\\L} &=\n",
    "\\displaystyle \\iint \n",
    "\\L(t, y(\\xb)) ~p(\\xb, t) ~d\\xb ~dt\n",
    "\\\\\n",
    "&=\n",
    "\\displaystyle \\iint \n",
    "\\left\\{\n",
    "  y(\\xb) - t\n",
    "\\right\\}^2\n",
    "~p(\\xb, t)~d\\xb ~dt\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac\n",
    "{\\partial \\E{\\L}}\n",
    "{\\partial y(\\xb)}\n",
    "=\n",
    "2 \\displaystyle \\int\n",
    "\\left\\{\n",
    "  y(\\xb) - t\n",
    "\\right\\}\n",
    "~p(\\xb, t) ~dt\n",
    "= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "$$\n",
    "y(\\xb) = \n",
    "\\frac\n",
    "{\\int t ~p(\\xb, t) ~dt}\n",
    "{p(\\xb)}\n",
    "=\n",
    "\\displaystyle \\int\n",
    "t ~p(t \\mid \\xb) ~dt\n",
    "=\n",
    "\\mathbb{E}_t[t \\mid \\xb]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the conditional average of t conditioned on **x** is called the regression function. for multivariate case\n",
    "$$\n",
    "y(\\xb) = \\mathbb{E}_{\\tb}[\\tb \\mid \\xb]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving this in a different way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{\n",
    "  y(\\xb) - t\n",
    "\\right\\}^2\n",
    "=\n",
    "\\left\\{\n",
    "  y(\\xb) - \\E{t \\mid \\xb} + \\E{t \\mid \\xb} - t\n",
    "\\right\\}^2\n",
    "\\\\ \n",
    "=\n",
    "\\left\\{\n",
    "  y(\\xb) - \\E{t \\mid \\xb}\n",
    "\\right\\}^2\n",
    "+ 2\n",
    "\\left\\{ y(\\xb) - \\E{t \\mid \\xb} \\right\\}\n",
    "\\left\\{ \\E{t \\mid \\xb} - t \\right\\}\n",
    "+\n",
    "\\left\\{\n",
    "  \\E{t \\mid \\xb} - t\n",
    "\\right\\}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub this into the loss function and int over t, the second subterm of the second term becomes $\\E{t} - \\E{t}$. Thus the second term gets the fuck lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,\n",
    "$$\n",
    "\\arrthree{\n",
    "\\E{\\L}\n",
    "&=\n",
    "\\displaystyle \\int\n",
    "\\left\\{\n",
    "  y(\\xb) - \\E{t \\mid \\xb}\n",
    "\\right\\}^2\n",
    "~p(\\xb) ~d\\xb\n",
    "+\n",
    "\\displaystyle \\int\n",
    "\\left\\{\n",
    "  \\E{t \\mid \\xb} - t\n",
    "\\right\\}^2\n",
    "~p(\\xb) ~d\\xb\n",
    "\\\\\n",
    "&=\n",
    "\\displaystyle \\int\n",
    "\\left\\{\n",
    "  y(\\xb) - \\E{t \\mid \\xb}\n",
    "\\right\\}^2\n",
    "~p(\\xb) ~d\\xb\n",
    "+\n",
    "\\underbrace{\n",
    "\\displaystyle \\int\n",
    "\\V{t \\mid \\xb}\n",
    "~p(\\xb) ~d\\xb\n",
    "}_{\\text{irreducible minimum}}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function y(**x**) is present only in the first term. this vanishes when $y(\\xb) = \\E{t \\mid \\xb}$, which is what we fucking got "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second term  \n",
    "* variance of t averaged over **x**.  \n",
    "* represents the intrinsic variability of target data\n",
    "* since it is independent of y(**x**), represends the irreducible minimum value of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other loss functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Minkowski Loss*\n",
    "$$\n",
    "\\E{\\L_q}\n",
    "= \\displaystyle \\iint\n",
    "\\Norm{y(\\xb) - t}^q ~p(\\xb,t) ~d\\xb ~dt\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
