{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization\n",
    "$\n",
    "\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\Bracket}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\cases}[1]{\\begin{cases}#1\\end{cases}}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\diff}[2]{\\frac{d #1}{d #2}}\n",
    "\\newcommand{\\difftwo}[2]{\\frac{d^2 #1}{d {#2}^2}}\n",
    "\\newcommand{\\diffn}[2]{{#1}^{\\prime}(#2)}\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\expb}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\frachalf}[1]{\\frac{#1}{2}~}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}~}\n",
    "\\newcommand{\\fracrec}[1]{\\frac{1}{#1}~}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\H}[1]{\\mathbb{H}\\left[#1\\right]}\n",
    "\\newcommand{\\Int}[2]{\\displaystyle \\int_{#1}^{#2}~}\n",
    "\\newcommand{\\intinfinf}{\\Int{-\\infty}{\\infty}}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\invp}[1]{\\left({#1}\\right)^{-1}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left(#1 \\Vert #2\\right)}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Ln}[1]{\\ln \\left\\(#1\\right\\)}\n",
    "\\newcommand{\\Lnb}[1]{\\ln \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\Mod}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\newcommand{\\Normsqr}[1]{\\Norm{#1}^2}\n",
    "\\newcommand{\\map}[1]{#1_{\\text{MAP}}}\n",
    "\\newcommand{\\ml}[1]{#1_{\\text{ML}}}\n",
    "\\newcommand{\\MI}[1]{\\mathcal{I}\\left(#1\\right)}\n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\Paran}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\Partial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\sqrbrkt}[1]{\\Bracket{#1}^2}\n",
    "\\newcommand{\\sqrbrc}[1]{\\Brace{#1}^2}\n",
    "\\newcommand{\\trace}[1]{\\text{Tr}\\left( #1 \\right)}\n",
    "\\newcommand{\\traceb}[1]{\\text{Tr}\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "$\n",
    "$\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\mat}[1]{ \\left[ \\begin{matrix} #1 \\end{matrix} \\right] }\n",
    "\\newcommand{\\matp}[1]{ \\left( \\begin{matrix} #1 \\end{matrix} \\right)}\n",
    "\\newcommand{\\mats}[1]{ \\begin{matrix}#1\\end{matrix} }\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr} #1 \\end{array}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Ca}{\\mathcal{C}}\n",
    "\\newcommand{\\Caone}{\\Ca_1}\n",
    "\\newcommand{\\Catwo}{\\Ca_2}\n",
    "\\newcommand{\\Cak}{\\Ca_k}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Ra}{\\mathcal{R}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumi}{\\displaystyle \\sum_i}\n",
    "\\newcommand{\\sumiD}{\\displaystyle \\sum_{i=1}^{D}}\n",
    "\\newcommand{\\sumiL}{\\displaystyle \\sum_{i=1}^{L}}\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodi}{\\displaystyle \\prod_i}\n",
    "\\newcommand{\\prodiD}{\\displaystyle \\prod_{i=1}^{D}}\n",
    "\\newcommand{\\prodiL}{\\displaystyle \\prod_{i=1}^{L}}\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet shortcuts\n",
    "$\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\at}{\\ab^T}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\At}{\\Ab^T}\n",
    "\\newcommand{\\Ai}{\\inv{\\Ab}}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\bt}{\\bb^T}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\fb}{\\mathbf{f}}\n",
    "\\newcommand{\\fp}{f^{\\prime}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\hx}{h(\\xb)}\n",
    "\\newcommand{\\Jb}{\\mathbf{J}}\n",
    "\\newcommand{\\kb}{\\mathbf{k}}\n",
    "\\newcommand{\\Kb}{\\mathbf{K}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lt}{\\Lb^T}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\mt}{\\mb^T}\n",
    "\\newcommand{\\mbn}{\\mb_N}\n",
    "\\newcommand{\\mbnt}{\\mbn^T}\n",
    "\\newcommand{\\mbN}{\\mb_N}\n",
    "\\newcommand{\\mbNt}{\\mbn^T}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\sb}{\\mathbf{s}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\tt}{\\tb^T}\n",
    "\\newcommand{\\Tb}{\\mathbf{T}}\n",
    "\\newcommand{\\Tt}{\\Tb^T}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "\\newcommand{\\Ut}{\\Ub^T}\n",
    "\\newcommand{\\vb}{\\mathbf{v}}\n",
    "\\newcommand{\\Vb}{\\mathbf{V}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wnr}[1]{\\wb^{(\\text{#1})}}\n",
    "\\newcommand{\\wt}{\\wb^T}\n",
    "\\newcommand{\\Wb}{\\mathbf{W}}\n",
    "\\newcommand{\\Wt}{\\Wb^T}\n",
    "\\newcommand{\\Wtilde}{\\widetilde{\\Wb}}\n",
    "\\newcommand{\\Wtildet}{\\Wtilde^T}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\Xtilde}{\\widetilde{\\Xb}}\n",
    "\\newcommand{\\Xtildet}{\\Xtilde^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xt}{\\xb^T}\n",
    "\\newcommand{\\xtilde}{\\widetilde{\\xb}}\n",
    "\\newcommand{\\xtilden}{\\xtilde_n}\n",
    "\\newcommand{\\xtildent}{\\xtilden^T}\n",
    "\\newcommand{\\xp}{x^{\\prime}}\n",
    "\\newcommand{\\xbp}{\\xb^{\\prime}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\yx}{y(\\xb)}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zt}{\\zb^T}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math shortcuts\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\Lambdab}{\\pmb{\\Lambda}}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\Li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\mut}{\\mub^T}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "\\newcommand{\\Phibt}{\\Phib^T}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\sigmasqr}{\\sigma^2}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\Sigmai}{\\inv{\\Sigma}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\zerob}{\\pmb{0}}\n",
    "\\newcommand{\\ed}{\\mathbb{E}_{\\D}}\n",
    "\\newcommand{\\edyx}{\\ed\\left[y(\\xb ; \\D)\\right]}\n",
    "\\newcommand{\\dx}{~dx}\n",
    "\\newcommand{\\dxb}{~d\\xb}\n",
    "\\newcommand{\\pxdxb}{p(\\xb) \\dxb}\n",
    "\\newcommand{\\dwb}{~d\\wb}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one of the choices for the basis functions for linear regression models would be radial basis, given by\n",
    "$$\n",
    "\\phi_j(\\xb) = h(\\Norm{\\xb - \\mu_j})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why rbf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exact function interpolation\n",
    "\n",
    "\\citeme{Powell, 1987}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Green's functions\n",
    "\n",
    "\\citeme{Poggio and Girosi, 1990}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy input variables\n",
    "\n",
    "\\citeme{Webb, 1994}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basis function for each data point which becomes costly while making predictions.\n",
    "\n",
    "\\citeme{Broomhead and Lowe, 1988; Moody and Darken, 1989; Poggio and Girosi, 1990}  \n",
    "Models which retain the expansion in rbf's but where number of basis functions  < number of data points\n",
    "\n",
    "The number of basis functions and the locations of their centers are determined just from the data set. the basis functions are then kept fixed and the coefficients $\\wb_i$ are determined using least squares using the normal, time-tested boring way as was discussed in linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orthogonal least squares\n",
    "\n",
    "\\citeme{Che et. al, 1991}\n",
    "\n",
    "A sequential selection process in which at each step the next data point to be chosen as a basis function centre corresponds to the one that gives the greatest reduction in the sum-of-squares error.\n",
    "\n",
    "Values for the expansion coefficients are determined as part of the algorithm. Clustering algorithms such as K-means have also been used, which give a set of basis function centres that no longer coincide with training data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nadaraya-Watson model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe prediction of a linear regression model for a new input x takes the form of a linear combination of the training set target values with coefficients given by the equivalent kernel satisfies the summation constraint\n",
    "\n",
    "[here](../linear-models-for-regression/bayesian-linear-regression.ipynb#Equivalent-Kernel)\n",
    "$$\n",
    "y(\\xb,\\mbN) = \\sumnN k(\\xb,\\xb_n) t_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets consider kernel density estimation. say we have a training set $\\Brace{\\xb_n, t_n}$.  \n",
    "\n",
    "lets use a Parzen density estimator to model the joint distribution $p(\\xb, t)$ so that\n",
    "$$\n",
    "p(\\xb, t) = \\fracrec{N} \\sumnN f(\\xb - \\xbn, t-t_n)\n",
    "$$\n",
    "\n",
    "Here f is the component density function, one centered at each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal:  \n",
    "to find an expression for regression function y(**x**), corresponding to the conditional average of the target variable conditioned on the input variable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "y(\\xb) &=\n",
    "\\E{t \\mid x} = \\intinfinf t ~p(t \\mid \\xb) ~dt\n",
    "\\\\ &=\n",
    "\\intinfinf t ~\\frac{p(t, \\xb)}{p(\\xb)} ~dt\n",
    "\\\\ &=\n",
    "\\frac\n",
    "{\\int t ~p(\\xb, t) ~dt}\n",
    "{\\int p(\\xb, t) ~dt}\n",
    "\\\\ &=\n",
    "\\frac\n",
    "{\\sum_n \\int t ~f(\\xb - \\xbn, t - t_n) ~dt}\n",
    "{\\sum_n \\int ~f(\\xb - \\xbn, t - t_n) ~dt}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define\n",
    "$$\n",
    "g(\\xb) = \\intinfinf f(\\xb, t) ~dt\n",
    "$$\n",
    "\n",
    "if $\\int ~t ~f(\\xb , t) ~dt = 0$, then we have\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\text{numerator} &=\n",
    "\\intinfinf ~(t-t_n) ~f(\\xb, t - t_n) ~d(t-t_n)\n",
    "+ t_n \\intinfinf ~f(\\xb, t) ~d(t)\n",
    "\\\\ &=\n",
    "\\intinfinf ~t^{\\prime} ~f(\\xb, t^{\\prime}) ~dt^{\\prime}\n",
    "+ t_n g(\\xb)\n",
    "\\\\ &=\n",
    "t_n g(\\xb)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus\n",
    "$$\n",
    "\\arrthree{\n",
    "y(\\xb) &=\n",
    "\\frac\n",
    "{\\sum_n t_n ~g(\\xb - \\xbn)}\n",
    "{\\sum_m ~g(\\xb - \\xbm)}\n",
    "\\\\ &=\n",
    "\\sum_n t_n \\kappa(\\xb, \\xbn)\n",
    "\\\\ \\text{where }\n",
    "\\kappa(\\xb, \\xbn)\n",
    "&=\n",
    "\\frac{~g(\\xb - \\xbn)}\n",
    "{\\sum_m ~g(\\xb - \\xbm)}\n",
    "& \\commentgray{kernel function}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the Nadaraya-Watson model or kernel regression  \n",
    "\\citeme{Nadaraya, 1964; Watson, 1964}\n",
    "\n",
    "For a localized kernel function, it has the property of giving more weight to the data points $\\xbn$ that are close to **x**. \n",
    "\n",
    "the kernel satisfies the summation constraint\n",
    "$$\n",
    "\\sumnN \\kappa(\\xb, \\xbn) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codu machi"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(in_pts=20, sigma_noise=1, plot_vals=True):\n",
    "    x = np.random.rand(in_pts).reshape(-1,1)*2*math.pi\n",
    "    x.sort()\n",
    "    y_noise = np.random.normal(0,sigma_noise,size=(in_pts,1))\n",
    "    y = np.sin(x).reshape(-1,1) + y_noise\n",
    "    if plot_vals:\n",
    "        plt.plot(x, y, '.b', label='gt', MarkerSize=8)\n",
    "    return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sin():\n",
    "    x = np.linspace(0,2*math.pi, 200)\n",
    "    plt.plot(x, np.sin(x), 'g', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "rbf(\\xb,\\xbn) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\expb{-\\frac{\\Norm{\\xb-\\xbn}^2}{2\\sigma^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rbf_val(x, x_n, sigma):\n",
    "    factor = math.sqrt(2*math.pi)*sigma\n",
    "    exponent = (x-x_n)**2/(2*sigma**2)\n",
    "    return math.e**(-exponent)/factor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y(\\xb) =\n",
    "\\frac\n",
    "{\\sum_n t_n ~g(\\xb - \\xbn)}\n",
    "{\\sum_m ~g(\\xb - \\xbm)}\n",
    "=\n",
    "\\frac{\\sum_n t_n ~rbf(\\xb, \\xbn)}{\\sum_m ~rbf(\\xb, \\xbm}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, T, x, sigma=1):\n",
    "    nr, dr = 0.0, 0.0\n",
    "    N = T.shape[0]\n",
    "    for ix in range(N):\n",
    "        xn, tn = X[ix,0], T[ix, 0]\n",
    "        g_x_xn = compute_rbf_val(x, xn, sigma)\n",
    "        nr += tn*g_x_xn\n",
    "        dr += g_x_xn\n",
    "    return nr/dr"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(X, T, x, sigma=1.):\n",
    "    factor = sigma*math.sqrt(2*math.pi)\n",
    "    g_X_xn = math.e**(-(X-x)**2 / (2*sigma**2))/factor\n",
    "    return np.sum(T * g_X_xn,0) / np.sum(g_X_xn,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\Xbp}{\\Xb^{\\prime}}\n",
    "\\newcommand{\\Tbp}{\\Tb^{\\prime}}\n",
    "\\newcommand{\\Xbp}{\\Xb_{\\text{test}}}\n",
    "\\newcommand{\\Tbp}{\\Tb_{\\text{test}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\Xb &&: N \\times 1 \\\\\n",
    "\\Xbp &&: NN \\times 1 \\\\\n",
    "\\Xb - \\Xbp^T  &\n",
    "\\mat{\n",
    "\\xb_1- \\Xbp^{(1)} & \\cdots & \\xb_1- \\Xbp^{(NN)} \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\xb_N- \\Xbp^{(1)} & \\cdots & \\xb_N- \\Xbp^{(NN)} \n",
    "}\n",
    "&: N \\times NN \\\\ \\\\\n",
    "\\text{Hence g_X_Xtest} &\n",
    "\\mat{\n",
    "\\kappa(\\xb_1, \\Xbp^{(1)}) & \\cdots & \\kappa(\\xb_1,\\Xbp^{(NN)}) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\kappa(\\xb_N, \\Xbp^{(1)}) & \\cdots & \\kappa(\\xb_N,\\Xbp^{(NN)}) \\\\\n",
    "}\n",
    "&: N \\times NN\\\\ \\\\\n",
    "\\text{g_X_Xtest * }\\Tb &\n",
    "\\mat{\n",
    "\\tb_1 \\kappa(\\xb_1, \\Xbp^{(1)}) & \\cdots & \\tb_1 \\kappa(\\xb_1,\\Xbp^{(NN)}) \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "\\tb_N \\kappa(\\xb_N, \\Xbp^{(1)}) & \\cdots & \\tb_N \\kappa(\\xb_N,\\Xbp^{(NN)})\\\\\n",
    "}\n",
    "&: N \\times NN\\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict3(X, T, X_test, sigma=1.):\n",
    "    factor = sigma*math.sqrt(2*math.pi)\n",
    "    g_X_Xtest = math.e**(-(X-X_test.T)**2 / (2*sigma**2))/factor\n",
    "    exp_t = (np.sum( g_X_Xtest * T,0) / np.sum(g_X_Xtest,0)).reshape(-1,1)\n",
    "    exp_tsqr = (np.sum( g_X_Xtest * T**2,0) / np.sum(g_X_Xtest,0)).reshape(-1,1)\n",
    "    mean = exp_t\n",
    "    var = exp_tsqr - exp_t**2\n",
    "    return (mean, var)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_covar(x, mu, varr):\n",
    "    n = x.shape[0]\n",
    "    for ix in range(n):\n",
    "        xx = x[ix,0]\n",
    "        m, v = mu[ix,0], varr[ix,0]\n",
    "        plt.plot((xx,xx),(m-v,m+v), color=(248/255., 163/255., 211/255.))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(in_pts=50, sigma_noise=0.15, sigma_rbf=1.0):\n",
    "    X, T = get_samples(in_pts, sigma_noise)\n",
    "    plot_sin()\n",
    "\n",
    "    x = np.linspace(0,2*math.pi,200).reshape(-1,1)\n",
    "    #y = [predict3(X, T, xx) for xx in x]\n",
    "    y_mean, y_var = predict3(X, T, x, sigma_rbf)\n",
    "    plt.plot(x, y_mean, 'r', linewidth=3, label='predictedm')\n",
    "    plot_covar(x, y_mean, y_var)\n",
    "\n",
    "    plt.legend(loc=(1,0.5))\n",
    "    plt.show()\n",
    "interact(plot_prediction, in_pts=(10,200), sigma_noise=(0.0, 3.0, 0.1), sigma_rbf=(0.0,3.0,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sigma of rbf allows us to generalize well, at the cost of increases bias.   \n",
    "the mean of the prediction would be way off target as we increase sigma but the generalization is very good"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
