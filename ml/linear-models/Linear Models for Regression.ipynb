{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$ \\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}$  \n",
    "$ \\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}$\n",
    "$ \\newcommand{\\EXP}[1]{\\exp\\left(#1\\right)}$  \n",
    "$ \\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\n",
    "\\newcommand{\\sumnN}{\\sum_{n=1}^{N}}\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr}\n",
    "#1\n",
    "\\end{array}\n",
    "}\n",
    "\\newcommand{\\mat}[1]{\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "#1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\dmc}{\\mathcal{D}}\n",
    "\\newcommand{\\norm}[1]{\\|#1\\|}\n",
    "\\newcommand{\\normsqr}[1]{\\norm{#1}^2}\n",
    "\\newcommand{\\frachalf}[1]{\\frac{#1}{2}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\Abt}{\\Ab^T}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Ib}{\\mathbf{I}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wbt}{\\wb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\hx}{h(\\xb)}\n",
    "\\newcommand{\\yx}{y(\\xb; \\mathcal{D})}\n",
    "\\newcommand{\\ed}[1]{\\mathbb{E}_D\\left[ #1 \\right]}\n",
    "\\newcommand{\\edyx}{\\ed{\\yx}}\n",
    "\\newcommand{\\px}{~p(\\xb)}\n",
    "\\newcommand{\\dx}{~d\\xb}\n",
    "\\newcommand{\\pxdx}{\\px \\dx}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**  \n",
    "1. Use different basis models for the same problem\n",
    "1. Compare w/ and w/o regularization\n",
    "1. Use different regularizers for the same problem\n",
    "1. Bias Variance Decomposition + Experiments\n",
    "\n",
    "**Questions**\n",
    "1. Equation \\ref{eq:ptw} under ML and Least Squares\n",
    "1. Why does Lasso act as a feature selector?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "=======\n",
    "\n",
    "Simple Linear Model\n",
    "------------------\n",
    "\n",
    "y(\\textbf{x},**w**) $ = w_0 + \\sum_{i=1}^{D} w_i x_i$ \n",
    "\n",
    "where  \n",
    "**x** $ = (x_1, \\cdots, x_D)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basis Functions\n",
    "---------------\n",
    "\n",
    "$\n",
    "y(\\mathbf{x},\\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_i ~ \\phi_j(\\mathbf{x})\n",
    "$ \n",
    "\n",
    "* where $\\phi_j(\\mathbf{x})$ are called basis functions. \n",
    "* Total #parameters = M\n",
    "\n",
    "If $\\phi_0(\\mathbf{x}) = 1$, then  \n",
    "$y(\\mathbf{x},\\mathbf{w})  = \\sum_{j=0}^{M-1} w_i ~ \\phi_j(\\text{x}) = \\mathbf{w}^T \\phi(\\mathbf{x})$ \n",
    "\n",
    "* Linear in **w**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choices of Basis Functions\n",
    "-------------------------\n",
    "\n",
    "1. Polynomial Basis\n",
    "  * $\\phi_j(\\mathbf{x}) = x^j$\n",
    "  * Limitation: Global models\n",
    "  * Cure: Spline Functions: fit different polynomials based on region [EOSL Hastie]\n",
    "1. Gaussian Basis FUnction:\n",
    "  * $\\phi_j(\\mathbf{x}) = \\EXP{-\\frac{(x-\\mu_j)^2}{2s^2}}$\n",
    "  * Need not be a pdf\n",
    "1. Sigmoidal\n",
    "  * $\\phi_j(\\mathbf{x}) = \\sigma \\left( \\frac{x-\\mu_j}{s} \\right)$\n",
    "  * where $\\sigma(a) = \\frac{1}{1+\\EXP{-a}}$ is the logistic sigmoid function\n",
    "  * Can use $\\tanh$, since $\\tanh(a) = 2\\sigma(2a) - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions for Regression\n",
    "=================\n",
    "\n",
    "[PRML]1.5.5\n",
    "\n",
    "Say we fit a function y(x) to give the target variable t. \n",
    "Let $\\mathcal{L}(t, y(x))$ denote the loss function.\n",
    "\n",
    "Then, Average or Expected Loss is given by,\n",
    "$\\E{\\mathcal{L}} = \\iint \\mathcal{L}(t, y(x)) ~p(x,t) ~dx ~dt$\n",
    "\n",
    "If we consider the squared loss function $\\mathcal{L}(t, y(\\mathbf{x})) = \\left( t - y(\\mathbf{x})\\right)^2$, then,\n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\E{\\mathcal{L}} \n",
    "&= \\iint \\left( t - y(\\mathbf{x})\\right)^2 ~p(\\mathbf{x},t) ~d\\mathbf{x} ~dt\n",
    "\\\\\n",
    "\\dfrac{\\partial \\E{\\mathcal{L}}}{\\partial y(\\mathbf{x})}\n",
    "&=\n",
    "2 \\int \\left( t - y(\\mathbf{x})\\right) ~p(\\mathbf{x},t) ~dt = 0\n",
    "\\\\\n",
    "\\int t ~p(\\mathbf{x},t) ~dt  \n",
    "&=\n",
    "\\int y(\\mathbf{x}) ~p(\\mathbf{x},t) ~dt = y(\\mathbf{x}) \\int  ~p(\\mathbf{x},t) ~dt\n",
    "\\\\\n",
    "\\int t ~p(\\mathbf{x},t) ~dt  \n",
    "&=\n",
    "y(\\mathbf{x}) p(\\mathbf{x})\n",
    "\\\\\n",
    "y(\\mathbf{x})\n",
    "&= \n",
    "\\dfrac{\\int t ~p(\\mathbf{x},t) ~dt  }{p(\\mathbf{x})}\n",
    "= \\int t ~p(t \\mid \\mathbf{x}) ~dt\n",
    "\\\\\n",
    "y(\\mathbf{x}) \n",
    "&= \\mathbb{E}_t \\left[ t \\mid \\mathbf{x} \\right]\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\color{green}{\n",
    "  \\text{ Thus for a squared loss function, the optimal prediction \n",
    "  is given by the conditional mean of the target variable}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt. Derivation   \n",
    "\n",
    "$t = \\E{\\E{t \\mid \\mathbf{x}}}$\n",
    "hence, $\\E{\\E{t \\mid \\mathbf{x}} - t}^2 = \\V{t \\mid \\mathbf{x}}$ \n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\left( y(\\mathbf{x}) - t \\right)^2\n",
    "&=\n",
    "\\left( y(\\mathbf{x}) - \\E{t \\mid X} + \\E{t \\mid \\mathbf{x}} - t \\right)^2\n",
    "\\\\\n",
    "&=\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}} \n",
    "\\right)^2 \n",
    "+\n",
    "2\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}}\n",
    "\\right)\n",
    "\\left(\n",
    "    \\E{t \\mid \\mathbf{x}} - t\n",
    "\\right)\n",
    "+\n",
    "\\left(\n",
    "    \\E{t \\mid \\mathbf{x}} - t\n",
    "\\right)^2\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting into $\\E{\\mathcal{L}}$, the cross term vanishes (the first term of the cross term). Hence  \n",
    "$$\n",
    "\\E{\\mathcal{L}} =\n",
    "\\underbrace{\\int\n",
    "\\left(\n",
    "    y(\\mathbf{x}) - \\E{t \\mid \\mathbf{x}} \n",
    "\\right)^2 ~p(\\mathbf{x}) ~d\\mathbf{x}\n",
    "}_{\\text{Bias Term}}\n",
    "-\n",
    "\\underbrace{\n",
    "\\int\n",
    "\\V{t \\mid \\mathbf{x}} ~p(\\mathbf{x}) ~d\\mathbf{x}\n",
    "}_{\\text{Variance term}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML and Least Squares\n",
    "===========\n",
    "\n",
    "Let *t* be given by a deterministic *y* and additive Gaussian noise as,\n",
    "$t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon$ where $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$  \n",
    "Then,  \n",
    "<div id='GaussianIidLikelihood'/>\n",
    "$$\n",
    "p(t \\mid \\xb, \\wb, \\beta) = \\mathcal{N}(t \\mid y(\\xb,\\wb), \\beta^{-1})\n",
    "=\n",
    "\\left( \\dfrac{\\beta}{2\\pi} \\right)^{1/2}\n",
    "\\exp\n",
    "\\left\\{\n",
    "-\\dfrac{\\beta}{2} (t - \\wbt \\phi(\\xb))^2\n",
    "\\right\\}\n",
    "\\label{eq:ptw}\n",
    "$$\n",
    "[?]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a squared loss function, optimal t is given by conditional mean of t. In case of a Gaussian, it becomes\n",
    "$\\E{t \\mid \\mathbf{x}} = \\int t ~p(t \\mid \\mathbf{x}) ~dt = y(\\mathbf{x},\\mathbf{w})$\n",
    "Refer: <a href=\"#Loss-Functions-for-Regression\">Loss Functions for Regression</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $\\mathbf{x}$ is IID Normal given by $\\ref{eq:ptw}$, then  \n",
    "$$\n",
    "p(\\mathbf{t} \\mid \\mathbf{X}, \\mathbf{w}, \\beta)\n",
    "=\n",
    "\\prod_{n=1}^{N}\n",
    "\\mathcal{N}\n",
    "    \\left(\n",
    "        t_n \\mid \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_n}), \\beta^{-1}\n",
    "    \\right)\n",
    "\\label{eq:ptiid}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log likehood, we get  \n",
    "\\begin{array}{ll}\n",
    "\\ln p(\\mathbf{t} \\mid \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\frac{N}{2} \\ln \\beta\n",
    "-\n",
    "\\frac{N}{2} \\ln(2\\pi)\n",
    "-\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter Estimation\n",
    "-------------------\n",
    "Grad of the log likehood (wrt $\\mathbf{w}$) gives,  \n",
    "\\begin{array}{ll}\n",
    "\\nabla \\ln p(\\mathbf{t} | \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\beta\n",
    "\\sum_{n=1}^{N}\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "= 0\n",
    "\\\\\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "&= 0\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{\\phi}\n",
    "=\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        \\phi_1\\\\\n",
    "        \\phi_2\\\\\n",
    "        \\vdots\\\\\n",
    "        \\phi_{M-1}\n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "\\hspace{5pt}\n",
    "\\mathbf{t}\n",
    "=\n",
    "\\left[\n",
    "    \\begin{matrix}\n",
    "        t_1\\\\\n",
    "        t_2\\\\\n",
    "        \\vdots\\\\\n",
    "        t_{N}\n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "\\hspace{20pt}\n",
    "\\mathbf{\\phi} \\mathbf{\\phi}^T\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\phi_0 \\phi_0 & \\phi_0 \\phi_1 & \\cdots & \\phi_0 \\phi_{M-1}\\\\\n",
    "\\phi_1 \\phi_0 & \\phi_1 \\phi_1 & \\cdots & \\phi_1 \\phi_{M-1}\\\\\n",
    "\\vdots        & \\vdots        & \\ddots       & \\vdots\\\\\n",
    "\\phi_{M-1} \\phi_0 & \\phi_{M-1} \\phi_1 & \\cdots & \\phi_{M-1} \\phi_{M-1}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let\n",
    "$$\n",
    "\\Phi(\\mathbf{X})\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\phi_0(x_1) & \\phi_1(x_1) & \\cdots & \\phi_{M-1}(x_1)\\\\\n",
    "\\phi_0(x_2) & \\phi_1(x_2) & \\cdots & \\phi_{M-1}(x_2)\\\\\n",
    "\\vdots      & \\vdots      & \\ddots & \\vdots\\\\\n",
    "\\phi_0(x_N) & \\phi_1(x_N) & \\cdots & \\phi_{M-1}(x_N)\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) ~ \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "=\n",
    "\\Phi(\\mathbf{X})^T \\Phi(\\mathbf{X})\n",
    "\\\\\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "=\n",
    "\\Phi(\\mathbf{X})^T \\mathbf{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$\n",
    "\\mathbf{w}_{ML} = \\left(\\Phi^T \\Phi \\right)^{-1} \\Phi^T \\mathbf{t}\n",
    "$\n",
    "which is called *normal equations* for least squares.  \n",
    "$\\Phi$ is called the *design matrix* which is $N \\times M$ matrix.  \n",
    "Each row of $\\Phi$ is a feature vector transposed $\\phi(x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Parameter\n",
    "---------------\n",
    "\n",
    "Now, consider the last term and seperate out the bias parameter\n",
    "$\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\right)^2$  \n",
    "Diff wrt $w_0$ and equating it to zero, we get  \n",
    "\n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "2(t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n))\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n\n",
    "-\n",
    "\\sum_{n=1}^{N}\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n\n",
    "-\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m \n",
    "\\sum_{n=1}^{N}\n",
    "\\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "w_0\n",
    "&=\n",
    "\\overline{t}_n\n",
    "-\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m\n",
    "\\overline{\\phi}_m\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\overline{t}_n &= \\frac{1}{N} \\sum_{n=1}^{N} t_n \\\\\n",
    "\\overline{\\phi}_m &= \\frac{1}{N} \\sum_{n=1}^{N} \\phi_m(\\mathbf{x}_n) \\\\\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "\n",
    "Thus the basis $w_0$ compensates for the difference between the average of target values\n",
    "and\n",
    "the weighted sum of the average of the basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameter\n",
    "--------------\n",
    "\n",
    "Diff log likelihood wrt $\\beta$ and equating it to zero, we get  \n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{N}{2}\n",
    "\\frac{1}{\\beta}\n",
    "-\n",
    "\\frac{1}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\frac{1}{\\beta_{ML}}\n",
    "&=\n",
    "\\frac{1}{N}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\end{array}\n",
    "That is, the inverse of precision is the variance of the target\n",
    "about the regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Learning\n",
    "===========\n",
    "\n",
    "* Stochastic Gradient can be used to find the parameters sequentially.\n",
    "* Update rule: $\\mathbf{w}_{\\tau+1} = \\mathbf{w}_{\\tau} + \\eta \\nabla E(\\mathbf{w})$\n",
    "* If squared loss function is assumed,  \n",
    "  $\\nabla E_D(\\mathbf{w}) = \n",
    "  \\left( \n",
    "      t_n - \n",
    "      \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  \\right)\n",
    "  \\mathbf{\\phi}(\\mathbf{x}_n)$\n",
    "* Hence, the update rule becomes,  \n",
    "  $\\mathbf{w}_{\\tau+1}\n",
    "  = \\mathbf{w}_{\\tau}\n",
    "    + \\eta\n",
    "      \\left( \n",
    "          t_n - \n",
    "          \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "      \\right)\n",
    "      \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "========\n",
    "\n",
    "* Form: $E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Regularization\n",
    "-----------------------\n",
    "\n",
    "* $E_W(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}$\n",
    "* Called as *weight decay* since $\\mathbf{w}$ decays to zero when $\\lambda$ is high\n",
    "* Called as *Parameter Shrinkage* as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total Error Function:  \n",
    "\\begin{array}{rlr}\n",
    "E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})\n",
    "&=\n",
    "\\frac{1}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+\n",
    "\\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\sum_{n=1}^{N}\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sum_{n=1}^{N}\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+ \\lambda \\mathbf{w}^T\n",
    "& \\color{gray}{\\text{Diff wrt w}}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\mathbf{t}^T \\Phi - \\mathbf{w}^T \\Phi^T \\Phi + \\lambda \\mathbf{w}^T\n",
    "\\\\\n",
    "\\left( \\Phi^T \\Phi - \\lambda \\mathcal{I} \\right) \\mathbf{w} \n",
    "&=\n",
    "\\Phi^T \\mathbf{t}\n",
    "\\\\\n",
    "\\mathbf{w}\n",
    "&= \n",
    "\\left( \n",
    "    \\Phi^T \\Phi - \\lambda \\mathcal{I}\n",
    "\\right)^{-1} \n",
    "\\Phi^T \\mathbf{t}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Regularizer\n",
    "---------------------\n",
    "* $\\frac{\\lambda}{2} \\sum_{m=0}^{M-1} \\left| \\mathbf{w_j} \\right|^q$\n",
    "* Allows complex models to be fit on smaller data sets\n",
    "* The problem of finding the right model complexity transforms to that of finding the right value for $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    return +1 if x>=0 else -1\n",
    "\n",
    "thetas = np.linspace(0,2*math.pi,10**3)\n",
    "pts = np.asarray([[math.cos(theta),math.sin(theta)] for theta in thetas])\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in [abs(pts[ix,0]-pts[100-1-ix,0])<1e-5 for ix in range(50)] if x is False]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q=1/2\n",
    "def show_regularizer(q=0.5):\n",
    "    ptsq = np.zeros_like(pts)\n",
    "    for ix in range(math.floor(ptsq.shape[0]/2.)):\n",
    "        x = pts[ix,0]\n",
    "        ptsq[ix,0] = ptsq[100-1-ix,0] = x\n",
    "        y_abs = abs((1 - abs(x)**q)**(1./q))\n",
    "        ptsq[ix,1] = y_abs\n",
    "        ptsq[100-1-ix,1] = -y_abs\n",
    "    plt.plot(ptsq[:,0],ptsq[:,1],'.',label='q='+str(q))\n",
    "    \n",
    "def interact_regularizer(q=0.5,offset=0.7,show_all=True):\n",
    "    qs = [0.5,1.,2.,4.] if show_all else [q]\n",
    "    [show_regularizer(qq) for qq in qs]\n",
    "    plt.xlim(plt.xlim()[0]-0.1,plt.xlim()[1]+offset)\n",
    "    plt.ylim(plt.ylim()[0]-0.1,plt.ylim()[1]+0.1)\n",
    "    plt.legend()\n",
    "    plt.title('Contours of Reqularizion function')\n",
    "    plt.show()\n",
    "    \n",
    "interact(interact_regularizer,q=(.5,4,.5),offset=(0,1.,.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso\n",
    "-----\n",
    "\n",
    "* By setting $q=1$ in the generalized regularizer\n",
    "* Has a tendency to set some weights to zero, thereby making it a \"feature selector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Outputs\n",
    "==========\n",
    "\n",
    "* Let\n",
    "  * $\\mathbf{y}(\\mathbf{x},\\mathbf{w})$ be K dimensional\n",
    "  * W be $M \\times K$ matrix of parameters\n",
    "  * $\\mathbf{\\phi}(\\mathbf{x})$ is M dimensional\n",
    "  * $\\mathbf{t}$ is K dimensional\n",
    "\n",
    "\\begin{array}{rlr}\n",
    "p(\\mathbf{t} \\mid \\mathbf{x}, \\mathbf{W}, \\beta)\n",
    "&=\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\mathbf{t} \\mid \\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x}),\n",
    "\\beta^{-1} \\mathcal{I} \n",
    "\\right)\n",
    "\\\\\n",
    "\\ln p = 0\n",
    "&=\n",
    "\\frac{NK}{2} \\ln\\left(\\frac{\\beta}{2\\pi}\\right)\n",
    "-\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left\\|\n",
    "\\mathbf{t}_n\n",
    "-\n",
    "\\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x})\n",
    "\\right\\|^2\n",
    "&\n",
    "\\color{gray}{\\text{See: Multivariate Gaussian}}\n",
    "\\\\\n",
    "\\mathbf{W}_{ML}\n",
    "&=\n",
    "\\left(\n",
    "\\Phi^T \\Phi\n",
    "\\right)^{-1}\n",
    "\\Phi^T \\mathbf{T}\n",
    "= \n",
    "\\Phi^{\\dagger} \\mathbf{T}\n",
    "\\end{array}\n",
    "* That is, the same $\\Phi^{\\dagger}$ can be used for all K output target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Variance Decomposition\n",
    "===============\n",
    "\n",
    "* Frequentist viewpoint of the model complexity issue\n",
    "* For a squared loss function, the optimal prediction is given by the conditional expectation h(x),  \n",
    "  $$h(\\xb) = \\E{t \\mid \\xb} = \\int t ~p (t \\mid \\xb) ~dt$$\n",
    "* The expected square loss can be written as,\n",
    "$$\n",
    "\\E{\\mathcal{L}}\n",
    "=\n",
    "\\int \\left(\n",
    "    y(\\xb) - h(\\xb)\n",
    "\\right)^2\n",
    "~p(\\xb) ~d\\xb\n",
    "+\n",
    "\\iint \\left(\n",
    "    h(\\xb) - t\n",
    "\\right)^2\n",
    "~p(\\xb, t) ~dx dt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the First term $\\left(y(\\mathbf{x}) - h(\\mathbf{x})\\right)^2$.  \n",
    "$\\pm \\edyx$, we get  \n",
    "$$\n",
    "\\left\\{\\yx - \\edyx + \\edyx - \\hx\\right\\}^2\n",
    "\\\\\n",
    "\\hspace{10pt}=\n",
    "\\left(\n",
    "    \\yx - \\edyx\n",
    "\\right)^2\n",
    "+\n",
    "\\left(\n",
    "    \\edyx - \\hx\n",
    "\\right)^2\n",
    "+\n",
    "2\n",
    "\\left(\\yx - \\edyx\\right)\n",
    "\\left(\\edyx - \\hx\\right)\n",
    "$$\n",
    "\n",
    "Taking the expectation wrt $\\mathcal{D}$, the last term vanishes.  \n",
    "$$\n",
    "\\ed{\\left(\\yx - \\hx\\right)^2}\n",
    "=\n",
    "\\underbrace{\\left(\n",
    "    \\ed{\\yx - \\hx}\n",
    "\\right)^2}_{(\\text{bias})^2}\n",
    "+\n",
    "\\underbrace{\n",
    "\\ed{\n",
    " \\left(\n",
    "     \\yx - \\edyx\n",
    " \\right)\n",
    "}}_{\\text{variance}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Squared-bias: Deviation of the average prediction from the desired\n",
    "1. Variation of the individual predictions about the average prediction\n",
    "\n",
    "Thus,  \n",
    "\\begin{array}{ll}\n",
    "\\text{expected loss} &= (\\text{bias})^2 + \\text{variance} + \\text{noise}\\\\\n",
    "&\\color{green}{\\text{where}}&\\\\\n",
    "(\\text{bias})^2\n",
    "&=\n",
    "\\int \\left(\\edyx - \\hx\\right)^2 \\pxdx\n",
    "\\\\\n",
    "\\text{variance}\n",
    "&=\n",
    "\\int \\ed{\\left( \\yx - \\edyx \\right)^2} \\pxdx\n",
    "\\\\\n",
    "\\text{noise}\n",
    "&=\n",
    "\\iint \\left(\\hx - t\\right)^2 ~p(\\xb,t) \\dx ~dt\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Value**  \n",
    "* Zilch\n",
    "* we have only a single data set\n",
    "* If there are large no. of data sets, we can combine them into a single large dataset\n",
    "* This would reduce the overall level of overfitting for a given model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Linear Regression\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter distribution\n",
    "-----------------------\n",
    "\n",
    "From <a href='#GaussianIidLikelihood'>Likelihood</a> given by\n",
    "$$\n",
    "p(\\tb \\mid \\wb) =\n",
    "\\left(\n",
    "  \\dfrac{\\beta}{2\\pi}\n",
    "\\right)^{N/2}\n",
    "\\exp\n",
    "\\left\\{\n",
    "  -\\dfrac{\\beta}{2}\n",
    "  \\sumnN \\left( t_n - \\wbt \\phi\\left(\\xb_n\\right) \\right)^2\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "Since the likelihood is exponential of quadratic function of **w**,\n",
    "the prior is a Gaussian given by\n",
    "$$\n",
    "p(\\wb)\n",
    "=\n",
    "\\Nl{\\wb}{\\mb_0}{\\Sb_0}\n",
    "$$\n",
    "Now we need to find $p(\\wb \\mid \\tb)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things to remember (<a href='../Gaussian Stuff.ipynb#BayesTheoremForGaussianVariables'>Bayes' Theorem for Gaussian Variables</a>)\n",
    "\n",
    "Given a marginal Gaussian for **x** and a conditional gaussian for **y** given **x** of the form\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\xb) \n",
    "&=\n",
    "\\Nl{\\xb}{\\mub}{\\li}\n",
    "\\\\\n",
    "p(\\yb \\mid \\xb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\xb + \\bb}{\\Lbi}\n",
    "\\\\\n",
    "\\color{green}{\\text{Marginal }}\n",
    "p(\\yb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\mub+\\bb}{\\Lbi + \\Ab\\Lambda^{-1}\\Abt}\n",
    "\\\\\n",
    "\\color{green}{\\text{Conditional }}\n",
    "p(\\xb \\mid \\yb)\n",
    "&=\n",
    "\\Nl\n",
    "{\\xb}\n",
    "{\\Sigma \\left\\{ \\Abt\\Lb(\\yb-\\bb) + \\Lambda\\mub \\right\\}}\n",
    "{\\Sigma}\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\Sigma\n",
    "&= \\left(\\Lambda + \\Abt\\Lb\\Ab\\right)^{-1}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus\n",
    "$$\n",
    "\\arrthree{\n",
    "\\mub  &\\equiv \\mb_0 \\\\\n",
    "\\li &\\equiv \\Sb_0 \\\\\n",
    "\\Ab &\\equiv \\Phi \\\\\n",
    "\\yb &\\equiv \\tb \\\\\n",
    "\\Lb &\\equiv \\beta\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the posterior is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\wb \\mid \\tb)\n",
    "&=\n",
    "\\Nl{\\wb}{\\mb_N}{\\Sb_N}\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\mb_N\n",
    "&=\n",
    "\\Sigma \\left\\{ \\Abt\\Lb(\\yb-\\bb) + \\Lambda\\mub \\right\\}\n",
    "&=\n",
    "\\Sb_N \\left\\{ \\Phi^T \\beta \\tb + \\Sb_0^{-1} \\mb_0\\right\\}\n",
    "\\\\\n",
    "&=\n",
    "\\Sb_N \\left( \\beta \\Phi^T \\tb + \\Sb_0^{-1} \\mb_0\\right)\n",
    "\\\\\n",
    "\\Sb_N^{-1}\n",
    "&=\n",
    "\\left(\\Lambda + \\Abt\\Lb\\Ab\\right)\n",
    "&=\n",
    "\\left(\\Sb_0^{-1} + \\Phi^T \\beta \\Phi\\right)\n",
    "\\\\\n",
    "&=\n",
    "\\left(\\Sb_0^{-1} + \\beta \\Phi^T \\Phi\\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider the prior as an isotropic Gaussian with precision $\\alpha$, such that\n",
    "<div id='BayesianLinearRegressionPosteriorParameters'/>\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\wb \\mid \\alpha)\n",
    "&=\n",
    "\\Nl{\\wb}{\\mathbf{0}}{\\alpha^{-1}\\Ib}\n",
    "\\\\\n",
    "\\text{Thus the posterior} & \\text{ parameters becomes}\n",
    "\\\\\n",
    "\\mb_N\n",
    "&=\n",
    "\\Sb_N (\\beta \\Phib^T \\tb)\n",
    "=\n",
    "\\beta \\Sb_N \\Phib^T \\tb\n",
    "\\\\\n",
    "\\Sb_N^{-1} \n",
    "&=\n",
    "\\alpha\\Ib + \\beta \\Phib^T \\Phib\n",
    "}\n",
    "$$\n",
    "The log of the posterior given by the sum of the log likelihood and log prior, as\n",
    "$$\n",
    "\\ln p(\\wb \\mid \\tb)\n",
    "=\n",
    "-\\dfrac{\\beta}{2}\n",
    "\\sumnN \\left\\{ t_n - \\wbt \\phi(\\xb_n)\\right\\}\n",
    "-\\dfrac{\\alpha}{2} \\wbt \\wb\n",
    "+ \\text{ const}\n",
    "$$\n",
    "The max of this posterior wrt **w** is equivalent to \n",
    "minimization of the sum-of-the-squares error function\n",
    "with the addition of the quadratic regularization term,\n",
    "with $\\lambda = \\alpha / \\beta$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "def fun_f(x, a):\n",
    "    return a[0] + a[1]*1.*x\n",
    "\n",
    "def show_gaussian(mean, precision_mat):\n",
    "    in_pts = 10**2\n",
    "    x = np.linspace(-1,1,in_pts)\n",
    "    xv,yv=np.meshgrid(x,x)\n",
    "    #(2 \\pi \\beta)^(-1/2)\n",
    "    coeff =math.sqrt(np.linalg.det(precision_mat)/(2.*math.pi))\n",
    "    img = np.zeros((in_pts,in_pts))\n",
    "    for ix in range(in_pts):\n",
    "        for iy in range(in_pts):\n",
    "            x = np.matrix([xv[ix,iy],yv[ix,iy]]).transpose()\n",
    "            # -1/2 * (x-mu)T Precision (x-mu)\n",
    "            expt = -1./2. * (x - mean).transpose() * precision_mat * (x-mean)\n",
    "            img[ix,iy] = coeff * math.exp(expt[0,0])\n",
    "    \n",
    "    ticks = np.arange(-1,1,1.0)\n",
    "    extent = (-1,1,-1,1)\n",
    "    plt.imshow(img, extent=extent, origin='lower')\n",
    "\n",
    "def show_bay_lin_reg(in_samples,a0,a1,beta_sd,alpha):\n",
    "    mean_0 = np.matrix([0,0]).transpose()\n",
    "    xn = np.random.rand(in_samples)*2.0 - 1.0\n",
    "    fn = fun_f(xn, [a0,a1])\n",
    "    xn_noise = np.random.normal(0,beta_sd,in_samples)\n",
    "    # tn = a0 + a1*x + N(0,1/beta)\n",
    "    tn = np.matrix(fn + xn_noise).reshape(in_samples,1)\n",
    "    # phi = [ones^T x^T]\n",
    "    phi = np.matrix(np.vstack([np.ones_like(xn),xn])).transpose()\n",
    "    # Sn = beta*phi^T*phi + alpha*I\n",
    "    sn_inv = beta_sd * phi.transpose() * phi + alpha*np.eye(2)\n",
    "    sn = np.linalg.inv(sn_inv)\n",
    "    # m_n = beta*Sn*Phi^T*Tn\n",
    "    mean_n = beta_sd * sn * phi.transpose() * tn\n",
    "    print(mean_n)\n",
    "    show_gaussian(mean_n, sn_inv)\n",
    "    plt.plot(a0,a1,'+w',markersize=30)\n",
    "    plt.show()\n",
    "def interact_bay_lin_reg(in_samples=40,\n",
    "                         a0=-0.3,a1=0.5,\n",
    "                         beta_sd=0.2,alpha=2.0,\n",
    "                         show_all=True):\n",
    "    show_bay_lin_reg(in_samples,a0,a1,beta_sd,alpha)\n",
    "    \n",
    "interact(interact_bay_lin_reg,\n",
    "         in_samples=(1,200),\n",
    "         a0=(-1,1,0.1),a1=(-1,1,0.1),\n",
    "         beta_sd=(0,1,0.05),alpha=(1,50,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Distribution\n",
    "-----------------------\n",
    "\n",
    "* Usually not interested in the value of **w** itself.\n",
    "* need to make prediction of *t* for new values of **x**\n",
    "* Predictive Distribution $\n",
    "p(t \\mid \\tb, \\alpha, \\beta)\n",
    "=\n",
    "\\int p(t \\mid \\wb, \\beta) p(\\wb \\mid \\tb, \\alpha, \\beta)\n",
    "$\n",
    "* The conditional distribution is given by\n",
    "$$\n",
    "p(t \\mid \\wb, \\beta) = \\Nl{t}{y(\\xb,\\wb)}{\\beta^{-1}}\n",
    "$$\n",
    "* Posterior is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\wb \\mid \\tb)\n",
    "&=\n",
    "\\Nl{\\wb}{\\mb_N}{\\Sb_N}\n",
    "\\\\\n",
    "\\text{where }\n",
    "\\\\\n",
    "\\mb_N &= \\Sb_N \\left( \\Sb_0^{-1} \\mb_0 + \\beta \\Phib^T \\Phib \\right)\n",
    "\\\\\n",
    "\\Sb_{N}^{-1} &= \\Sb_{0}^{-1} + \\beta \\Phib^T \\Phib\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using <a href='../Gaussian Stuff.ipynb#BayesTheoremForGaussianVariables'>Bayes' Theorem for Gaussian Variables</a>, we have the following\n",
    "\n",
    "Given a marginal Gaussian for **x** and a conditional gaussian for **y** given **x** of the form\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\xb) \n",
    "&=\n",
    "\\Nl{\\xb}{\\mub}{\\li}\n",
    "\\\\\n",
    "p(\\yb \\mid \\xb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\xb + \\bb}{\\Lbi}\n",
    "\\\\\n",
    "\\color{green}{\\text{Marginal }}\n",
    "p(\\yb)\n",
    "&=\n",
    "\\Nl{\\yb}{\\Ab\\mub+\\bb}{\\Lbi + \\Ab\\Lambda^{-1}\\Abt}\n",
    "\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "Here\n",
    "$$\n",
    "\\arrthree{\n",
    "\\xb &\\equiv \\wb\n",
    "&\n",
    "\\mub &\\equiv \\mb_N\n",
    "&\n",
    "\\li &\\equiv \\Sb_N\n",
    "\\\\\n",
    "\\yb &\\equiv t\n",
    "&\n",
    "\\Abt,\\bb &\\equiv \\Phib,\\mathbf{0}\n",
    "&\n",
    "\\Lbi &\\equiv \\beta^{-1}\n",
    "}\n",
    "$$\n",
    "\n",
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\E{\\yb}\n",
    "&=\n",
    "\\Ab \\mub + \\bb = \n",
    "\\mb_N^T \\Phib\n",
    "\\\\\n",
    "\\sigma(\\yb)\n",
    "&=\n",
    "\\beta^{-1} + \\Phib^T \\Sb_N \\Phib\n",
    "}\n",
    "$$\n",
    "\n",
    "The second term goes to zero as N increases ([Qazaz][qazaz1997]).\n",
    "\n",
    "[qazaz1997]: http://dl.acm.org/citation.cfm?id=268081 \"Cambridge University Press. Qazaz, C. S., C. K. I. Williams, and C. M. Bishop (1997). An upper bound on the Bayesian error bars for generalized linear regression. In S. W. Ellacott, J. C. Mason, and I. J. Anderson (Eds.), Mathematics of Neural Networks: Models, Algorithms and Applications, pp. 295–299. Kluwer.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat both $\\wb, \\beta$ as unknown, the predictive distribution becomes a Student't t-distribution (<a href='/notebooks/void-main/Gaussian%20Stuff.ipynb#Unknown-mean,-unknown-variance'>Unknown Mean,Variance</a>)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample theta from [0,2*pi]\n",
    "in_samples, in_pts_pred=10,10**2\n",
    "thetas_sample = np.random.rand(in_samples)*2*math.pi\n",
    "thetas = np.linspace(0,2*math.pi,in_pts_pred)\n",
    "plt.plot(thetas, [math.sin(theta) for theta in thetas],label='sine')\n",
    "\n",
    "# adding noise\n",
    "beta_sd = 0.2\n",
    "beta = 1./(beta_sd**2)\n",
    "xn_noise = np.random.normal(0,beta_sd,in_samples)\n",
    "tn = [[math.sin(theta) for theta in thetas_sample]+xn_noise]\n",
    "tn = np.matrix(tn).reshape(in_samples,1)\n",
    "plt.plot(thetas_sample, tn, 'or',label='tn')\n",
    "\n",
    "alpha=2.0\n",
    "phi_posterior = np.matrix(\n",
    "    np.vstack(\n",
    "        [np.ones_like(thetas_sample),\n",
    "         thetas_sample\n",
    "        ]\n",
    "    )\n",
    ").transpose()\n",
    "# Sn = beta*phi^T*phi + alpha*I\n",
    "sn_inv = beta * phi_posterior.transpose() * phi_posterior + alpha*np.eye(2)\n",
    "sn = np.linalg.inv(sn_inv)\n",
    "# m_n = beta*Sn*Phi^T*Tn\n",
    "mean_n = beta * sn * phi_posterior.transpose() * tn\n",
    "\n",
    "phi_pred = np.matrix(\n",
    "    np.vstack(\n",
    "        [np.ones_like(thetas), thetas]))\n",
    "mean_pred = mean_n.transpose() * phi_pred\n",
    "plt.plot(np.matrix(thetas).transpose(), mean_pred.transpose(),label='predicted mean')\n",
    "\n",
    "covar_pred = 1./beta + phi_pred.transpose()*sn*phi_pred\n",
    "print(covar_pred.shape)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent Kernel\n",
    "-----------------\n",
    "\n",
    "* The posterior mean (<a href='#BayesianLinearRegressionPosteriorParameters'>Posterior Mean</a>) is given by $\\mb_N = \\beta \\Sb_N \\Phib^T \\tb$.\n",
    "* Sub this into the regression function, we have\n",
    "$$\n",
    "\\arrthree{\n",
    "y(\\xb, \\mb_N)\n",
    "&=\n",
    "\\mb_N^T \\phi(x)\n",
    "\\\\\n",
    "&=\n",
    "\\phi(x) \\mb_N^T\n",
    "\\\\\n",
    "&=\n",
    "\\beta \\phi(\\xb)^T \\Sb_N \\Phib^T \\tb\n",
    "\\\\\n",
    "&=\n",
    "\\beta \\phi(\\xb)^T \\Sb_N\n",
    "\\mat{\n",
    "\\vdots      & \\cdots & \\vdots \\\\\n",
    "\\phi(\\xb_1) & \\cdots & \\phi(\\xb_N) \\\\\n",
    "\\vdots      & \\cdots & \\vdots \\\\\n",
    "}\n",
    "\\tb\n",
    "\\\\\n",
    "&=\n",
    "\\sumnN \\beta \\phi(\\xb)^T \\Sb_N \\phi(\\xb_n) t_n\n",
    "\\\\\n",
    "&=\n",
    "\\sumnN k(\\xb,\\xb_n) t_n\n",
    "\\\\\n",
    "\\text{where }\\\\\n",
    "k(\\xb,\\xb^{\\prime})\n",
    "&=\n",
    "\\beta \\phi(\\xb)^T \\Sb_N \\phi(\\xb_n)\n",
    "}\n",
    "$$\n",
    "\n",
    "* *k* is called *smoother matrix* or the *equivalent kernel*\n",
    "* *Linear Smoothers*: Makes predictions by taking linear combination of the training set target values\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\sigma[y(\\xb),y(\\xb^{\\prime})]\n",
    "&=\n",
    "\\sigma[\\phi(\\xb)^T\\wb, \\wb^T \\phi(\\xb^{\\prime})]\n",
    "\\\\\n",
    "&=\n",
    "\\phi(\\xb)^T \\Sb_N \\phi(\\xb^{\\prime})\n",
    "\\\\\n",
    "&=\n",
    "\\beta^{-1} k(\\xb, \\xb^{\\prime})\n",
    "}\n",
    "$$\n",
    "[? How the fuck did you get to that second step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidence Approximation\n",
    "=======================\n",
    "\n",
    "* There are two hyperparameters, $\\alpha$ and $\\beta$\n",
    " * $\\alpha$: for the prior of $\\wb$\n",
    " * $\\beta$ for the noise of y\n",
    " \n",
    "* Can integrate analytically over **w** or hyperparameters\n",
    "  * but complete marginalization is analytically intractable\n",
    "  \n",
    "* Here, we use an approximation to find the hyperparameters by maximizing marginal likelihood function obtained by first integrating over **w**\n",
    "* This framework is called\n",
    "  * empirical Bayes\n",
    "  * type 2 maximum likelihood\n",
    "  * generalized maximum likelihood\n",
    "  * evidence approximation\n",
    "  \n",
    "Steps involved\n",
    "1. use the prior and likelihood forms\n",
    "1. complete the square to separate out **w**\n",
    "1. use normalization coefficient of a Gaussian\n",
    "1. compute the log evidence\n",
    "1. find $\\alpha$ and $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive distribution:\n",
    "$$\n",
    "p(t \\mid \\tb) = \n",
    "\\iiint\n",
    "~p(t \\mid \\wb, \\beta)\n",
    "~p(\\wb \\mid \\tb, \\alpha, \\beta)\n",
    "~p(\\alpha, \\beta \\mid \\tb)\n",
    "~d\\wb ~d\\alpha ~d\\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal likelihood:\n",
    "$$\n",
    "p(\\tb \\mid \\alpha, \\beta) = \n",
    "\\int\n",
    "~p(\\tb \\mid \\wb, \\alpha, \\beta)\n",
    "~p(\\wb \\mid \\alpha)\n",
    "~d\\wb\n",
    "$$\n",
    "\n",
    "* this can be evaluated using the [conditional distribution](/notebooks/void-main/Gaussian%20Stuff.ipynb#Conditional-Gaussian-Distributions) of a gaussian model\n",
    "* we shall evaluate by completing the integral and making use of the std. form of the normalization coefficient of the gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional likelihood:\n",
    "$$\n",
    "p(\\tb \\mid \\wb, \\beta) = \n",
    "\\mathcal{N}(t_n \\mid \\wbt \\phi(\\xb_n), \\beta^{-1})\n",
    "=\n",
    "\\frac{N}{2} \\ln \\beta\n",
    "-\\frac{N}{2} \\ln(2\\pi)\n",
    "-\\beta E_D(\\wb)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "E_D(\\wb) = \n",
    "\\frac12 \\sumnN\n",
    "\\left\\{\n",
    "  t_n - \\wbt \\phi(\\xb_n)\n",
    "\\right\\}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior:\n",
    "$$\n",
    "p(\\wb \\mid \\alpha) = \n",
    "\\mathcal{N}(\\wb \\mid \\mathbf{0}, \\alpha^{-1} \\mathbf{I})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence,\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\tb \\mid \\alpha, \\beta) &=\n",
    "\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\n",
    "\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}\n",
    "\\int \\exp\\{-E(\\wb)\\} ~d\\wb\n",
    "\\\\\n",
    "\\text{where} \\\\\n",
    "E(\\wb)\n",
    "&=\n",
    "\\beta E_D(\\wb) + \\alpha E_W(\\wb)\n",
    "\\\\ &=\n",
    "\\beta \\normsqr{\\tb - \\Phi \\wb} +\n",
    "\\frac{\\alpha}{2} \\wbt \\wb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "E(\\wb)\n",
    "&=\n",
    "\\beta \\normsqr{\\tb - \\Phi \\wb} + \\frac{\\alpha}{2} \\wbt \\wb\n",
    "\\\\ &=\n",
    "\\beta (\\tb - \\Phi \\wb)^T (\\tb - \\Phi \\wb) + \\frac{\\alpha}{2} \\wbt \\wb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\beta\n",
    "\\left(\n",
    "  \\tb^T \\tb\n",
    "- \\tb^T \\Phi \\wb\n",
    "- \\wbt \\Phi^T \\tb\n",
    "+ \\wbt \\Phi^T \\Phi \\wb\n",
    "\\right)\n",
    "+ \\frac{\\alpha}{2} \\wbt \\wb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a lotta shit,\n",
    "$$\n",
    "\\arrthree{\n",
    "E(\\wb) &=\n",
    "E(\\mb_N) + \\frac12 (\\wb-\\mb_N)^T \\Ab (\\wb -\\mb_N)\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\Ab &= \\alpha \\Ib + \\beta \\Phi^T \\Phi\n",
    "\\\\\n",
    "\\mb_N &= \\beta \\Ab^{-1} |Phi^T \\tb\n",
    "\\\\\n",
    "E(\\mb_N) &=\n",
    "\\frac{\\beta}{2} \\normsqr{\\tb - \\Phi \\mb_N} \n",
    "+\\frac{\\alpha}{2} \\mb_{N}^{T} \\mb_N\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\int &\\exp \\left\\{ -E(\\wb) \\right\\} ~d\\wb\n",
    "\\\\ &=\n",
    "\\exp\\left\\{ -E(\\mb_N)\\right\\}\n",
    "\\int \\exp\n",
    "\\left\\{\n",
    "  -\\frac12 (\\wb - \\mb_N)^T \\Ab (\\wb - \\mb_N)\n",
    "\\right\\}\n",
    "~d\\wb\n",
    "\\\\ \n",
    "&=\n",
    "\\exp\\left\\{ -E(\\mb_N)\\right\\}\n",
    "(2\\pi)^{M/2}\n",
    "\\left| \\Ab \\right|^{-1/2}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\ln p(\\tb \\mid \\alpha, \\beta)\n",
    "&=\n",
    "\\frachalf{M} \\ln \\alpha\n",
    "+ \\frachalf{N} \\ln \\beta\n",
    "- E(\\mb_N)\n",
    "- \\frac12 \\ln \\left|\\Ab\\right|\n",
    "- \\frac12 \\ln(2\\pi)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding alpha\n",
    "-------------------------\n",
    "\n",
    "Consider the eigenvector equation\n",
    "$$\n",
    "\\left(\n",
    "  \\beta \\Phi^T \\Phi\n",
    "\\right)\n",
    "\\ub_i\n",
    "=\n",
    "\\lambda_i \\ub_i\n",
    "$$\n",
    "Hence the eigenvalues of **A** is $\\alpha + \\lambda_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\frac{d}{d\\alpha} \\ln \\left| \\Ab \\right|\n",
    "&=\n",
    "\\frac{d}{d\\alpha} \\ln \\prod_i \\left( \\alpha + \\lambda_i \\right)\n",
    "\\\\ &=\n",
    "\\frac{d}{d\\alpha} \\sum_i \\ln \\left( \\alpha + \\lambda_i \\right)\n",
    "\\\\ &=\n",
    "\\sum_i \\frac{1}{\\alpha + \\lambda_i}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, diff ln p(**t** | $\\alpha,\\beta$) wrt $\\alpha$\n",
    "$$\n",
    "\\arrthree{\n",
    "0 &=\n",
    "\\frac{M}{2\\alpha}\n",
    "- \\frac12 \\mb_N^T \\mb_N\n",
    "- \\frac12 \\sum_i \\frac{1}{\\alpha + \\lambda_i}\n",
    "\\\\\n",
    "\\alpha \\mb_N^T \\mb_N\n",
    "&=\n",
    "M - \\alpha \\sum_i \\frac{1}{\\alpha + \\lambda_i}\n",
    "\\\\\n",
    "&=\n",
    "\\sum_i 1\n",
    "- \\sum_i \\frac{\\alpha}{\\alpha + \\lambda_i}\n",
    "\\\\ &=\n",
    "\\sum_i 1 - \\frac{\\alpha}{\\alpha + \\lambda_i}\n",
    "\\\\ &=\n",
    "\\sum_i \\frac{\\lambda_i}{\\alpha + \\lambda_i}\n",
    "\\\\ &=\n",
    "\\gamma\n",
    "\\\\ \\text{Thus,} \\\\\n",
    "\\alpha &= \\frac{\\gamma}{\\mb_N^T \\mb_N}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is an implicit solution for $\\alpha$\n",
    "* have to resort to an iterative procedure to find $\\alpha$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding beta\n",
    "----------------\n",
    "\n",
    "* Eigenvalues $\\lambda_i$ are proportional to $\\beta$\n",
    "* Hence $$d\\lambda_i/d\\beta = \\lambda_i / \\beta$$\n",
    "Hence,\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{d}{d\\beta}\n",
    "&= \n",
    "\\ln \\left| \\Ab \\right|\n",
    "\\\\\n",
    "&=\n",
    "\\frac{d}{d\\beta}\n",
    "\\ln \\prod_i (\\alpha + \\lambda_i)\n",
    "\\\\ &=\n",
    "\\frac{d}{d\\beta}\n",
    "\\sum_i \\ln (\\alpha + \\lambda_i)\n",
    "\\\\ &=\n",
    "\\frac{1}{\\beta} \\sum_i \\frac{\\lambda_i}{\\lambda_i + \\alpha}\n",
    "\\\\ &=\n",
    "\\frac{\\gamma}{\\beta}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, diff ln p(**t** | $\\alpha,\\beta$) wrt $\\beta$\n",
    "$$\n",
    "\\arrthree{\n",
    "0 &=\n",
    "\\frac{N}{2\\beta}\n",
    "-\\frac12 \\sumnN\n",
    "\\left\\{\n",
    "  t_n - \\mb_N^T \\phi(x_n)\n",
    "\\right\\}^2\n",
    "- \n",
    "\\frac{\\gamma}{2\\beta}\n",
    "\\\\\n",
    "\\frac{1}{\\beta}\n",
    "&=\n",
    "\\frac{1}{N-\\gamma}\n",
    "-\\frac12 \\sumnN\n",
    "\\left\\{\n",
    "  t_n - \\mb_N^T \\phi(x_n)\n",
    "\\right\\}^2\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is an implicit solution for $\\beta$ and we have to resort to an iterative procedure to find its value.\n",
    "\n",
    "Thanks and Regards,  \n",
    "Your soul reaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
