{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$ \\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}$  \n",
    "$ \\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}$\n",
    "$ \\newcommand{\\EXP}[1]{\\exp\\left(#1\\right)}$  \n",
    "$ \\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\n",
    "\\newcommand{\\sumnN}{\\sum_{n=1}^{N}}\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr}\n",
    "#1\n",
    "\\end{array}\n",
    "}\n",
    "\\newcommand{\\mat}[1]{\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "#1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\dmc}{\\mathcal{D}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\Abt}{\\Ab^T}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Ib}{\\mathbf{I}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\Tb}{\\mathbf{T}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\Wb}{\\mathbf{W}}\n",
    "\\newcommand{\\WbT}{\\widetilde{\\mathbf{W}}}\n",
    "\\newcommand{\\wbt}{\\wb^T}\n",
    "\\newcommand{\\Wbt}{\\Wb^T}\n",
    "\\newcommand{\\WbTt}{\\WbT^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xbT}{\\tilde{\\xb}}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\XbT}{\\widetilde{\\Xb}}\n",
    "\\newcommand{\\Xbt}{\\Xb^T}\n",
    "\\newcommand{\\XbTt}{\\XbT^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "$\n",
    "$\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\hx}{h(\\xb)}\n",
    "\\newcommand{\\yx}{y(\\xb; \\mathcal{D})}\n",
    "\\newcommand{\\ed}[1]{\\mathbb{E}_D\\left[ #1 \\right]}\n",
    "\\newcommand{\\edyx}{\\ed{\\yx}}\n",
    "\\newcommand{\\px}{~p(\\xb)}\n",
    "\\newcommand{\\dx}{~d\\xb}\n",
    "\\newcommand{\\pxdx}{\\px \\dx}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "============\n",
    "\n",
    "* In regression, we used $y(\\xb) = \\wbt \\xb + w_o$\n",
    "* But we need a discrete output\n",
    "* Thus, we define $y(\\xb) = f(\\wbt \\xb + w_o)$\n",
    "* Here, f(.) is a nonlinear function\n",
    "  * called as activation function in ML\n",
    "  * $f^{-1}$ called link function in statistics\n",
    "* f(.) is called *Generalized linear model* since the decision surfaces correspond to f(.) = const => $\\wbt \\xb + w_0$ = constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least squares for classification\n",
    "-------------------------------\n",
    "\n",
    "* Let there be K classes $\\{\\mathcal{C}_k\\}$, each with its own linear model\n",
    "$$\n",
    "y_k(\\xb) = \\wb_k^T  \\xb + w_{k0}\n",
    "$$\n",
    "\n",
    "* These can be grouped together as \n",
    "$$\n",
    "\\yb(\\xb) = \\WbTt \\xbT\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\WbT = \n",
    "\\mat{\n",
    "w_{10} & w_{20} & \\cdots & w_{K0} \\\\\n",
    "\\wb_1 & \\wb_2 & \\cdots & \\wb_{K}\n",
    "}\n",
    "\\hspace{20pt}\n",
    "\\xbT = \\mat{1 \\\\ \\xb}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training set $\\{\\xb_n, \\tb_n \\}_{n=1}^{N}$\n",
    "* Let \n",
    "$$\n",
    "\\Tb = \\mat{\n",
    "\\vdots \\\\\n",
    "\\tb_n^T \\\\\n",
    "\\vdots \\\\\n",
    "}\n",
    "\\hspace{20pt}\n",
    "\\XbT = \\mat{\n",
    "\\vdots \\\\\n",
    "\\xbT_n^T \\\\\n",
    "\\vdots\n",
    "}\n",
    "$$\n",
    "* Sum of squres error function becomes\n",
    "$$\n",
    "E_D(\\WbT)\n",
    "=\n",
    "\\frac{1}{2}\n",
    "Tr\n",
    "\\left\\{\n",
    "\\left( \\XbT \\WbT - \\Tb \\right)^T\n",
    "\\left( \\XbT \\WbT - \\Tb \\right)\n",
    "\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets consider Tr($(\\Xb\\Wb - \\Tb)^T(\\Xb\\Xb - \\Tb)$)\n",
    "$$\n",
    "\\arrthree{\n",
    "d E_D\n",
    "&=\n",
    "d ~tr\\left\\{(\\Xb\\Wb - \\Tb)^T(\\Xb\\Xb - \\Tb)\\right\\}\n",
    "\\\\ &=\n",
    "d ~tr\\left\\{ \\Wbt\\Xbt\\Xb\\Wb - \\Wbt\\Xbt\\Tb - \\Tb^T\\Xb\\Wb - \\Tb^T\\Tb \\right\\}\n",
    "\\\\ &=\n",
    "tr\\left\\{ d(\\Wbt)\\Xbt\\Xb\\Wb + \\Wbt\\Xbt\\Xb ~d(\\Wb) - d(\\Wbt)\\Xbt\\Tb - \\Tb^T\\Xb~d(\\Wb) \\right\\}\n",
    "\\\\ &=\n",
    "tr\\left\\{ (d\\Wb)^T\\Xbt\\Xb\\Wb + \\Wbt\\Xbt\\Xb ~d\\Wb - (d\\Wb)^T\\Xbt\\Tb - \\Tb^T\\Xb~d\\Wb \\right\\}\n",
    "\\\\ &=\n",
    "tr\\left\\{\\Wbt\\Xbt\\Xb ~d\\Wb \\right\\}\n",
    "+\n",
    "tr\\left\\{\\Wbt\\Xbt\\Xb ~d\\Wb \\right\\}\n",
    "- \n",
    "tr\\left\\{\\Tb^T\\Xb~d\\Wb \\right\\}\n",
    "-\n",
    "tr\\left\\{\\Tb^T\\Xb~d\\Wb \\right\\}\n",
    "\\\\ &=\n",
    "2 ~tr \\left\\{ \\left(\\Wbt\\Xbt\\Xb - \\Tb^T\\Xb \\right) ~d\\Wb \\right\\}\n",
    "}\n",
    "$$\n",
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{d ~E_D}{d\\Wb}\n",
    "&=\n",
    "\\Wbt\\Xbt\\Xb - \\Tb^T\\Xb\n",
    "& \\color{gray}{\\text{Denominator layout}}\n",
    "\\\\\n",
    "&=\n",
    "\\Xbt\\Xb\\Wb - \\Xbt\\Tb\n",
    "& \\color{gray}{\\text{Numerator layout}}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{E_D\\left(\\WbT\\right)}{d\\WbT}\n",
    "=\n",
    "\\XbTt\\XbT\\WbT - \\XbTt\\Tb\n",
    "$$\n",
    "Setting this to zero, we get\n",
    "$$\n",
    "\\WbT = \\left( \\XbTt \\XbT \\right)^{-1} \\XbTt \\Tb\n",
    "= \\XbT^\\dagger \\Tb\n",
    "$$\n",
    "The discriminant function becomes\n",
    "$$\n",
    "\\yb(\\xb) = \\WbTt \\xb = \\Tb^T \\left( \\XbT^\\dagger \\right)^T \\xb\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_w(X,T):\n",
    "    Xt = np.matrix(X.transpose())\n",
    "    W = np.linalg.inv(Xt*X) * Xt * T\n",
    "    W = np.matrix(W)\n",
    "    #print('W')\n",
    "    #print(W)\n",
    "    return W\n",
    "\n",
    "def plot_regions(W, resolution=40):\n",
    "    x = np.linspace(-4,8,resolution)\n",
    "    y = np.linspace(-8,4,resolution)\n",
    "    Wt = W.transpose()\n",
    "    for xx in x:\n",
    "        for yy in y:\n",
    "            result = Wt * np.matrix((1,xx,yy)).reshape(3,1)\n",
    "            clr_result = 'r' if(result[0,0]>result[1,0]) else 'b'\n",
    "            plt.plot(xx,yy, 'o', color=clr_result, markersize=2)\n",
    "\n",
    "def find_y(W, x, y_arr):\n",
    "    pts_current = np.matrix(np.vstack((np.ones_like(y_arr),np.ones_like(y_arr)*x, y_arr)))\n",
    "    vals = W.transpose()*pts_current\n",
    "    diff = (vals[0,:] - vals[1,:]).tolist()[0]\n",
    "    ix_first = next((xx1[0] for xx1 in enumerate(diff) if xx1[1]>0),-1)\n",
    "    return y_arr[ix_first]\n",
    "\n",
    "def plot_line():\n",
    "    xplot = np.linspace(-4,4,100)\n",
    "    yplot = xplot\n",
    "    plt.plot(xplot, yplot, label='expected seperation')\n",
    "    plt.xlim(-4,8)\n",
    "    plt.ylim(-8,4)\n",
    "    # plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def points_class1():\n",
    "    n1 = 50\n",
    "    x1 = np.random.rand(n1)*8-4\n",
    "    x1 = np.random.normal(0,2,(n1,1))\n",
    "    y_noise = np.random.rand(n1)*3+0.3\n",
    "    y1 = x1 + np.reshape(y_noise,x1.shape)\n",
    "    x1_aug = np.hstack((np.ones_like(x1), x1,y1))\n",
    "    t1 = np.hstack((np.ones_like(x1), np.zeros_like(x1)))\n",
    "    plt.plot(x1, y1, '+', label='class1')\n",
    "    return (x1_aug, t1)\n",
    "\n",
    "def points_class2(add_outliers=False):\n",
    "    n2 = 50\n",
    "    x2 = np.random.rand(n2)*4\n",
    "    x2 = np.random.normal(0,2,(n2,1))\n",
    "    y_noise = np.random.rand(n2)*3+0.5\n",
    "    y2 = x2 - np.reshape(y_noise,x2.shape)\n",
    "    \n",
    "    if add_outliers:\n",
    "        n2_outlier = 10\n",
    "        x2_outlier = np.random.rand(n2_outlier)*2+6\n",
    "        y2_outlier = np.random.normal(-6,1,(n2_outlier,1))\n",
    "        x2_outlier = x2_outlier.reshape((n2_outlier,1))\n",
    "        x2 = np.vstack((x2,x2_outlier))\n",
    "        y2 = np.vstack((y2,y2_outlier))\n",
    "    \n",
    "    x2_aug = np.hstack((np.ones_like(x2), x2,y2))\n",
    "    t2 = np.hstack((np.zeros_like(x2), np.ones_like(x2)))\n",
    "    plt.plot(x2, y2, 'o', label='class2')\n",
    "    return (x2_aug, t2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a29e22b45250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#show_solution()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_solution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'interact' is not defined"
     ]
    }
   ],
   "source": [
    "def show_solution(add_outliers=True):\n",
    "    x1_aug, t1 = points_class1()\n",
    "    x2_aug, t2 = points_class2(add_outliers)\n",
    "\n",
    "    X = np.vstack((x1_aug,x2_aug))\n",
    "    X = np.matrix(X)\n",
    "    T = np.vstack((t1,t2))\n",
    "    T = np.matrix(T)\n",
    "    W = compute_w(X,T)\n",
    "    \n",
    "    x_end = 8 if add_outliers else 4\n",
    "    x_actual = np.linspace(-4,x_end,100)\n",
    "    y_arr = np.linspace(-8,4,100)\n",
    "    y_actual = [find_y(W, xx, y_arr) for xx in x_actual]\n",
    "    plt.plot(x_actual, y_actual, label='actual LoS')\n",
    "\n",
    "    #plot_regions(W, resolution=resolution)\n",
    "    plot_line()\n",
    "    str_connector = 'with' if add_outliers else 'w/o'\n",
    "    plt.title('Least squares '+str_connector+' outliers')\n",
    "    location = 'lower center' if add_outliers else 'lower right'\n",
    "    plt.legend(loc=location)\n",
    "    plt.show()\n",
    "\n",
    "#show_solution()\n",
    "interact(show_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Logistic Regression\n",
    "============================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps involved\n",
    "--------------\n",
    "\n",
    "1. Predictive = $\\int sigmoid \\times posterior$ \n",
    "1. posterior $\\approx$ Gaussian using Laplace method\n",
    "1. sigmoid = $\\int dirac(.) \\times sigmoid$\n",
    "1. Hence Predictive = $\\int gaussian (\\int ~dirac ~sigmoid)$\n",
    "1. Change the order of integration: $\\int (gaussian ~dirac) ~sigmoid $\n",
    "1. first term marginalizes to gaussian\n",
    "1. predictive = $\\int sigmoid ~gaussian$\n",
    "1. sigmoid $\\approx$ probit\n",
    "1. predictive = $\\int probit ~gaussian$\n",
    "1. conv of probit with gaussian is another probit\n",
    "1. predictive = probit\n",
    "1. since probit $\\approx$ sigmoid, we have predictive $\\approx$ sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive distribution\n",
    "-----------------------\n",
    "\n",
    "for a class $\\mathcal{C}_1$ and a given feature vector $\\phi(\\xb)$ is obtained by marginalizing wrt to the posterior $p(\\wb \\mid \\tb)$. That is,\n",
    "$$\n",
    "p(\\mathcal{C}_1 \\mid \\phi, \\tb)\n",
    "= \\int\n",
    "p(\\mathcal{C}_1 \\mid \\phi, \\wb)\n",
    "~p(\\wb \\mid \\tb) ~d\\wb\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lap Apprx to posterior\n",
    "-----------------------\n",
    "The posterior can be approximated using laplace method as follows:\n",
    "\n",
    "Gaussian prior is given by\n",
    "$$\n",
    "p(\\wb) = \\mathcal{N}(\\wb \\mid \\mb_0, \\Sb_0)\n",
    "$$\n",
    "Hence,\n",
    "$$\n",
    "p(\\wb \\mid \\tb) \\propto p(\\wb) ~p(\\tb \\mid \\wb)\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\arrthree{\n",
    "\\ln p(\\wb \\mid \\tb)\n",
    "&= -\\frac12 (\\wb-\\mb_0)^T \\Sb_0^{-1} (\\wb-\\mb_0)\n",
    "&\\commentgray{prior}\n",
    "\\\\ \n",
    "&+\n",
    "\\sumnN t_n \\ln y_n + (1-t_n) \\ln (1-y_n)\n",
    "&\\commentgray{sigmoid likelihood}\n",
    "\\\\\n",
    "&+\n",
    "\\text{const}\n",
    "}\n",
    "$$\n",
    "\n",
    "For the laplace approximation, the mode is $\\wb_{MAP}$ and the cov is given by,\n",
    "$$\n",
    "\\Sb_N = - \\nabla \\nabla \\ln p(\\wb \\mid \\tb)\n",
    "= \n",
    "\\Sb_{0}^{-1} + \\sumnN y_n (1-y_n) \\phi_n \\phi_n^T\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "p(\\wb \\mid \\tb) \\approx q(\\wb) = \\mathcal{N}(\\wb \\mid \\wb_{MAP}, \\Sb_N)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the marginal\n",
    "-----------------\n",
    "Thus\n",
    "$$\n",
    "p(\\mathcal{C}_1 \\mid \\phi, \\tb)\n",
    "= \\int\n",
    "p(\\mathcal{C}_1 \\mid \\phi, \\wb)\n",
    "~p(\\wb \\mid \\tb) ~d\\wb\n",
    "\\approx\n",
    "\\int \\sigma(\\wbt\\phi) q(\\wb) ~d\\wb\n",
    "$$\n",
    "\n",
    "* $\\sigma(\\wbt \\phi)$ depends on $\\wb$ only thro' its projection on $\\phi$.\n",
    "* Denote $a = \\wbt \\phi$ and we have,\n",
    "\n",
    "$$\n",
    "\\sigma(\\wbt \\phi) = \\int \\delta(a - \\wbt \\phi) ~\\sigma(a) ~da\n",
    "$$\n",
    "Sub this above, we have\n",
    "$$\n",
    "\\arrthree{\n",
    "\\int \\sigma(\\wbt\\phi) q(\\wb) ~d\\wb\n",
    "&= \n",
    "\\int \n",
    "     \\left\\{\\int \\delta(a - \\wbt \\phi) ~\\sigma(a) ~da \\right\\}\n",
    "     ~q(\\wb) ~d\\wb\n",
    "\\\\ &= \n",
    "\\int \n",
    "  \\left\\{\n",
    "    \\int ~q(\\wb) \\delta(a - \\wbt \\phi) ~d\\wb\n",
    "  \\right\\}\n",
    "   ~\\sigma(a) ~da\n",
    "\\\\ &=\n",
    "\\int p(a) ~\\sigma(a) ~da\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters of the marginal\n",
    "---------------------------\n",
    "\n",
    "$$\n",
    "p(a) = \\int \\delta(a-\\wbt\\phi) ~q(\\wb) ~d\\wb\n",
    "$$\n",
    "\n",
    "* The delta function imposes a linear contraint since $a-\\wbt\\phi$ is a hyperplane and only those $\\wb$'s which are on the hyperplane are chosen for the integrand. \n",
    "* This in effect trims outs $\\wb \\bot \\phi$ and marginalizes $q(\\wb)$.\n",
    "* Since $q(\\wb)$ is Gaussian, the marginal would be Gaussian as well\n",
    "* we can find the mean and cov as follows\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\mu_a = \\E{a}\n",
    "\\\\ &=\n",
    "\\int a ~p(a) ~da\n",
    "\\\\ &=\n",
    "\\int \\wbt \\phi ~q(\\wb) ~d\\wb\n",
    "\\\\\n",
    "\\mu_a &= \\wb_{MAP}^{T} \\phi\n",
    "\\\\\n",
    "\\sigma_{a}^{2} = \\text{var}[a]\n",
    "\\\\ &=\n",
    "\\int \\left\\{ a^2 - \\E{a}^2 \\right\\}^2 ~p(a) ~da\n",
    "\\\\ &=\n",
    "\\int\n",
    "  \\left\\{\n",
    "    \\left(\\wbt \\phi \\right)^2 -\n",
    "    \\left( \\mb_{N}^{T} \\phi \\right)\n",
    "  \\right\\}\n",
    "\\\\ &=\n",
    "\\phi^T \\Sb_N \\phi\n",
    "}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\mathcal{C}_1 \\mid \\tb)\n",
    "&=\n",
    "\\int \\sigma(a) ~p(a) ~da\n",
    "\\\\ &=\n",
    "\\int \\sigma(a) ~\\mathcal{N}(a \\mid \\mu_a,\\sigma_{a}^{2}) ~da\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probit\n",
    "-------\n",
    "\n",
    "Probit function $\\Phi(a)$ is given by\n",
    "$$\n",
    "\\Phi(a) = \\int_{-\\infty}^{a} \\mathcal{N}(\\theta \\mid 0,1) ~d\\theta\n",
    "= \\frac12 + \\int_{0}^{a} \\mathcal{N}(\\theta \\mid 0,1) ~d\\theta\n",
    "$$\n",
    "\n",
    "This has close resemblance to the logistic sigmoid given by\n",
    "$$\n",
    "\\sigma(a) = \\frac1{1+e^{-a}}\n",
    "$$\n",
    "\n",
    "The best approximation is obtained by scaling the horizontal axis by $\\lambda$ so that $\\sigma(a) \\approx \\Phi(\\lambda a)$\n",
    "\n",
    "Suitable value for $\\lambda$ can be obtained by ensuring that the slopes of sigmoid and probit to be the same at origin, which gives $\\lambda^2 = \\pi/8$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\mathcal{C}_1 \\mid \\tb)\n",
    "&=\n",
    "\\int \\sigma(a) ~p(a) ~da\n",
    "\\\\ &=\n",
    "\\int \\sigma(a) ~\\mathcal{N}(a \\mid \\mu_a,\\sigma_{a}^{2}) ~da\n",
    "\\\\ &=\n",
    "\\int \\Phi(\\lambda a) ~\\mathcal{N}(a \\mid \\mu_a,\\sigma_{a}^{2}) ~da\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convolving normal with probit\n",
    "--------------------------\n",
    "\n",
    "The adv of using probit is that its convolution with gaussian is another probit. that is\n",
    "$$\n",
    "\\int \\Phi(\\lambda a) ~\\mathcal{N}(a \\mid \\mu,\\sigma^{2}) ~da\n",
    "=\n",
    "\\phi\n",
    "\\left(\n",
    "  \\frac{\\mu}\n",
    "       {\\left(\n",
    "         \\frac1{\\lambda^2} + \\sigma^2\n",
    "        \\right)^{1/2}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the approximation $\\sigma(a) = \\Phi(\\lambda a)$ to the probit functions on both sides\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\int \\sigma(a) ~\\mathcal{N}(a \\mid \\mu,\\sigma^{2}) ~da\n",
    "&=\n",
    "\\int \\Phi(\\lambda a) ~\\mathcal{N}(a \\mid \\mu,\\sigma^{2}) ~da\n",
    "\\\\ &=\n",
    "\\phi\n",
    "\\left(\n",
    "  \\frac{\\mu}\n",
    "       {\\left(\n",
    "         \\lambda^{-2} + \\sigma^2\n",
    "        \\right)^{1/2}}\n",
    "\\right)\n",
    "\\\\ &=\n",
    "\\sigma(\\left(\n",
    "         \\lambda^{-2} + \\sigma^2\n",
    "        \\right)^{-1/2}\n",
    "       \\mu\n",
    "      )\n",
    "\\\\\n",
    "p(\\mathcal{C}_1 \\mid \\tb)\n",
    "&=\n",
    "\\sigma(\\kappa(\\sigma^2) \\mu)\n",
    "\\\\ \\text{where }\n",
    "& \\kappa(\\sigma^2) = \\left( 1 + \\pi \\sigma^2/8 \\right)^{-1/2}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is\n",
    "$$\n",
    "p(\\mathcal{C}_1 \\mid \\tb)\n",
    "=\n",
    "\\sigma(\\kappa(\\sigma_{a}^2) \\mu_a)\n",
    "$$\n",
    "Thus the predictive distribution is given by a logistic sigmoid. Aint that super cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo\n",
    "1. 3 class for least squares and its craziness"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
