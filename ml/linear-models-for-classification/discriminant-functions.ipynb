{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization\n",
    "$\n",
    "\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\Bracket}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\frachalf}[1]{\\frac{#1}{2}}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\fracrec}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\H}[1]{\\mathbb{H}\\left[#1\\right]}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\invp}[1]{\\left({#1}\\right)^{-1}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left(#1 \\Vert #2\\right)}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Ln}[1]{\\ln \\left\\(#1\\right\\)}\n",
    "\\newcommand{\\Lnb}[1]{\\ln \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\LN}[1]{\\ln\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\Mod}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\newcommand{\\Normsqr}[1]{\\Norm{#1}^2}\n",
    "\\newcommand{\\map}[1]{#1_{\\text{MAP}}}\n",
    "\\newcommand{\\ml}[1]{#1_{\\text{ML}}}\n",
    "\\newcommand{\\MI}[1]{\\mathcal{I}\\left(#1\\right)}\n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\Partial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\sqrbrkt}[1]{\\Bracket{#1}^2}\n",
    "\\newcommand{\\sqrbrc}[1]{\\Brace{#1}^2}\n",
    "\\newcommand{\\trace}[1]{\\text{Tr}\\left( #1 \\right)}\n",
    "\\newcommand{\\traceb}[1]{\\text{Tr}\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "$\n",
    "$\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\mat}[1]{ \\left[ \\begin{matrix} #1 \\end{matrix} \\right] }\n",
    "\\newcommand{\\matp}[1]{ \\left( \\begin{matrix} #1 \\end{matrix} \\right)}\n",
    "\\newcommand{\\mats}[1]{ \\begin{matrix}#1\\end{matrix} }\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr} #1 \\end{array}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Ca}{\\mathcal{C}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Ra}{\\mathcal{R}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumi}{\\displaystyle \\sum_i}\n",
    "\\newcommand{\\sumiD}{\\displaystyle \\sum_{i=1}^{D}}\n",
    "\\newcommand{\\sumiL}{\\displaystyle \\sum_{i=1}^{L}}\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodi}{\\displaystyle \\prod_i}\n",
    "\\newcommand{\\prodiD}{\\displaystyle \\prod_{i=1}^{D}}\n",
    "\\newcommand{\\prodiL}{\\displaystyle \\prod_{i=1}^{L}}\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet shortcuts\n",
    "$\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\At}{\\Ab^T}\n",
    "\\newcommand{\\Ai}{\\inv{\\Ab}}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\bt}{\\bb^T}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\fb}{\\mathbf{f}}\n",
    "\\newcommand{\\fp}{f^{\\prime}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\hx}{h(\\xb)}\n",
    "\\newcommand{\\Jb}{\\mathbf{J}}\n",
    "\\newcommand{\\Kb}{\\mathbf{K}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lt}{\\Lb^T}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\mt}{\\mb^T}\n",
    "\\newcommand{\\mbn}{\\mb_N}\n",
    "\\newcommand{\\mbnt}{\\mbn^T}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\sb}{\\mathbf{s}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\tt}{\\tb^T}\n",
    "\\newcommand{\\Tb}{\\mathbf{T}}\n",
    "\\newcommand{\\Tt}{\\Tb^T}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "\\newcommand{\\Ut}{\\Ub^T}\n",
    "\\newcommand{\\vb}{\\mathbf{v}}\n",
    "\\newcommand{\\Vb}{\\mathbf{V}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wt}{\\wb^T}\n",
    "\\newcommand{\\Wb}{\\mathbf{W}}\n",
    "\\newcommand{\\Wt}{\\Wb^T}\n",
    "\\newcommand{\\Wtilde}{\\widetilde{\\Wb}}\n",
    "\\newcommand{\\Wtildet}{\\Wtilde^T}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\Xtilde}{\\widetilde{\\Xb}}\n",
    "\\newcommand{\\Xtildet}{\\Xtilde^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xt}{\\xb^T}\n",
    "\\newcommand{\\xtilde}{\\widetilde{\\xb}}\n",
    "\\newcommand{\\xtilden}{\\xtilde_n}\n",
    "\\newcommand{\\xtildent}{\\xtilden^T}\n",
    "\\newcommand{\\xp}{x^{\\prime}}\n",
    "\\newcommand{\\xbp}{\\xb^{\\prime}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\yx}{y(\\xb)}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zt}{\\zb^T}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math shortcuts\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\Lambdab}{\\pmb{\\Lambda}}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\Li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\mut}{\\mub^T}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "\\newcommand{\\Phibt}{\\Phib^T}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\sigmasqr}{\\sigma^2}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\zerob}{\\pmb{0}}\n",
    "\\newcommand{\\ed}{\\mathbb{E}_{\\D}}\n",
    "\\newcommand{\\edyx}{\\ed\\left[y(\\xb ; \\D)\\right]}\n",
    "\\newcommand{\\dx}{~dx}\n",
    "\\newcommand{\\dxb}{~d\\xb}\n",
    "\\newcommand{\\pxdxb}{p(\\xb) \\dxb}\n",
    "\\newcommand{\\dwb}{~d\\wb}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In regression, we used $y(\\xb) = \\wt \\xb + w_o$\n",
    "* But we need a discrete output\n",
    "* Thus, we define $y(\\xb) = f(\\wt \\xb + w_o)$\n",
    "* Here, f(.) is a nonlinear function\n",
    "  * called as activation function in ML\n",
    "  * $f^{-1}$ called link function in statistics\n",
    "* f(.) is called *Generalized linear model* since the decision surfaces correspond to f(.) = const => $\\wt \\xb + w_0$ = constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let there be K classes $\\{\\mathcal{C}_k\\}$, each with its own linear model\n",
    "$$\n",
    "y_k(\\xb) = \\wb_k^T  \\xb + w_{k0}\n",
    "$$\n",
    "\n",
    "* These can be grouped together as \n",
    "$$\n",
    "\\yb(\\xb) = \\Wtildet \\xtilde\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Wtilde = \n",
    "\\mat{\n",
    "w_{10} & w_{20} & \\cdots & w_{K0} \\\\\n",
    "\\wb_1 & \\wb_2 & \\cdots & \\wb_{K}\n",
    "}\n",
    "\\hspace{20pt}\n",
    "\\xtilde = \\mat{1 \\\\ \\xb}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training set $\\{\\xb_n, \\tb_n \\}_{n=1}^{N}$\n",
    "* Let \n",
    "$$\n",
    "\\Tb = \\mat{\\vdots \\\\ - \\tb_n^T- \\\\ \\vdots \\\\}\n",
    "\\hspace{20pt}\n",
    "\\Xtilde = \\mat{\n",
    "\\vdots \\\\\n",
    "-\\xtildent- \\\\\n",
    "\\vdots\n",
    "}\n",
    "$$\n",
    "* Sum of squres error function becomes\n",
    "$$\n",
    "E_D(\\Wtilde)\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\trace{\n",
    "\\left( \\Xtilde \\Wtilde - \\Tb \\right)^T\n",
    "\\left( \\Xtilde \\Wtilde - \\Tb \\right)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets consider Tr($(\\Xb\\Wb - \\Tb)^T(\\Xb\\Xb - \\Tb)$)\n",
    "$$\n",
    "\\arrthree{\n",
    "d E_D\n",
    "&=\n",
    "\\half d ~\\traceb{(\\Xb\\Wb - \\Tb)^T(\\Xb\\Xb - \\Tb)}\n",
    "\\\\ &=\n",
    "\\half d ~\\traceb{ \\Wt\\Xt\\Xb\\Wb - \\Wt\\Xt\\Tb - \\Tt\\Xb\\Wb - \\Tt\\Tb }\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\text{First two terms } &=\\traceb{ d(\\Wt)\\Xt\\Xb\\Wb + \\Wt\\Xt\\Xb ~d(\\Wb)}\n",
    "\\\\ &=\n",
    "\\traceb{ (d\\Wb)^T\\Xt\\Xb\\Wb + \\Wt\\Xt\\Xb ~d\\Wb}\n",
    "\\\\ &=\n",
    "\\traceb{\\Wt\\Xt\\Xb ~d\\Wb }\n",
    "+\n",
    "\\traceb{\\Wt\\Xt\\Xb ~d\\Wb }\n",
    "\\\\ &=\n",
    "2\\traceb{\\Wt\\Xt\\Xb ~d\\Wb }\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\text{Last two terms }&=\n",
    "d ~\\traceb{ - \\Tt\\Xb\\Wb - \\Tt\\Tb }\n",
    "\\\\ &=\n",
    "-\\traceb{d(\\Wt)\\Xt\\Tb + \\Tt\\Xb~d(\\Wb)}\n",
    "\\\\ &=\n",
    "-\\traceb{ (d\\Wb)^T\\Xt\\Tb + \\Tt\\Xb~d\\Wb }\n",
    "\\\\ &=\n",
    "-  \\traceb{\\Tt\\Xb~d\\Wb } - \\traceb{\\Tt\\Xb~d\\Wb }\n",
    "\\\\ &=\n",
    "-2 \\traceb{\\Tt\\Xb~d\\Wb }\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{d ~E_D}{d\\Wb}\n",
    "&=\n",
    "\\Wt\\Xt\\Xb - \\Tt\\Xb\n",
    "& \\color{gray}{\\text{Denominator layout}}\n",
    "\\\\\n",
    "&=\n",
    "\\Xt\\Xb\\Wb - \\Xt\\Tb\n",
    "& \\color{gray}{\\text{Numerator layout}}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{E_D\\left(\\Wtilde\\right)}{d\\Wtilde}\n",
    "=\n",
    "\\Xtildet\\Xtilde\\Wtilde - \\Xtildet\\Tb\n",
    "$$\n",
    "Setting this to zero, we get\n",
    "$$\n",
    "\\Wtilde = \\left( \\Xtildet \\Xtilde \\right)^{-1} \\Xtildet \\Tb\n",
    "= \\Xtilde^\\dagger \\Tb\n",
    "$$\n",
    "The discriminant function becomes\n",
    "$$\n",
    "\\yb(\\xb) = \\Wtildet \\xb = \\Tt \\left( \\Xtilde^\\dagger \\right)^T \\xb\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_range(imin=0, imax=1):\n",
    "    return np.random.rand(n1).reshape(-1,1)*imax-imin\n",
    "def points_class1(n1 = 50, noise_sigma=3, noise_offset=0.3, plot_vals=False):\n",
    "    x1 = np.random.rand(n1).reshape(-1,1)*8-4\n",
    "    y_noise = np.random.rand(n1).reshape(-1,1)*noise_sigma+noise_offset\n",
    "    y1 = x1 + y_noise\n",
    "    x1_aug = np.hstack((np.ones_like(x1), x1,y1))\n",
    "    t1 = np.hstack((np.ones_like(x1), np.zeros_like(x1)))\n",
    "    if plot_vals:\n",
    "        plt.plot(x1, y1, '+', label='class1')\n",
    "    return (x1_aug, t1)\n",
    "\n",
    "def points_class2(n2=50, noise_sigma=3, noise_offset=0.5, plot_vals=False, add_outliers=False):\n",
    "    x2 = (np.random.rand(n2)*4).reshape(-1,1)\n",
    "    y_noise = (np.random.rand(n2)*noise_sigma+noise_offset).reshape(-1,1)\n",
    "    y2 = x2 - y_noise\n",
    "    \n",
    "    if add_outliers:\n",
    "        n2_outlier = 10\n",
    "        x2_outlier = (np.random.rand(n2_outlier)*2+6).reshape(-1,1)\n",
    "        y2_outlier = (np.random.normal(-6,1,(n2_outlier,1))).reshape(-1,1)\n",
    "        x2 = np.vstack((x2,x2_outlier))\n",
    "        y2 = np.vstack((y2,y2_outlier))\n",
    "    \n",
    "    x2_aug = np.hstack((np.ones_like(x2), x2,y2))\n",
    "    t2 = np.hstack((np.zeros_like(x2), np.ones_like(x2)))\n",
    "    if plot_vals:\n",
    "        plt.plot(x2, y2, 'o', label='class2')\n",
    "    return (x2_aug, t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w(X,T):\n",
    "    W = np.linalg.inv(X.T @ X) @ X.T @ T\n",
    "    print('W:',W.shape)\n",
    "    return W\n",
    "# start from the bottom, ie, from -\\infty\n",
    "# find the first y coordinate for which the prediction>0 => class 2 ends, class 1 begins\n",
    "def find_y(W, x, y_arr):\n",
    "    pts_current = np.hstack((np.ones_like(y_arr),np.ones_like(y_arr)*x, y_arr))\n",
    "    vals = pts_current @ W\n",
    "    diff = (vals[:,0] - vals[:,1]).tolist()\n",
    "    ix_first = next((xx1[0] for xx1 in enumerate(diff) if xx1[1]>0),-1)\n",
    "    return y_arr[ix_first]\n",
    "def plot_line():\n",
    "    xplot = np.linspace(-4,4,100)\n",
    "    plt.plot(xplot, xplot, label='expected seperation')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_solution(add_outliers=True):\n",
    "    x1_aug, t1 = points_class1(50, plot_vals=True)\n",
    "    x2_aug, t2 = points_class2(50, plot_vals=True, add_outliers=add_outliers)\n",
    "\n",
    "    X = np.vstack((x1_aug,x2_aug))\n",
    "    T = np.vstack((t1,t2))\n",
    "    W = compute_w(X,T)\n",
    "    \n",
    "    x_end = 8 if add_outliers else 4\n",
    "    x_actual = np.linspace(-4,x_end,100).reshape(-1,1)\n",
    "    y_arr = np.linspace(-8,4,100).reshape(-1,1)\n",
    "    y_actual = [find_y(W, xx, y_arr) for xx in x_actual]\n",
    "    plt.plot(x_actual, y_actual, label='actual LoS')\n",
    "\n",
    "    #plot_regions(W, resolution=resolution)\n",
    "    plot_line()\n",
    "    str_connector = 'with' if add_outliers else 'w/o'\n",
    "    plt.title('Least squares '+str_connector+' outliers')\n",
    "    location = 'lower center' if add_outliers else 'lower right'\n",
    "    plt.legend(loc=(1,0.5))\n",
    "    plt.xlim(-4,8)\n",
    "    plt.ylim(-8,4)\n",
    "    plt.show()\n",
    "\n",
    "#show_solution()\n",
    "interact(show_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous figure, without outliers, the decision surface is fine.   \n",
    "But on addition of outliers on the \"right\" side of the decision surface leads to a bad decision surface.  \n",
    "Not cool, least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the points of class 1, 2\n",
    "x1_aug, t1 = points_class1(100, noise_sigma=5, noise_offset=1, plot_vals=False)\n",
    "x2_aug, t2 = points_class2(100, noise_sigma=5, noise_offset=1, plot_vals=False)\n",
    "plt.plot(x1_aug[:,1],x1_aug[:,2],'.r')\n",
    "plt.plot(x2_aug[:,1],x2_aug[:,2],'.b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\mb_1 = \\fracone{N1} \\sum_{n \\in \\Ca_1} \\xbn &&\n",
    "\\mb_2 = \\fracone{N2} \\sum_{n \\in \\Ca_2} \\xbn \\\\\n",
    "&\\wb \\propto (\\mb_1 - \\mb_2)&\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_perp_line_equation(pt1, pt2, pt_on_line=None):\n",
    "    assert((pt1.shape == pt2.shape) and (pt1.size==2))\n",
    "    if pt_on_line is None:\n",
    "        pt_on_line = (pt1+pt2)/2.\n",
    "    # slope of perp = -(x2-x1)/(y2-y1)\n",
    "    slope = -1.*(pt2[0] - pt1[0])/(pt2[1] - pt1[1])\n",
    "    # y3 = m * x3 + c\n",
    "    y_intercept = pt_on_line[1] - slope * pt_on_line[0]\n",
    "    return (slope, y_intercept)\n",
    "    \n",
    "def plot_bounded_line(slope, y_intercept, plt_obj, color, linewidth):\n",
    "    x_lims, y_lims = plt_obj.xlim(), plt_obj.ylim()\n",
    "    x = np.linspace(x_lims[0], x_lims[1], 100)\n",
    "    y = 1.*slope*x + y_intercept\n",
    "    plt_obj.plot(x, y, c=color, linewidth=linewidth)\n",
    "    plt_obj.xlim(x_lims); plt_obj.ylim(y_lims)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1_aug[:,1],x1_aug[:,2],'.r')\n",
    "plt.plot(x2_aug[:,1],x2_aug[:,2],'.b')\n",
    "\n",
    "# find m1 and m2\n",
    "m1 = np.average(x1_aug, axis=0)\n",
    "m2 = np.average(x2_aug, axis=0)\n",
    "plt.plot(m1[1],m1[2], 'or', MarkerSize=10, label='m1')\n",
    "plt.plot(m2[1],m2[2], 'ob', MarkerSize=10, label='m2')\n",
    "\n",
    "w = m1-m2\n",
    "plt.plot(w[1],w[2], 'og', MarkerSize=10, label='w')\n",
    "plt.plot((m1[1],m2[1]),(m1[2],m2[2]), 'm')\n",
    "plt.plot((0,w[1]),(0, w[2]), 'm')\n",
    "plt.plot((m1[1]+m2[1])/2., (m1[2]+m2[2])/2., 'om', MarkerSize=10)\n",
    "m, c = find_perp_line_equation(m1[1:], m2[1:])\n",
    "plt.axis('equal')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plot_bounded_line(m, c, plt, color=(0,1,0), linewidth=3)\n",
    "#plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections1 = [w @ x for x in x1_aug]\n",
    "projections2 = [w @ x for x in x2_aug]\n",
    "plt.hist(projections1,color='r', alpha=0.5, label='class 1')\n",
    "plt.hist(projections2,color='b', alpha=0.5, label='class 2')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the histograms overlap.  \n",
    "the naive way sucks, as naive ways do.  \n",
    "Lets see what Fisher has to offer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define\n",
    "$$\n",
    "\\arrthree{\n",
    "m_k &= \\wt \\mb_k\n",
    "\\\\\n",
    "y_n &= \\wt \\xbn\n",
    "}\n",
    "$$\n",
    "The within class variance is\n",
    "$$\n",
    "s_k^2 = \\displaystyle \\sum_{n \\in \\Ca_k} (y_k - m_k)^2\n",
    "$$\n",
    "\n",
    "hence, total within-class variance is given by $s_1^2 + s_2^2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Fisher's criterion can be defined as the ratio of the between-class variance to the total within-class variance. That is\n",
    "$$\n",
    "\\Jb(\\wb) = \n",
    "\\frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}\n",
    "=\n",
    "\\frac{\\wt \\Sb_B \\wb}{\\wt \\Sb_W \\wb}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\arrthree{\n",
    "\\Sb_B &= (\\mb_2 - \\mb_1)(\\mb_2 - \\mb_1)^T \\\\\n",
    "\\Sb_W &= \n",
    "\\displaystyle \\sum_{n \\in \\Ca_1} (\\xbn - \\mb_1)(\\xbn - \\mb_1)^T\n",
    "+ \\displaystyle \\sum_{n \\in \\Ca_2} (\\xbn - \\mb_2)(\\xbn - \\mb_2)^T\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\frac{d\\Jb}{\\dwb} &= 0\\\\\n",
    "\\frac{2 \\Sb_B \\wb}{\\wt \\Sb_W \\wb}\n",
    "- (\\wt \\Sb_B \\wb) \\fracone{(\\wt \\Sb_W \\wb)^2} 2 \\Sb_W \\wb\n",
    "&= 0\n",
    "\\\\\n",
    "(\\wt \\Sb_B \\wb) (2 \\Sb_W \\wb) &= (\\wt \\Sb_W \\wb)(2 \\Sb_B \\wb)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Sb_B = (\\mb_2 - \\mb_1)(\\mb_2 - \\mb_1)^T$, we have $\\Sb_B \\wb$ in the direction of $(\\mb_2 - \\mb_1)$  \n",
    "Also $\\wt \\Sb_B \\wb$ and $\\wt \\Sb_W \\wb$ are scalars.  \n",
    "Hence\n",
    "$$\n",
    "\\wb \\propto \\inv{\\Sb_W} (\\mb_2 - \\mb_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sw(x, m):\n",
    "    assert((x[0,:].shape == m.shape))\n",
    "    d = x.shape[1]\n",
    "    sw = np.zeros((d,d))\n",
    "    for xx in x:\n",
    "        diff = (xx - m).reshape(-1,1)\n",
    "        sw = sw + diff @ diff.T\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1_aug[:,1],x1_aug[:,2],'.r')\n",
    "plt.plot(x2_aug[:,1],x2_aug[:,2],'.b')\n",
    "\n",
    "# find m1 and m2\n",
    "m1 = np.average(x1_aug, axis=0)\n",
    "m2 = np.average(x2_aug, axis=0)\n",
    "sw_1 = compute_sw(x1_aug[:,1:], m1[1:])\n",
    "sw_2 = compute_sw(x2_aug[:,1:], m2[1:])\n",
    "swi = np.linalg.inv(sw_1 + sw_2)\n",
    "w = swi @ (m2-m1)[1:].reshape(-1,1)\n",
    "print('w: ',w.T)\n",
    "\n",
    "plt.plot(m1[1],m1[2], 'or', MarkerSize=10, label='m1')\n",
    "plt.plot(m2[1],m2[2], 'ob', MarkerSize=10, label='m2')\n",
    "plt.plot((m1[1],m2[1]),(m1[2],m2[2]), 'm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_aug = np.insert(w, 0, 1)\n",
    "projections1 = [w_aug @ x for x in x1_aug]\n",
    "projections2 = [w_aug @ x for x in x2_aug]\n",
    "plt.hist(projections1,color='r', alpha=0.5, label='class 1')\n",
    "plt.hist(projections2,color='b', alpha=0.5, label='class 2')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't this cool. there is a clear separation and we won. Now how do we determine the threshold?  \n",
    "Find the min and max of both the projections. This gives us two ranges, range1 and range2.  \n",
    "Sort the ranges $r_i$ by the min vals.  \n",
    "If there are nonoverlapping, then if $\\max(r_{i-1}) \\le \\min(r_{i}))$ for $i \\in [1, N-1]$, then we are good"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# gimme an array of tuples of form (min, max)\n",
    "def are_ranges_nonoverlapping(ranges):\n",
    "    # key is redundant since tuples are compared position by position\n",
    "    # but then again, cant resist showing offs\n",
    "    thresholds = []\n",
    "    ranges_s = sorted(ranges, key=itemgetter(1))\n",
    "    for ix in range(1,len(ranges_s)):\n",
    "        # max(r_{i-1}) > min(r_{i})\n",
    "        if ranges_s[ix-1][1] > ranges_s[ix][0]:\n",
    "            return (False, None)\n",
    "        else:\n",
    "            thresholds.append((ranges_s[ix][0]+ranges_s[ix-1][1])/2.)\n",
    "    return (True, thresholds)\n",
    "def form_ranges(projectionss):\n",
    "    return [(min(projections), max(projections)) for projections in projectionss]\n",
    "are_ranges_nonoverlapping(form_ranges([projections2, projections1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's and Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically they are the same. so fuck off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher's for multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "* K > 2\n",
    "* D > K\n",
    "\n",
    "Introductin $D^{\\prime} > 1$ linear features $y_k = \\wb_k^T \\xb$ where $k = 1, \\cdots, D^{\\prime}$. That is,\n",
    "$$\n",
    "\\mat{y_1 \\\\ \\vdots \\\\ y_{D^{\\prime}}} = \n",
    "\\mat{\\wb_1^T \\xb \\\\ \\vdots \\\\ \\wb_{D^{\\prime}}^T \\xb} = \n",
    "\\mat{-\\wb_1^T- \\\\ \\vdots \\\\ -\\wb_{D^{\\prime}}^T-} \\xb = \n",
    "\\Wt \\xb\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the within class covar\n",
    "$$\n",
    "\\arrthree{\n",
    "\\Sb_W &= \\sumkK \\Sb_k \\\\\n",
    "\\text{where }\n",
    "\\Sb_k &= \\displaystyle \\sum_{n \\in \\Ca_k} (\\xb - \\mb_k) (\\xb - \\mb_k)^T \\\\\n",
    "\\mb_k &= \\fracrec{N_k} \\displaystyle \\sum_{n \\in \\Ca_k} \\xb_n\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COnsider the total covar \n",
    "$$\n",
    "\\arrthree{\n",
    "\\Sb_T &= \\sumnN (\\xb_n - \\mb) (\\xb_n - \\mb)^T \\\\\n",
    "\\text{where }\n",
    "\\mb &= \\fracrec{N} \\sumnN \\xb_n = \\fracrec{N} N_k \\mb_k\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna prove $\\Sb_T = \\Sb_W + \\Sb_B$ where\n",
    "$$\n",
    "\\Sb_B = \\sumkK N_k (\\mb_k - \\mb) (\\mb_k - \\mb)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\Sb_W + \\Sb_B &=\n",
    "\\sumkK \n",
    "\\Brace{ N_k (\\mb_k - \\mb) (\\mb_k - \\mb)^T + \\sum_{n \\in \\Ca_k} (\\xb_n - \\mb_k) (\\xb_n - \\mb_k)^T}\n",
    "\\\\ &=\n",
    "\\sumkK  \\sum_{n \\in \\Ca_k} \n",
    "(\\mb_k - \\mb) (\\mb_k - \\mb)^T + (\\xb_n - \\mb_k) (\\xb_n - \\mb_k)^T\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\mk}{\\mb_k}\n",
    "\\newcommand{\\mkt}{\\mk^T}\n",
    "\\newcommand{\\mt}{\\mb^T}\n",
    "\\newcommand{\\xn}{\\xb_n}\n",
    "\\newcommand{\\xnt}{\\xn^T}\n",
    "\\newcommand{\\sumnck}{\\sum_{n \\in \\Ca_k}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\overbrace{\\xn \\xnt}^{1} - \\overbrace{\\xn \\mkt}^{2} - \\overbrace{\\mkt \\xnt}^{3} + \\overbrace{\\mk \\mkt}^{4} \\\\\n",
    "\\underbrace{\\mk \\mkt}_{5} - \\underbrace{\\mk \\mt}_{6} - \\underbrace{\\mb \\mkt}_{7} + \\underbrace{\\mb \\mt}_{8}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "1 &= \\sumnN \\xn \\xnt \\\\\n",
    "2,3 &= -2\\sumkK \\sumnck \\xn \\mkt &= -2\\sumkK N_k \\mk \\mkt \\\\\n",
    "4,5 &= +2\\sumkK \\sumnck \\mk \\mkt &= +2\\sumkK N_k \\mk \\mkt \\\\\n",
    "6 &= -\\sumkK \\sumnck \\mk \\mt &= -\\mt \\sumkK \\sumnck \\mk & = -\\mt \\sumnN \\xn \\\\\n",
    "7 &= -\\mb \\sumnN \\xt \\\\\n",
    "8 &= \\sumnN \\mb \\mt \n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus\n",
    "$$\n",
    "\\Sb_W + \\Sb_B = \\sumnN\n",
    "\\Brace{\\xn \\xnt - \\mt \\xn \\mb \\xnt + \\mb \\mt} = \\sumnN (\\xn - \\mb) (\\xn - \\mb)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These covar matrices are defined in the **x**-space. We can define similar matrices in $D^{\\prime}$ **y**-space\n",
    "$$\n",
    "\\arrthree{\n",
    "\\sb_W &= \\sumkK \\sum_{n \\in \\Ca_k} (\\yb_n - \\mub_k) (\\yb_n - \\mub_k)^T \\\\\n",
    "\\sb_B &= \\sumkK N_k (\\mub_k - \\mub) (\\mub_k - \\mub)^T \\\\\n",
    "\\text{where} \\\\\n",
    "\\mub_k &= \\fracrec{N_k} \\displaystyle \\sum_{n \\in \\Ca_k} \\yb_n \\\\\n",
    "\\mub &= \\fracrec{N} \\sumkK N_k \\mub_k\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to formulate a score which is large when $\\sb_B$ is large and $\\sb_W$ is small. One such criteria is\n",
    "$$\n",
    "\\Jb(\\Wb) = \\traceb{\\inv{\\sb_W} \\sb_B}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one important result that is common to all such criteria.  \n",
    "\n",
    "* $\\Sb_B$ is composed of the sum of K matrices, each of which is an outer product of two vectors and therefore of rank 1. \n",
    "* Only (K − 1) of these matrices are independent as a result of the constraint $\\mb = \\fracrec{N} \\sumkK N_k \\mb_k$.\n",
    "* Thus, $\\Sb_B$ has rank at most equal to (K − 1) and so there are at most (K − 1) nonzero eigenvalues.\n",
    "* Hence, the projection onto the (K−1)-dimensional subspace spanned by the eigenvectors of $\\Sb_B$ does not alter the value of **J(W)**\n",
    "* That is, we can't find more than (K − 1) linear \"features\" by this means \\citeme(Fukunaga, 1990)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two class model in which **x** is transformed by $\\phi(\\xb)$ and then used to construct a generalized linear model of the form\n",
    "$$\n",
    "y(\\xb) = f(\\wt \\phi(\\xb)) \n",
    "$$\n",
    "where f(.) is given by\n",
    "$$\n",
    "f(a) = \\begin{cases}\n",
    "+1 & a \\ge 0 \\\\\n",
    "-1 & a \\lt 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate error function  \n",
    "if **x** is in class 1, $\\wt \\phi(\\xb_n) \\gt 0$ else it is < 0.   \n",
    "That is $t_n \\wt \\phi(\\xb_n) > 0$ for correctly classified points and negative for misclassified points.\n",
    "\n",
    "$$\n",
    "E_P(\\wb) = - \\displaystyle \\sum_{n \\in \\M} \\wt \\phi(\\xb_n) t_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent  \n",
    "$$\n",
    "\\wb^{\\tau + 1} = \\wb^{\\tau} - \\eta \\nabla E_P(\\wb)\n",
    "= \\wb^{\\tau} + \\eta \\phi(\\xb_n) t_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of single update\n",
    "\n",
    "$$\n",
    "-\\wb^{(\\tau+1)T} \\phi(\\xb_n) t_n = \n",
    "- \\wb^{(\\tau) T} \\phi(\\xb_n) t_n - \\eta (\\phi(\\xb_n) t_n)^T (\\phi(\\xb_n) t_n)\n",
    "< - \\wb^{(\\tau) T} \\phi(\\xb_n) t_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence\n",
    "\n",
    "Perceptron convergence theorem  \n",
    "\\citeme{Rosenblatt (1962), Block (1962), Nilsson (1965), Minsky and Papert (1969), Hertz et al. (1991), and Bishop (1995a).}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "\n",
    "* Convergence achievable and slow or unachievable?\n",
    "* Solution depends on initialization and order of points\n",
    "* Outputs not probabilistic\n",
    "* doesn't generalize well for K>2 classes\n",
    "* Fixed set of basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron w/o stoppage"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original line\n",
    "a, b = np.matrix((1, -2)), 3.0\n",
    "x = range(10)\n",
    "# <a,pt> = b\n",
    "# => a_0*x + a_1*y = b\n",
    "# => y = (b - a_0*x) / a_1\n",
    "y = (-b - x * a[0, 0])*1.0 / a[0, 1]\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random points\n",
    "val_min = min(min(x), min(y))\n",
    "val_max = max(max(x), max(y))\n",
    "val = max(abs(val_min), abs(val_max))\n",
    "n_random = 20\n",
    "data = np.random.rand(2, n_random) * (2 * val) - val\n",
    "vals_pts = a * data + b\n",
    "ixs_pos = np.where(vals_pts >= 0)[1]\n",
    "ixs_neg = np.where(vals_pts < 0)[1]\n",
    "in_pos, in_neg = ixs_pos.shape[0], ixs_neg.shape[0]\n",
    "plt.scatter(data[0, ixs_pos], data[1, ixs_pos], c=['0'] * in_pos)\n",
    "plt.scatter(data[0, ixs_neg], data[1, ixs_neg], c=['1'] * in_neg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "## init weight\n",
    "# perceptron learning algo\n",
    "offset = 10\n",
    "# to speed up comparison\n",
    "signs_pts = np.sign(vals_pts)\n",
    "T = 2000\n",
    "w = np.matrix((1, 1, -11))\n",
    "max_p = int(np.max(data))\n",
    "min_p = int(np.min(data))\n",
    "print('max: {0} min: {1}'.format(max_p, min_p))\n",
    "# add 1's as the first entry of each point\n",
    "pts = np.vstack([np.ones(n_random), data])\n",
    "in_discrepancies_all = []\n",
    "in_disc_best, w_best = 1000.0, None\n",
    "for t in range(T):\n",
    "    # show w\n",
    "    x = range(min_p-1, max_p+1)\n",
    "    y = (-w[0,0] - x * w[0,1]) * 1.0 / w[0,2]\n",
    "    #plt.plot(x, y, color = str(t*1.0/T))\n",
    "    \n",
    "    # find the vals\n",
    "    vals_t = w * pts\n",
    "    ixs_discrepancies = np.where(signs_pts != np.sign(vals_t))[1]\n",
    "    in_discrepancies = ixs_discrepancies.shape[0]\n",
    "    in_discrepancies_all.append(in_discrepancies)\n",
    "    \n",
    "    if (in_discrepancies < in_disc_best):\n",
    "        in_disc_best, w_best = in_discrepancies, w\n",
    "    # if there are no discrepencies, the get the fuck lost\n",
    "    if (in_discrepancies == 0):\n",
    "        break\n",
    "    \n",
    "    ix_pt = ixs_discrepancies[0]\n",
    "    # update w\n",
    "    w = w + signs_pts[0, ix_pt] * pts[:, ix_pt]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the points\n",
    "plt.scatter(data[0, ixs_pos], data[1, ixs_pos], c=['0'] * in_neg)\n",
    "plt.scatter(data[0, ixs_neg], data[1, ixs_neg], c=['1'] * in_neg)\n",
    "# plot the actual line\n",
    "y_actual = (-b - x * a[0, 0]) * 1.0 / a[0, 1]\n",
    "plt.plot(x, y_actual, color='g', label='ground truth')\n",
    "# plot the best line found by perceptron\n",
    "y = (-w_best[0,0] - x * w_best[0,1]) * 1.0 / w_best[0,2]\n",
    "plt.plot(x, y, color='r', label='estimated')\n",
    "\n",
    "plt.axis((val_min-offset,val_max+offset,val_min-offset,val_max+offset))\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(in_discrepancies_all)), in_discrepancies_all)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
