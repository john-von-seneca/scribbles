{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization\n",
    "$\n",
    "\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\Bracket}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\cases}[1]{\\begin{cases}#1\\end{cases}}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\diff}[2]{\\frac{d #1}{d #2}}\n",
    "\\newcommand{\\diffn}[2]{{#1}^{\\prime}(#2)}\n",
    "\\newcommand{\\ds}{\\displaystyle}\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\frachalf}[1]{\\frac{#1}{2}}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\fracrec}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\H}[1]{\\mathbb{H}\\left[#1\\right]}\n",
    "\\newcommand{\\Int}[2]{\\displaystyle \\int_{#1}^{#2}}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\invp}[1]{\\left({#1}\\right)^{-1}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left(#1 \\Vert #2\\right)}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Ln}[1]{\\ln \\left\\(#1\\right\\)}\n",
    "\\newcommand{\\Lnb}[1]{\\ln \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\Mod}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\newcommand{\\Normsqr}[1]{\\Norm{#1}^2}\n",
    "\\newcommand{\\map}[1]{#1_{\\text{MAP}}}\n",
    "\\newcommand{\\ml}[1]{#1_{\\text{ML}}}\n",
    "\\newcommand{\\MI}[1]{\\mathcal{I}\\left(#1\\right)}\n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\Paran}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\Partial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\sqrbrkt}[1]{\\Bracket{#1}^2}\n",
    "\\newcommand{\\sqrbrc}[1]{\\Brace{#1}^2}\n",
    "\\newcommand{\\trace}[1]{\\text{Tr}\\left( #1 \\right)}\n",
    "\\newcommand{\\traceb}[1]{\\text{Tr}\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "$\n",
    "$\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\mat}[1]{ \\left[ \\begin{matrix} #1 \\end{matrix} \\right] }\n",
    "\\newcommand{\\matp}[1]{ \\left( \\begin{matrix} #1 \\end{matrix} \\right)}\n",
    "\\newcommand{\\mats}[1]{ \\begin{matrix}#1\\end{matrix} }\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr} #1 \\end{array}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Ca}{\\mathcal{C}}\n",
    "\\newcommand{\\Caone}{\\Ca_1}\n",
    "\\newcommand{\\Catwo}{\\Ca_2}\n",
    "\\newcommand{\\Cak}{\\Ca_k}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Ra}{\\mathcal{R}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumi}{\\displaystyle \\sum_i}\n",
    "\\newcommand{\\sumiD}{\\displaystyle \\sum_{i=1}^{D}}\n",
    "\\newcommand{\\sumiL}{\\displaystyle \\sum_{i=1}^{L}}\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodi}{\\displaystyle \\prod_i}\n",
    "\\newcommand{\\prodiD}{\\displaystyle \\prod_{i=1}^{D}}\n",
    "\\newcommand{\\prodiL}{\\displaystyle \\prod_{i=1}^{L}}\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet shortcuts\n",
    "$\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\At}{\\Ab^T}\n",
    "\\newcommand{\\Ai}{\\inv{\\Ab}}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\bt}{\\bb^T}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\fb}{\\mathbf{f}}\n",
    "\\newcommand{\\fp}{f^{\\prime}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\hx}{h(\\xb)}\n",
    "\\newcommand{\\Jb}{\\mathbf{J}}\n",
    "\\newcommand{\\Kb}{\\mathbf{K}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lt}{\\Lb^T}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\mt}{\\mb^T}\n",
    "\\newcommand{\\mbn}{\\mb_N}\n",
    "\\newcommand{\\mbnt}{\\mbn^T}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\sb}{\\mathbf{s}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\tt}{\\tb^T}\n",
    "\\newcommand{\\Tb}{\\mathbf{T}}\n",
    "\\newcommand{\\Tt}{\\Tb^T}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "\\newcommand{\\Ut}{\\Ub^T}\n",
    "\\newcommand{\\vb}{\\mathbf{v}}\n",
    "\\newcommand{\\Vb}{\\mathbf{V}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wnr}[1]{\\wb^{(\\text{#1})}}\n",
    "\\newcommand{\\wt}{\\wb^T}\n",
    "\\newcommand{\\Wb}{\\mathbf{W}}\n",
    "\\newcommand{\\Wt}{\\Wb^T}\n",
    "\\newcommand{\\Wtilde}{\\widetilde{\\Wb}}\n",
    "\\newcommand{\\Wtildet}{\\Wtilde^T}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\Xtilde}{\\widetilde{\\Xb}}\n",
    "\\newcommand{\\Xtildet}{\\Xtilde^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xt}{\\xb^T}\n",
    "\\newcommand{\\xtilde}{\\widetilde{\\xb}}\n",
    "\\newcommand{\\xtilden}{\\xtilde_n}\n",
    "\\newcommand{\\xtildent}{\\xtilden^T}\n",
    "\\newcommand{\\xp}{x^{\\prime}}\n",
    "\\newcommand{\\xbp}{\\xb^{\\prime}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\yx}{y(\\xb)}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zt}{\\zb^T}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math shortcuts\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\Lambdab}{\\pmb{\\Lambda}}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\Li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\mut}{\\mub^T}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "\\newcommand{\\Phibt}{\\Phib^T}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\sigmasqr}{\\sigma^2}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\Sigmai}{\\inv{\\Sigma}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\zerob}{\\pmb{0}}\n",
    "\\newcommand{\\ed}{\\mathbb{E}_{\\D}}\n",
    "\\newcommand{\\edyx}{\\ed\\left[y(\\xb ; \\D)\\right]}\n",
    "\\newcommand{\\dx}{~dx}\n",
    "\\newcommand{\\dxb}{~d\\xb}\n",
    "\\newcommand{\\pxdxb}{p(\\xb) \\dxb}\n",
    "\\newcommand{\\dwb}{~d\\wb}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of choosing a class-conditional distribution $p(\\xb \\mid \\Ca_k)$ and finding the prior $p(\\Ca_k)$ and computing the class posterior $p(\\Ca_k \\mid \\xb)$, we can model the class posterior directly.\n",
    "\n",
    "Advantages:  \n",
    "* Fewer adaptive parameters\n",
    "* May lead to improved predictive perf, when class-condional densities give a poor approximation to the true distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There might be a significant overlap between class-conditional densities $p(\\xb \\mid \\Ca_k)$.  \n",
    "* This leads to the posterior $p(\\Ca_k \\mid \\xb)$ neither zero nor 1\n",
    "* In such cases, we gotta use decision theory\n",
    "\n",
    "* $\\phi$ can exacerbate the overlap or create new overlaps when none existed in the original input space.\n",
    "* still, suitable choice of $\\phi$ can make the process of modelling the posteriors easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Sigmoid\n",
    "\n",
    "$$\n",
    "p(\\Caone \\mid \\phi) = y(\\phi) = \\sigma(\\wt \\phi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For M-dimensional feature space $\\phi$, this has M adjustable parameters\n",
    "* if we used Gaussian, we have\n",
    "  * M parameters for each class mean\n",
    "  * M(M+1)/2 parameters for the (shared) covariance matrix\n",
    "  * in total, $O(M^2)$, that is, quadratic.\n",
    "* log reg has just M parameters for each class and thats it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML for log reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\sigma(a) = \\invp{1 + \\EXP{-a}}$, we have\n",
    "$$\n",
    "\\arrthree{\n",
    "\\diff{\\sigma}{a}\n",
    "&= -\\Bracket{1 + \\EXP{-a}}^{-2} (- \\EXP{-a}) \\\\\n",
    "&= \\fracrec{1 + \\EXP{-a}} \\frac{\\EXP{-a}}{1 + \\EXP{-a}} \\\\\n",
    "\\diff{\\sigma}{a}\n",
    "&= \\sigma(a) (1 - \\sigma(a))\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given\n",
    "* a data set $\\Brace{\\phi_n, t_n}_{n=1}^{N}$\n",
    "* $\\phi_n = \\phi(\\xbn)$\n",
    "* $\\tb = \\Paran{t_1, \\cdots, t_N }^T$\n",
    "* $y_n = p(\\Caone \\mid \\phi_n) = \\sigma(a_n)$\n",
    "* $a_n = \\wt \\phi_n$\n",
    "\n",
    "The likelihood takes the form\n",
    "$$\n",
    "p(\\tb \\mid \\wb) = \\prodnN y_n^{t_n} \\Brace{1-y_n}^{1-t_n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross entropy error\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "E(\\wb) &=\n",
    "- \\ln p(\\tb \\mid \\wb)\n",
    "\\\\ &=\n",
    "- \\sumnN \\Brace{\n",
    "t_n \\ln y_n + (1 - t_n) \\ln (1 - y_n)\n",
    "}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the error fucntion wrt **w** is\n",
    "$$\n",
    "\\arrthree{\n",
    "\\nabla E(\\wb) &=\n",
    "-\\sumnN\n",
    "\\frac{t_n}{y_n} \\Paran{y_n(1-y_n)} \\phi_n\n",
    "- \\frac{1-t_n}{1-y_n} \\Paran{y_n(1-y_n)} \\phi_n\n",
    "\\\\ &=\n",
    "-\\sumnN\n",
    "t_n (1 - y_n) \\phi_n - (1-t_n)y_n \\phi_n\n",
    "\\\\ &=\n",
    "\\sumnN (y_n - t_n) \\phi_n\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The contribution to the gradient from data point n is given by the \"error\" $y_n − t_n$ between the actual and predicted, times $\\phi_n$. \n",
    "* Furthermore, this takes precisely the same form as the gradient of the sum-of-squares error function for the linear regression model.\n",
    "* This also yields well to a sequential algo in which\n",
    "$$\n",
    "\\nabla E(\\wb) = \\sumnN \\nabla E_n(\\wb) \\\\\n",
    "\\text{ where } \\nabla E_n(\\wb) = (y_n - t_n) \\phi_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trouble\n",
    "\n",
    "* severe overfitting for linearly separable data\n",
    "* coz ML solution corr to $\\sigma = 0.5$ which leads to\n",
    "$$\n",
    "\\arrthree{\n",
    "\\fracrec{1 + \\exp(-a)} &= \\half \\\\\n",
    "1 + \\exp(-a) &= 2 \\\\\n",
    "\\exp(-a) &= 1 \\\\\n",
    "a &= 0 \\\\\n",
    "\\wt \\phi &= 0\n",
    "}\n",
    "$$\n",
    "* this leads to **w** to infinity [??]\n",
    "* this leads to $\\sigma$ becoming too infinitely steep in $\\phi$-space\n",
    "  * called a Heaviside step function\n",
    "* there exist a continuum of solution and the choice is arbitrary\n",
    "  * choice of the optimization algo\n",
    "  * parameter init\n",
    "* such problems arise even if $N \\gg M$\n",
    "* can be avoided by\n",
    "  * inclusion of prior and find a MAP solution\n",
    "  * (equivalently) adding a regularization term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRLS\n",
    "\n",
    "[IRLS](pdm-irls.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in lin regression, the ml solution has a closed form coz of gaussian noise model\n",
    "  * consequence of the quadratic dependence of the log likelihood on **w**\n",
    "* in log reg, there is no closed form since $\\sigma$ is nonlinear\n",
    "* but the error function is convex and hence has a unique min\n",
    "* the error fucntion can be minimized by iterative tech based on newton-raphson iterative optimization scheme.\n",
    "  * NR uses a local quadratic approximation of the objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass logistic regression using generative models, the class posterior is given by softmax function,\n",
    "$$\n",
    "p(\\Ca_k \\mid \\phi) =\n",
    "y_k(\\phi) = \n",
    "\\frac\n",
    "{\\exp(a_k)}\n",
    "{\\sum_j ~\\exp(a_j)}\n",
    "\\\\\n",
    "\\text{where } \n",
    "a_k = \\wb_k^T \\phi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generative models, we used ML to find the class-conditional densities and class priors and then used Bayes' theorem to find the class posteriors. This lead to the determination of $\\Brace{\\wb_k}$\n",
    "\n",
    "Here, we use ML to find $\\Brace{\\wb_k}$ directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that\n",
    "$$\n",
    "\\arrthree{\n",
    "\\ds\\Partial{y_k}{a_j} &=\n",
    "\\ds \\Partial{}{a_j}\n",
    "\\frac{\\exp(a_k)}{\\sum_i ~\\exp(a_i)}\n",
    "\\\\ &=\n",
    "\\ds\n",
    "\\I_kj \\frac{\\exp(a_k)}{\\sum_i ~\\exp(a_i)}\n",
    "+\n",
    "\\exp(a_k) \\frac{-1}{\\Brace{\\sum_i ~\\exp(a_i)}^2} \\exp(a_j)\n",
    "\\\\ &=\n",
    "\\ds\n",
    "\\I_{kj} y_k -\n",
    "\\frac{\\exp(a_k)}{\\sum_i ~\\exp(a_i)}\n",
    "\\frac{\\exp(a_j)}{\\sum_i ~\\exp(a_i)}\n",
    "\\\\ &=\n",
    "y_k (\\I_{kj} - y_j)\n",
    "}\n",
    "$$\n",
    "Also, define $y_{nk} = y_k(\\phi_n)$ and let\n",
    "$$\n",
    "\\Tb = \\mat{\\vdots \\\\ -\\tb_n^T- \\\\ \\vdots}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the target vector $\\tb_n$ using 1-of-K coding scheme. Then the likelihood is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\Tb \\mid \\Brace{\\wb_k})\n",
    "&= \\prodnN \\prodkK p(\\Cak \\mid \\phi_n)^{t_{nk}}\n",
    "\\\\ &=\n",
    "\\prodnN \\prodkK y_{nk}^{t_{nk}}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the negative log, we get\n",
    "$$\n",
    "\\arrthree{\n",
    "E(\\Brace{\\wb_k})\n",
    "&=\n",
    "- \\sumnN \\sumkK t_{nk} ~\\ln ~y_{nk}\n",
    "}\n",
    "$$\n",
    "This is called the *cross-entropy* error function for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the gradient wrt $\\wb_j$, we get\n",
    "$$\n",
    "\\arrthree{\n",
    "\\nabla_{\\wb_j}E\\Paran{\\Brace{\\wb_k}}\n",
    "&=\n",
    "- \\sumnN \\sumkK\n",
    "\\frac{t_{nk}}{y_{nk}} ~\\diff{y_{nk}}{a_j} ~\\diff{a_j}{w_k}\n",
    "\\\\ &=\n",
    "- \\sumnN \\sumkK\n",
    "\\frac{t_{nk}}{y_{nk}} ~y_{nk}(\\I_{kj} - y_{nj}) ~\\phi_n\n",
    "\\\\ &=\n",
    "- \\sumnN \\sumkK t_{nk} ~(\\I_{kj} - y_{nj}) ~\\phi_n\n",
    "\\\\ &=\n",
    "- \\sumnN \\sumkK t_{nk} \\I_{kj} \\phi_n + \\sumnN y_{nj} ~\\phi_n \\sumkK t_{nk} \n",
    "\\\\ &=\n",
    "\\\n",
    "\\sumnN (y_{nj} - t_{nj}) ~\\phi_n\n",
    "}\n",
    "$$\n",
    "since $\\sum_k t_{nk} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same form that arose in\n",
    "* sum-of-squares error function with linear model\n",
    "* cross-entropy error for log reg model\n",
    "Also, this can be easily formulated for a sequential algo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The derivative of the log likelihood for linear regression wrt **w** \n",
    "* combination of logistic sigmoid withe cross-entropy error function\n",
    "* softmax activation with multiclass cross-entropy error function\n",
    "\n",
    "All these have the form of the \"error\" $(y_n - t_n)$ times the feature vector $\\phi_n$.  \n",
    "These are examples of a general result for exponential family.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch algo requires the eval of Hessian in blocks of size $M \\times M$\n",
    "refer text (4.3.4)[210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probit regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a broad range of class-conditional distributions described by the exponential family, the resulting posterior are given by logistic of softmax transformation acting on a linear fucntion of features  \n",
    "\n",
    "But not all choices of class-conditionals lead to a simple form for the posterior. It is importnat to worth exploring other types of discriminative probabilistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the 2-class case.  \n",
    "$$\n",
    "p(t = 1 \\mid a) = f(a)\n",
    "$$\n",
    "where\n",
    "* $a = \\wt \\phi$\n",
    "* f(.) is the activation function.\n",
    "  * $\\inv{f}$ is called the link function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the alternate chioces for the link function is to use a noisy threshold model.  \n",
    "For each $\\phi_n$, we eval $a_n = \\wt \\phi_n$ and set t to\n",
    "$$\n",
    "t_n = \\cases{\n",
    "1 & \\text{if } a_n \\ge \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value of $\\theta$ is drawn from a probablity distribution, *p*, then the corresponding activation function f, is given by the cumulative distribution function\n",
    "$$\n",
    "f(a) = \\Int{-\\infty}{a} ~p(\\theta) ~d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yo, Probit \n",
    "Suppose the density is given by the standard Gaussian. Then the cdf becomes\n",
    "$$\n",
    "\\Phi(a) = \\Int{-\\infty}{a} ~\\Nl{\\theta}{0}{1} ~d\\theta\n",
    "$$\n",
    "which is called the **probit** function and has the same shape as the logistic sigmoid function.  \n",
    "\n",
    "The use of a more general Gaussian doesnt change the model since this is equivalent to rescaling of **w**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical packages provide the evaluation of a closely related function called **erf function** or **error function**, which is\n",
    "$$\n",
    "\\text{erf}(a) = \\frac{2}{\\sqrt{\\pi}} \\Int{0}{a} ~\\EXP{-\\theta^2} ~d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## erf and probit\n",
    "\n",
    "This is related to the probit function as\n",
    "$$\n",
    "\\arrthree{\n",
    "\\Phi(a) &= \\Int{-\\infty}{a} ~\\Nl{\\theta}{0}{1} ~d\\theta\n",
    "\\\\ &=\n",
    "\\half + \\fracrec{\\sqrt{2\\pi}} ~\\Int{0}{a} \\EXP{-x^2/2}  \\dx\n",
    "\\\\ y = \\frac{x}{\\sqrt{2}}\n",
    "\\\\ \\Phi(a) &=\n",
    "\\half + \\fracrec{\\sqrt{2\\pi}} ~\\Int{0}{a/\\sqrt{2}} ~\\EXP{-y^2} \\sqrt{2} ~dy\n",
    "\\\\ &=\n",
    "\\half + \\fracrec{\\sqrt{\\pi}} ~\\Int{0}{a/\\sqrt{2}} ~\\EXP{-y^2} ~dy\n",
    "\\\\ &=\n",
    "\\half + \\half \\text{erf}(\\frac{a}{\\sqrt{2}})\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized linear model based on probit activation function is known as **probit regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $x \\rightarrow \\infty$,  \n",
    "* log sigmoid decays asymptotocally like $\\exp(-x)$\n",
    "* probit decays like $\\exp(-x^2)$\n",
    "\n",
    "Hence probit is more sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## incorrect labelling\n",
    "\n",
    "let $\\epsilon$ be the probability that the target value t is flipped to the wrong value. Then\n",
    "$$\n",
    "\\arrthree{\n",
    "p(t \\mid x) &= (1 - \\epsilon) \\sigma(x) + \\epsilon(1 - \\sigma(x))\n",
    "\\\\ &=\n",
    "\\epsilon + (1 - 2\\epsilon) \\sigma(x)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical link functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative log likelihood function takes the simple form of \"error\" $y_n - t_n$ times the feature vector $\\phi_n$ where $y_n = \\wt \\phi_n$ for the following models\n",
    "* linear regression with gaussian noise distribution\n",
    "* logistic sigmoid activation function withe cross-entropy error function\n",
    "* softmax activation functions with multiclass cross-entropy error function\n",
    "\n",
    "Thses are general results assuming the conditional distribution for the target value is of exponential family  \n",
    "along with a corresponding choice for the activation function called the **canonical link function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conditional distribution\n",
    "\n",
    "consider the conditional distribution of the target variable of the form\n",
    "$$\n",
    "p(t \\mid \\eta, s) = \\fracrec{s}\n",
    "~h\\Paran{\\frac{t}{s}} ~g(\\eta) ~\\EXP{\\frac{\\eta t}{s}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the result from [ML for exp family](../probability-distributions/4-exponential-family.ipynb#ML-and-Sufficient-statistics), we have the conditional mean of t, which we denote by y is given by\n",
    "$$\n",
    "y = \\E{t \\mid \\eta} = \n",
    "-s \\diff{}{\\eta} ~\\ln ~g(\\eta)\n",
    "$$\n",
    "\n",
    "Hence y and $\\eta$ must be related and this relation is denoted as\n",
    "$$\n",
    "\\eta = \\psi(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a **generalized linear model** based on \\citeme{Nelder and Wedderburn 1972}\n",
    "$$\n",
    "y = f(\\wt \\phi)\n",
    "$$\n",
    "\n",
    "* f(.) is called **activation function** in machine learning\n",
    "* $\\inv{f}$ is called **link function** in statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log likelihood\n",
    "\n",
    "The log likelihood function of the model is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "\\ln p(\\tb \\mid \\eta, s) &=\n",
    "\\sumnN \\ln p(t_n \\mid \\eta, s) \n",
    "\\\\ &=\n",
    "\\sumnN \\Brace{\n",
    "\\ln ~g(\\eta_n) + \\frac{\\eta_n ~t_n}{s} \n",
    "} + \\text{const}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption here is that the scale parameter \"s\" is common to all instances and hence independent of n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the log likelihood wrt **w** is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "\\nabla_{\\wb} &=\n",
    "\\sumnN \\Brace{\n",
    "\\diff{}{\\eta_n} \\ln g(\\eta_n) + \\frac{t_n}{s}} \\diff{\\eta_n}{\\wb}\n",
    "}\n",
    "$$\n",
    "\n",
    "We can simplify things further\n",
    "$$\n",
    "\\arrthree{\n",
    "\\diff{\\eta_n}{\\wb} &=\n",
    "\\diff{\\eta_n}{y_n} \\diff{y_n}{\\wb} \n",
    "& \\commentgray{$\\eta = \\psi(y)$}\n",
    "\\\\ &=\n",
    "\\diff{\\eta_n}{y_n} \\diff{y_n}{a_n} \\diff{a_n}{\\wb} \n",
    "& \\commentgray{$a_n = \\wt \\phi_n$}\n",
    "\\\\ &=\n",
    "\\diffn{\\psi}{y_n} ~\\diffn{f}{a_n} ~\\phi_n\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\nabla_{\\wb} \\ln p(\\tb \\mid \\eta, s)&=\n",
    "\\sumnN \\Brace{\n",
    "\\diff{}{\\eta_n} \\ln g(\\eta_n) + \\frac{t_n}{s}} \\diff{\\eta_n}{\\wb}\n",
    "\\\\ &=\n",
    "\\sumnN \\fracrec{s}\\Brace{t_n - y_n} \\diffn{\\psi}{y_n} ~\\diffn{f}{a_n} ~\\phi_n\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to simplify things in hand, we can do this\n",
    "$$\n",
    "\\arrthree{\n",
    "\\inv{f}(y) &= \\psi(y) \\\\\n",
    "\\Rightarrow ~~~~~ y &= f(\\psi(y)) \\\\\n",
    "\\Rightarrow ~~~~~ 1 &= \\diffn{f}{\\psi} \\diffn{\\psi}{y}\n",
    "}\n",
    "$$\n",
    "\n",
    "we also have\n",
    "$$\n",
    "y = f(a) ~~~~~\n",
    "\\Rightarrow ~~~~~ a = \\inv{f}(y)\n",
    "$$\n",
    "\n",
    "Since $\\inv{f}(y) = \\psi(y)$ and $a = \\inv{f}(y)$, we have $a = \\psi$ and hence\n",
    "$$\n",
    "\\diffn{f}{a} \\diffn{\\psi}{y} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hence\n",
    "$$\n",
    "\\nabla_{\\wb} \\ln p(\\tb \\mid \\eta, s) = \n",
    "\\fracrec{s} \\sumnN\n",
    "\\Brace{t_n - y_n} \\phi_n\n",
    "$$\n",
    "\n",
    "hence, the gradient of the error function becomes\n",
    "$$\n",
    "\\nabla \\ln E(\\wb) = \n",
    "\\fracrec{s} \\sumnN \\Brace{y_n - t_n} \\phi_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value of s\n",
    "* for Gaussian, $s = \\inv{\\beta}$\n",
    "* for logistic model, $s = 1$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
