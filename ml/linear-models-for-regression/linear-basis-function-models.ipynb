{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sp_la\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization\n",
    "$\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "\\newcommand{\\H}[1]{\\mathbb{H}\\left[#1\\right]}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\LN}[1]{\\ln\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\invp}[1]{\\left({#1}\\right)^{-1}}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\ml}[1]{#1_{\\text{ML}}}\n",
    "\\newcommand{\\Partial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left(#1 \\Vert #2\\right)}\n",
    "\\newcommand{\\MI}[1]{\\mathcal{I}\\left(#1\\right)}\n",
    "\\newcommand{\\Ln}[1]{\\ln \\left\\(#1\\right\\)}\n",
    "\\newcommand{\\Lnb}[1]{\\ln \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\Mod}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\Bracket}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\trace}[1]{\\text{Tr}\\left( #1 \\right)}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\mat}[1]{ \\left[ \\begin{matrix} #1 \\end{matrix} \\right] }\n",
    "\\newcommand{\\matp}[1]{ \\left( \\begin{matrix} #1 \\end{matrix} \\right)}\n",
    "\\newcommand{\\mats}[1]{ \\begin{matrix}#1\\end{matrix} }\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr} #1 \\end{array}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Ca}{\\mathcal{C}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Ra}{\\mathcal{R}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "\\newcommand{\\Ref}[1]{(\\ref{#1})}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumiD}{\\displaystyle \\sum_{i=1}^{D}}\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodiD}{\\displaystyle \\prod_{i=1}^{D}}\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet shortcuts\n",
    "$\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\At}{\\Ab^T}\n",
    "\\newcommand{\\Ai}{\\inv{\\Ab}}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\bt}{\\bb^T}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\fb}{\\mathbf{f}}\n",
    "\\newcommand{\\fp}{f^{\\prime}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\Jb}{\\mathbf{J}}\n",
    "\\newcommand{\\Kb}{\\mathbf{K}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lt}{\\Lb^T}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\mt}{\\mb^T}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "\\newcommand{\\Ut}{\\Ub^T}\n",
    "\\newcommand{\\vb}{\\mathbf{v}}\n",
    "\\newcommand{\\Vb}{\\mathbf{V}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wt}{\\wb^T}\n",
    "\\newcommand{\\Wb}{\\mathbf{W}}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xt}{\\xb^T}\n",
    "\\newcommand{\\xp}{x^{\\prime}}\n",
    "\\newcommand{\\xbp}{\\xb^{\\prime}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zt}{\\zb^T}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math shortcuts\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\Lambdab}{\\pmb{\\Lambda}}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\Li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\mut}{\\mub^T}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "\\newcommand{\\Phibt}{\\Phib^T}\n",
    "\\newcommand{\\sigmasqr}{\\sigma^2}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\zerob}{\\pmb{0}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Model\n",
    "\n",
    "$y(\\xb,\\wb)  = w_0 + \\sum_{i=1}^{D} w_i x_i$ \n",
    "\n",
    "where  \n",
    "$\\xb = (x_1, \\cdots, x_D)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Key property of this model is that it is linear in parameters $w_0, \\cdots, w_D$\n",
    "* But this is also linear in $\\xb$ which imposes a significant restriction on the model.\n",
    "* So, we consider functions $\\phi_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis Functions\n",
    "\n",
    "$\n",
    "y(\\mathbf{x},\\mathbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_i ~ \\phi_j(\\mathbf{x})\n",
    "$ \n",
    "\n",
    "* where $\\phi_j(\\mathbf{x})$ are called basis functions. \n",
    "* Total #parameters = M\n",
    "* The parameter $w_0$ is called the bias parameter\n",
    "\n",
    "\n",
    "If $\\phi_0(\\mathbf{x}) = 1$, then  \n",
    "$y(\\mathbf{x},\\mathbf{w})  = \\sum_{j=0}^{M-1} w_i ~ \\phi_j(\\text{x}) = \\mathbf{w}^T \\phi(\\mathbf{x})$ \n",
    "\n",
    "* Thing to note here is that the model is still linear in **w**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choices\n",
    "\n",
    "1. Polynomial Basis\n",
    "  * $\\phi_j(\\mathbf{x}) = x^j$\n",
    "  * Limitation: Global models\n",
    "  * Cure: Spline Functions: fit different polynomials based on region [EOSL Hastie]\n",
    "1. Gaussian Basis FUnction:\n",
    "  * $\\phi_j(\\mathbf{x}) = \\EXP{-\\frac{(x-\\mu_j)^2}{2s^2}}$\n",
    "  * Need not be a pdf\n",
    "1. Sigmoidal\n",
    "  * $\\phi_j(\\mathbf{x}) = \\sigma \\left( \\frac{x-\\mu_j}{s} \\right)$\n",
    "  * where $\\sigma(a) = \\frac{1}{1+\\EXP{-a}}$ is the logistic sigmoid function\n",
    "  * Can use $\\tanh$, since $\\tanh(a) = 2\\sigma(a) - 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML and Least Squares\n",
    "\n",
    "Let *t* be given by a deterministic function $y(\\xb, \\wb)$ and additive Gaussian noise so that\n",
    "$$\n",
    "t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon$$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$  \n",
    "\n",
    "Then,  \n",
    "<div id='GaussianIidLikelihood'/>\n",
    "$$\n",
    "p(t \\mid \\xb, \\wb, \\beta) = \\mathcal{N}(t \\mid y(\\xb,\\wb), \\beta^{-1})\n",
    "=\n",
    "\\left( \\dfrac{\\beta}{2\\pi} \\right)^{1/2}\n",
    "\\exp\n",
    "\\left\\{\n",
    "-\\dfrac{\\beta}{2} (t - \\wt \\phi(\\xb))^2\n",
    "\\right\\}\n",
    "\\label{eq:ptw}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume a squared loss function, optimal t is given by conditional mean of t. In case of a Gaussian, it becomes\n",
    "$$\n",
    "\\E{t \\mid \\mathbf{x}} = \\int t ~p(t \\mid \\mathbf{x}) ~dt = y(\\mathbf{x},\\mathbf{w})\n",
    "$$\n",
    "\n",
    "[For more info](/notebooks/void-main/introduction/5-decision-theory.ipynb#Loss-functions-for-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $\\mathbf{x}$ is IID Normal given by $(\\ref{eq:ptw})$, then  \n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\mathbf{t} \\mid \\mathbf{X}, \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\prodnN\n",
    "\\Nl{t_n}{\\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x_n})}{\\beta^{-1}}\n",
    "\\\\ &=\n",
    "\\left( \\frac{\\beta}{2\\pi} \\right)^{N/2}\n",
    "\\prodnN\n",
    "\\EXP{-\\frac{\\beta}{2} (t_n - \\wt \\phi(\\xb_n))^2}\n",
    "\\\\ &=\n",
    "\\left( \\frac{\\beta}{2\\pi} \\right)^{N/2}\n",
    "\\EXP{-\\frac{\\beta}{2} \\sumnN  (t - \\wt \\phi(\\xb_n))^2}\n",
    "}\n",
    "\\label{eq:ptiid}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log likehood, we get  \n",
    "\\begin{array}{ll}\n",
    "\\ln p(\\mathbf{t} \\mid \\mathbf{w}, \\beta)\n",
    "&=\n",
    "\\frac{N}{2} \\ln \\beta\n",
    "-\n",
    "\\frac{N}{2} \\ln(2\\pi)\n",
    "- \\beta\n",
    "\\underbrace{\\half\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2}_{E_D(\\wb)}\n",
    "\\end{array}\n",
    "where $E_D(\\wb)$ is the sum-of-squares error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating w\n",
    "\n",
    "Grad of the log likehood (wrt $\\mathbf{w}$) gives,  \n",
    "\\begin{array}{ll}\n",
    "\\nabla \\ln p(\\mathbf{t} | \\mathbf{w}, \\beta)\n",
    "=\n",
    "\\beta\n",
    "\\sumnN\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "&= 0\n",
    "\\\\\n",
    "\\sumnN\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sumnN\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "&= 0\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from strang that in least squares, we always had a data matrix of size $N \\times M$ where \n",
    "* N - Number of data points\n",
    "* M - dimension of each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we let\n",
    "$$\n",
    "\\Phi(\\mathbf{X})\n",
    "=\n",
    "\\mat{\n",
    "\\phi_0(x_1) & \\phi_1(x_1) & \\cdots & \\phi_{M-1}(x_1)\\\\\n",
    "\\phi_0(x_2) & \\phi_1(x_2) & \\cdots & \\phi_{M-1}(x_2)\\\\\n",
    "\\vdots      & \\vdots      & \\ddots & \\vdots\\\\\n",
    "\\phi_0(x_N) & \\phi_1(x_N) & \\cdots & \\phi_{M-1}(x_N)\\\\\n",
    "}\n",
    "=\n",
    "\\mat{\n",
    "\\varphi_0 & \\cdots & \\varphi_{M-1}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, remember, the multiplication of two matrix can be done as a sum of outer products of i-th col of the first matrix with the i-th row of the second matrix. That is\n",
    "$$\n",
    "AB = \n",
    "\\mat{\\ab_1 & \\cdots & \\ab_n}\n",
    "\\mat{\\bb_1^T \\\\ \\vdots \\\\ \\bb_n^T}\n",
    "=\n",
    "\\sumiN \\ab_i \\bb_i^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    "$$\n",
    "\\arrthree{\n",
    "\\sumnN\n",
    "\\mathbf{\\phi}(\\mathbf{x}_n) ~ \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "&=\n",
    "\\Phi(\\mathbf{X})^T \\Phi(\\mathbf{X})\n",
    "\\\\\n",
    "\\sumnN\n",
    "t_n \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "&=\n",
    "\\tb^T \\Phi(\\mathbf{X}) \n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence\n",
    "$$\n",
    "\\arrthree{\n",
    "0 &=\\tb^T \\Phi(\\Xb) - \\ml{\\wb}^T \\Phi(\\Xb)^T \\Phi(\\Xb) \\\\\n",
    "\\mathbf{w}_{ML} &= \\left(\\Phi^T \\Phi \\right)^{-1} \\Phi^T \\mathbf{t}\n",
    "}\n",
    "$$\n",
    "which is called *normal equations* for least squares.  \n",
    "$\\Phi$ is called the *design matrix* which is $N \\times M$ matrix.  \n",
    "Each row of $\\Phi$ is a feature vector transposed $\\phi(x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Parameter\n",
    "---------------\n",
    "\n",
    "Now, consider the last term and seperate out the bias parameter\n",
    "$\\frac{\\beta}{2}\n",
    "\\sumnN\n",
    "\\left(\n",
    "    t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\right)^2$  \n",
    "Diff wrt $w_0$ and equating it to zero, we get  \n",
    "\n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{\\beta}{2}\n",
    "\\sumnN\n",
    "2(t_n - w_0 - \\sum_{m=1}^{M-1} w_m \\phi_m(\\mathbf{x}_n))\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sumnN\n",
    "t_n\n",
    "-\n",
    "\\sumnN\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m \\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "N w_0\n",
    "&=\n",
    "\\sumnN t_n - \\sum_{m=1}^{M-1}\n",
    "w_m \n",
    "\\sumnN\n",
    "\\phi_m(\\mathbf{x}_n)\n",
    "\\\\\n",
    "w_0\n",
    "&=\n",
    "\\overline{t}_n\n",
    "-\n",
    "\\sum_{m=1}^{M-1}\n",
    "w_m\n",
    "\\overline{\\phi}_m\n",
    "\\\\\n",
    "\\text{where}\n",
    "\\\\\n",
    "\\overline{t}_n &= \\frac{1}{N} \\sumnN t_n \\\\\n",
    "\\overline{\\phi}_m &= \\frac{1}{N} \\sumnN \\phi_m(\\mathbf{x}_n) \\\\\n",
    "\\end{array}\n",
    "\n",
    "Thus the basis $w_0$ compensates for the difference between the average of target values and the weighted sum of the average of the basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be thought as follows:\n",
    "* There are two spaces, one of $\\phi$ and one of $t$\n",
    "* Find the average of all $\\phi_n$ and $t_n$\n",
    "* Map $\\phi$ average from $\\phi$ space to $t$ space using $\\wb$\n",
    "* Now the distance between this mapping and average $t_n$ gives the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameter\n",
    "--------------\n",
    "\n",
    "Diff log likelihood wrt $\\beta$ and equating it to zero, we get  \n",
    "\\begin{array}{ll}\n",
    "0\n",
    "&=\n",
    "\\frac{N}{2}\n",
    "\\frac{1}{\\beta}\n",
    "-\n",
    "\\frac{1}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\frac{1}{\\beta_{ML}}\n",
    "&=\n",
    "\\frac{1}{N}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left(\n",
    "    t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "\\right)^2\\\\\n",
    "\\end{array}\n",
    "That is, the inverse of precision is the variance of the target\n",
    "about the regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Learning\n",
    "===========\n",
    "\n",
    "* Stochastic (sequential) Gradient Descent can be used to find the parameters sequentially.\n",
    "* Update rule: $\\mathbf{w}_{\\tau+1} = \\mathbf{w}_{\\tau} + \\eta \\nabla E(\\mathbf{w})$\n",
    "* If squared loss function is assumed,  \n",
    "  $\\nabla E_D(\\mathbf{w}) = \n",
    "  \\left( \n",
    "      t_n - \n",
    "      \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  \\right)\n",
    "  \\mathbf{\\phi}(\\mathbf{x}_n)$\n",
    "* Hence, the update rule becomes,  \n",
    "  $\\mathbf{w}_{\\tau+1}\n",
    "  = \\mathbf{w}_{\\tau}\n",
    "    + \\eta\n",
    "      \\left( \n",
    "          t_n - \n",
    "          \\mathbf{w}_{\\tau}^T \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "      \\right)\n",
    "      \\mathbf{\\phi}(\\mathbf{x}_n)\n",
    "  $\n",
    "* This is called *Least-mean-squares* or *LMS* algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "* Form: $E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Regularization\n",
    "-----------------------\n",
    "\n",
    "* $E_W(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}$\n",
    "* Called as *weight decay* since $\\mathbf{w}$ decays to zero when $\\lambda$ is high\n",
    "* Called as *Parameter Shrinkage* as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Error Function:  \n",
    "\\begin{array}{rlr}\n",
    "E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})\n",
    "&=\n",
    "\\half \\sumnN\n",
    "(t_n - \\mathbf{w}^T \\mathbf{\\phi}(\\mathbf{x}_n)) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+\n",
    "\\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\sumnN t_n \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "-\n",
    "\\mathbf{w}^T \n",
    "\\sumnN \\mathbf{\\phi}(\\mathbf{x}_n) \\mathbf{\\phi}(\\mathbf{x}_n)^T\n",
    "+ \\lambda \\mathbf{w}^T\n",
    "& \\commentgray{Diff wrt w}\n",
    "\\\\\n",
    "0\n",
    "&=\n",
    "\\mathbf{t}^T \\Phi - \\mathbf{w}^T \\Phi^T \\Phi + \\lambda \\mathbf{w}^T\n",
    "\\\\\n",
    "\\left( \\Phi^T \\Phi - \\lambda \\mathcal{I} \\right) \\mathbf{w} \n",
    "&=\n",
    "\\Phi^T \\mathbf{t}\n",
    "\\\\\n",
    "\\mathbf{w}\n",
    "&= \n",
    "\\left( \n",
    "    \\Phi^T \\Phi - \\lambda \\mathcal{I}\n",
    "\\right)^{-1} \n",
    "\\Phi^T \\mathbf{t}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Regularizer\n",
    "---------------------\n",
    "* $\\frac{\\lambda}{2} \\sum_{m=0}^{M-1} \\left| \\mathbf{w_j} \\right|^q$\n",
    "* Allows complex models to be fit on smaller data sets\n",
    "* The problem of finding the right model complexity transforms to that of finding the right value for $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\Mod{(1 - \\Mod{x}^q)^{1/q}} ~~~\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(0,math.pi,10**3)\n",
    "pts = np.asarray([[math.cos(theta),math.sin(theta)] for theta in thetas])\n",
    "# q=1/2\n",
    "def show_regularizer(q=0.5):\n",
    "    ptsq = np.zeros_like(pts)\n",
    "    n = math.floor(ptsq.shape[0])\n",
    "    ys = np.zeros((n,1))\n",
    "    for ix in range(n):\n",
    "        x = pts[ix,0]\n",
    "        y_abs = abs((1 - abs(x)**q)**(1./q))\n",
    "        ys[ix,0] = y_abs\n",
    "    xs = np.hstack((pts[:,0],pts[:,0]))\n",
    "    ys = np.vstack((ys,-ys))\n",
    "    plt.plot(xs, ys, '.', label='q='+str(q))\n",
    "    \n",
    "def interact_regularizer(q=0.5,offset=0.7,show_all=True):\n",
    "    qs = [0.5,1.,2.,4.] if show_all else [q]\n",
    "    [show_regularizer(qq) for qq in qs]\n",
    "    plt.xlim(plt.xlim()[0]-0.1,plt.xlim()[1]+offset)\n",
    "    plt.ylim(plt.ylim()[0]-0.1,plt.ylim()[1]+0.1)\n",
    "    plt.legend()\n",
    "    plt.title('Contours of Reqularizion function')\n",
    "    plt.show()\n",
    "    \n",
    "interact(interact_regularizer,q=(.5,4,.5),offset=(0,1.,.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "* By setting $q=1$ in the generalized regularizer\n",
    "* if $\\lambda$ is set to a large value, some $w_j$'s become zero\n",
    "* This leads to a sparse model, thereby making it a \"feature selector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regularization allows us to use complex models for small data set without overfitting by reducing the model complexity\n",
    "* However, we trade the problem of finding right basis functions to that of finding the optimal value for the regularization parameter $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Outputs\n",
    "\n",
    "* Let\n",
    "  * $\\mathbf{y}(\\mathbf{x},\\mathbf{w})$ be K dimensional\n",
    "  * W be $M \\times K$ matrix of parameters\n",
    "  * $\\mathbf{\\phi}(\\mathbf{x})$ is M dimensional\n",
    "  * $\\mathbf{t}$ is K dimensional\n",
    "\n",
    "\\begin{array}{rlr}\n",
    "p(\\mathbf{t} \\mid \\mathbf{x}, \\mathbf{W}, \\beta)\n",
    "&=\n",
    "\\mathcal{N}\n",
    "\\left(\n",
    "\\mathbf{t} \\mid \\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x}),\n",
    "\\beta^{-1} \\mathcal{I} \n",
    "\\right)\n",
    "\\\\\n",
    "\\ln p = 0\n",
    "&=\n",
    "\\frac{NK}{2} \\ln\\left(\\frac{\\beta}{2\\pi}\\right)\n",
    "-\n",
    "\\frac{\\beta}{2}\n",
    "\\sum_{n=1}^{N}\n",
    "\\left\\|\n",
    "\\mathbf{t}_n\n",
    "-\n",
    "\\mathbf{W}^T \\mathbf{\\phi}(\\mathbf{x})\n",
    "\\right\\|^2\n",
    "&\n",
    "\\color{gray}{\\text{See: Multivariate Gaussian}}\n",
    "\\\\\n",
    "\\mathbf{W}_{ML}\n",
    "&=\n",
    "\\left(\n",
    "\\Phi^T \\Phi\n",
    "\\right)^{-1}\n",
    "\\Phi^T \\mathbf{T}\n",
    "= \n",
    "\\Phi^{\\dagger} \\mathbf{T}\n",
    "\\end{array}\n",
    "* That is, the same $\\Phi^{\\dagger}$ can be used for all K output target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original(x_min, x_max, data):\n",
    "    x_sin = np.array(np.linspace(x_min,x_max,100)).reshape(100,1)\n",
    "    plt.plot(x_sin, \n",
    "             np.array([math.sin(1+xx) for xx in x_sin]).reshape(x_sin.shape),\n",
    "             'k',label='actual',linewidth=9)\n",
    "    plt.plot(data[:,0], data[:,1],'o',c='b', MarkerSize=10,label='input')\n",
    "    return x_sin\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_svd(A):\n",
    "    U, s, Vh = sp_la.svd(A)\n",
    "    s = [1./ss if abs(ss)>1e-10 else 0 for ss in s]\n",
    "    S = np.diag(s)\n",
    "    return Vh.T @ S @ U.T"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = [coeff_const, coeff_deg_1, coeff_deg_2, ...]\n",
    "def lm_poly_fit(x, y, degree, regularizer=None):\n",
    "    # compute the transform from x-space to Z-space\n",
    "    n = x.shape[0]\n",
    "    X = np.ones((n, degree+1))\n",
    "    for ix_power in range(1,degree+1):\n",
    "        X[:,ix_power:ix_power+1] = (x**ix_power).reshape(n,1)\n",
    "    regularizer = math.e**regularizer if regularizer else 0 \n",
    "    XtX = X.transpose() @ X + regularizer*np.eye(degree+1)\n",
    "    XtXi = inv_svd(XtX)\n",
    "    #w = np.linalg.inv(XtX) @ X.transpose() @ y\n",
    "    w = XtXi @ X.transpose() @ y\n",
    "    w = w[:,np.newaxis]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_poly_fit(w, color='r',in_pts=100, x_lims=(0,+6)):\n",
    "    # compute the transform from x-space to Z-space\n",
    "    x_min, x_max = x_lims\n",
    "    x = np.linspace(x_min, x_max, in_pts).reshape(in_pts, 1)\n",
    "    degree = w.shape[0]-1\n",
    "    x_aug = np.ones_like(x)\n",
    "    for ix_degree in range(1,degree+1):\n",
    "        x_aug = np.hstack([x_aug, np.power(x, ix_degree)])\n",
    "    y = x_aug @ w # find the hypothesis val\n",
    "    plt.plot(x, y.reshape(in_pts,1), c=color,linewidth=2,label='degree '+str(degree))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample points from a sine function with noise\n",
    "def gen_data(sigma_noise=1e-5,in_pts=20):\n",
    "    x_min, x_max = 0, +6                            # max x values while sampling\n",
    "    x = np.random.rand(in_pts, 1)*(x_max-x_min)+x_min     # gen random points\n",
    "    x = np.sort(x,0)                                # sort them please\n",
    "    y = np.array([math.sin(1+xx) for xx in x]).reshape(in_pts,1) # find the value of sine\n",
    "    noise = np.random.normal(0, sigma_noise, (in_pts, 1)) # generate random noise\n",
    "    data = np.hstack([x, y+noise]) # compose data using [x y+noise]\n",
    "    return (x_min, x_max, data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_his(data, deg_test, x_vals_plot):\n",
    "    w_his=np.polynomial.polynomial.polyfit(data[:,0],data[:,1], deg=deg_test)\n",
    "    w_his = w_his[::-1]\n",
    "    p2 = np.poly1d(w_his)\n",
    "    plt.plot(x_vals_plot, p2(x_vals_plot),'y',linewidth=4,label='yo')\n",
    "    return w_his"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_norm(w):\n",
    "    return math.sqrt(np.sum(w**2))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max, data = gen_data(10**-2)\n",
    "def show_all_fit_poly(deg_test=5, sigma_noise_power=-5, regularizer=-18, show_non_reg=False):\n",
    "    x_sin = plot_original(x_min, x_max, data)\n",
    "    w_his = fit_and_plot_his(data, deg_test,x_sin)\n",
    "    #[show_poly_fit(lm_poly_fit(data[:,0], data[:,1], ref_deg),'r',100) for ref_deg in [1,2,3]]\n",
    "    diff_my, norm_my = 'None', -1\n",
    "    if show_non_reg:\n",
    "        w_my=lm_poly_fit(data[:,0], data[:,1], deg_test)\n",
    "        show_poly_fit(w_my,'r',100,(x_min, x_max))\n",
    "        w_my = w_my.ravel()\n",
    "        diff_my = np.sum((w_my-w_his[::-1])**2)\n",
    "        norm_my = find_norm(w_my)\n",
    "        \n",
    "    w_my_r=lm_poly_fit(data[:,0], data[:,1], deg_test, regularizer)\n",
    "    show_poly_fit(w_my_r,'g',100, (x_min, x_max))\n",
    "    print('norms', 'w/o reg: ', norm_my,\n",
    "          ' w/ reg: ', find_norm(w_my_r), \n",
    "          ' np :', find_norm(w_his))\n",
    "    print('diff:', 'w/o reg: ', diff_my, ' w/ reg: ', np.sum((w_my_r-w_his[::-1])**2))\n",
    "    plt.legend(loc=(1.1 ,0.5));plt.show()\n",
    "interactive(show_all_fit_poly, deg_test=(5,25),\n",
    "            sigma_noise_power=(-10,10), regularizer=(-20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the fits is coz\n",
    "* The matrix $\\Phib^T \\Phib$ can become rank deficient, which screws up the inverse\n",
    "* We can use SVD to get rid of that.\n",
    "* np.polynomial.polynomial.polyfit takes care of the problem as follows:\n",
    "  > If some of the singular values of V are so small that they are neglected \n",
    "* Hence, it is better to use SVD for the inverse "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_test_inv = np.array([[2,0],[0,2]])\n",
    "print('A\\n',A_test_inv)\n",
    "#print('A**-1\\n',A_test_inv**-1)\n",
    "print('inv(A)\\n',np.linalg.inv(A_test_inv))\n",
    "a=[1,2,3]; a.reverse(); print(a)\n",
    "bb = np.array([1,2,3,4])\n",
    "print('bb:\\n',bb)\n",
    "print('bb:\\n',bb[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_eye = np.eye(4)\n",
    "eye_eye = eye_eye*3\n",
    "eye_eye[1,1] = 0\n",
    "print(eye_eye)\n",
    "#print(sp_la.inv(eye_eye))\n",
    "u, s, Vh = sp_la.svd(eye_eye)\n",
    "s = [1./ss if abs(ss)>1e-10 else 0 for ss in s]\n",
    "print(np.diag(s))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
