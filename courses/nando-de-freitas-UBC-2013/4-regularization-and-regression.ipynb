{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\LN}[1]{\\ln\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\newcommand{\\partdiff}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "$\n",
    "\n",
    "$\\newcommand{\\mat}[1]{\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "#1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "}\n",
    "$\n",
    "\n",
    "$\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr}\n",
    "#1\n",
    "\\end{array}\n",
    "}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumiD}{\\displaystyle \\sum_{i=1}^{D}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet bold,\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\Abt}{\\Ab^T}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\Yb}{\\mathbf{Y}}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math bold\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\phib}{\\pmb{\\phi}}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "\\newcommand{\\Phit}{\\Phib^T}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions\n",
    "* what is #p complexity class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "========\n",
    "\n",
    "how to fit nonlinear functions using bases functions and how to control model complexity\n",
    "* how to derive **ridge regression**\n",
    "* trade-off of fitting the data and **regularizing** it\n",
    "* learning **polynomial regression**\n",
    "* if basis functions are given, the pmtr learning is still linear\n",
    "* **cross-validation**\n",
    "* effects of data size and number of basis functions on **generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization\n",
    "===============\n",
    "\n",
    "all the answers so far are of the form\n",
    "$$\n",
    "\\hat{\\thetab} = \\left( \\Xt \\Xb \\right)^{-1} \\Xt \\yb\n",
    "$$\n",
    "* this requires the inversion of $\\Xt \\Xb$\n",
    "* can be problematic if the system of equations is poorly conditioned.\n",
    "* solution: add a small element to the diagonal:\n",
    "  $$\n",
    "    \\hat{\\thetab} = \\left( \\Xt \\Xb + \\delta^2 \\mathcal{I} \\right)^{-1} \\Xt \\yb\n",
    "  $$\n",
    "  \n",
    "* this is ridge regression estimate\n",
    "* it is the solution to the following regularized quadratic cost function\n",
    "$$\n",
    "J(\\thetab) = (\\yb - \\Xb \\thetab)^T (\\yb - \\Xb \\thetab) + \\underbrace{\\delta^2 \\thetat \\thetab}_{\n",
    "\\begin{matrix}\n",
    "  \\text{penalty} \\\\ \\text{regularizer}\n",
    "\\end{matrix}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivation\n",
    "----------\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "J(\\thetab) &=\n",
    "(\\yb - \\Xb \\thetab)^T (\\yb - \\Xb \\thetab) + \\delta^2 \\thetat \\thetab\n",
    "\\\\\n",
    "\\partdiff{J(\\thetab)}{\\thetab} &=\n",
    "-2\\Xt y + 2 \\Xt \\Xb \\thetab + 2 \\delta^2 \\mathcal{I} \\thetab\n",
    "\\\\\n",
    "& \\text{equating this to zero, we get}\n",
    "\\\\\n",
    "\\left(\n",
    "  \\Xt \\Xb + \\delta^2 \\mathcal{I}\n",
    "\\right) \\thetab\n",
    "&=\n",
    "\\Xt \\yb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we find $\\delta^2?$  \n",
    "hmm, you need to have patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RR as constrained optimization\n",
    "------------------------------\n",
    "\n",
    "$$\n",
    "J(\\thetab) = (\\yb - \\Xb \\thetab)^T (\\yb - \\Xb \\thetab) + \\delta^2 \\thetat \\thetab\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\min_{\\thetab : \\thetat \\thetab \\le t(\\delta)}\n",
    "\\left\\{\n",
    "  (\\yb - \\Xb \\thetab)^T (\\yb - \\Xb \\thetab)\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in 2D, $\\thetab = \\mat{\\theta_1 & \\theta_2}$  \n",
    "And\n",
    "$\\thetat \\thetab = \\theta_1^2 + \\theta_2^2$\n",
    "\n",
    "so we got two sets of systems of concentric ellipses.\n",
    "* the first system\n",
    "  * has $\\hat{\\thetab}_{ML}$ at its center\n",
    "  * this center corresponds to the solution when $\\delta^2 = 0$\n",
    "* second system\n",
    "  * origin at center, which corresponds $\\hat{\\thetab}_R$\n",
    "  * corresponds to the solution when $\\delta^2 \\rightarrow \n",
    "  \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the solution pt will lie at the intersection of these ellipses\n",
    "* such a point will be the pt where the curves meet tangentially\n",
    "* hence, the gradients, which are $\\perp$ to the curves individually and hence are collinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* say we are not in the tangential meeting point\n",
    "* we would have reached that point, by say, travelling from the tangential meeting point, along, say, the red curve\n",
    "* by doing so, the cost of the first term doesn't change but that of the second term increases, since this corresponds to shifting to a curve of higher \"radius\".\n",
    "* similar argument applies if we the roles of the first and the second terms are interchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* by proceeding this way, we can say that the set of all solutions corresponds to the set of all points which are tangential to all the points which are the tangential meeting points of the two systems of ellipses\n",
    "* this curve, of finite length, connects the points $\\hat{\\thetab}_{ML}$ and $\\hat{\\thetab}_{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some thetas will go to zero faster than others\n",
    "* useful in models where a suitable choice of regularizer will automatically drive most of the thetas to zero,\n",
    "* higher the value of $\\delta$, more $\\theta_i$'s will be driven to zero\n",
    "* refer to table 1.2 [11] of prml to see the effect of the value of the regularizer on the values of the weights in case of regularized polynomial regression.\n",
    "* we will end up with a few thetas which are \"responsible\", establishing either correlation or if you will, causation\n",
    "*  this is akin to feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization paths\n",
    "---------------------\n",
    "\n",
    "as $\\delta$ increases, $t(\\delta)$ decreases and each $\\thetab_i$ goes to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going nonlinear via basis functions\n",
    "===================================\n",
    "\n",
    "we introduce basis functions $\\phi(.)$ to deal with nonlinearity.\n",
    "$$\n",
    "y(\\xb) = \\phib(\\xb) \\thetab + \\epsilon\n",
    "$$\n",
    "\n",
    "for example, $\\phib(x) = \\left[ 1,x,x^2 \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the solution in case of identity basis function, for reference, is\n",
    "$$\n",
    "\\hat{\\thetab}_{ML} = \n",
    "\\left[ \\Xt \\Xb \\right]^{-1} \\Xt \\yb\n",
    "$$\n",
    "\n",
    "the corresponding solution in case of basis function $\\phi$ would become\n",
    "$$\n",
    "\\hat{\\thetab}_{\\text{ML}} = \n",
    "\\left[ \\Phi^T \\Phi \\right]^{-1} \\Phi^T \\yb\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phib(\\xb) = \\left[ 1,x_1,x_2 \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phib(\\xb) = \\left[ 1,x_1,x_2, x_1^2, x_2^2 \\right]$\n",
    "This would require the cross-term $x_1 ~x_2$.\n",
    "In case of cubic basis function, we require 6 terms, along with terms corresponding to second order and first order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the general formula is \n",
    "$$\n",
    "\\arrthree{\n",
    "\\text{#terms} &= \n",
    "1 + \\sumiD \\frac{i(i+1)}{2}\n",
    "\\\\ &=\n",
    "1 + \\half \\left[\n",
    "\\sumiD i^2 + i\n",
    "\\right]\n",
    "\\\\ &=\n",
    "1 + \\half \\left[\n",
    "  \\frac{D(D+1)(2D+1)}{6} + \n",
    "  \\frac{D(D+1)}{2}\n",
    "\\right]\n",
    "\\\\ &=\n",
    "1 + \\frac{D(D+1)}{2} \\left[\n",
    " \\frac{2D+1}{6} + \\half\n",
    "\\right]\n",
    "\\\\ &=\n",
    "1 + \\frac{D(D+1)(D+2)}{6}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the number of terms increase cubically with the order of the basis function. so please fucking avoid it if possible or if a lower order would suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "--------\n",
    "Ridge regression with a polynomial of degree 14\n",
    "$$\n",
    "\\hat{y}(x_i) = \\theta_0 + \\theta_1 x_i + \\theta_2 x_i^2 + \\cdots + \\theta_14 x_i^14 = \\displaystyle \\sum_{j=1}^{14} \\theta_j x_i^j\n",
    "\\\\\n",
    "\\phib = \\mat{1 & x_i & x_i^2 & \\cdots & x_i^{14} }\n",
    "\\\\\n",
    "J(\\thetab) = (\\yb - \\Xb \\thetab)^T (\\yb - \\Xb \\thetab) + \\delta^2 \\thetat \\thetab\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* increasing the order of the basis, by adding, say $X_i^{15}$ and so on would make the polynomial more squiggly.\n",
    "* the question of how many powers of x to add is replaced by the value of $\\delta^2$\n",
    "* there are three cases to consider\n",
    "  * small $\\delta$: the regularizer has no effect and we are back to being squiggly\n",
    "  * medium $\\delta$: the regularizer does its job well, achieving what it was summoned for (from the deep dark abyssal parts of the universe)\n",
    "  * large $\\delta$: the regularizer does its job too well, that it is interfering with the job of the first aka primary term. this results in most of the terms going to zero. we are left with a curve with no bumps and shit and it becomes more featureless and tends to become a line. what a sad life it has. much more than the starving children of somalia. oh, hi mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lasso is more aggressive, drive more $\\theta_i$'s to go to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel regression and RBF's\n",
    "---------------------------\n",
    "\n",
    "we can use kernel or radial basis functions (RBFs) as features:\n",
    "$$\n",
    "\\phib(\\xb) = \n",
    "\\mat{\n",
    "  \\kappa(\\xb, \\mub_1, \\lambda)\n",
    "  & \\cdots\n",
    "  & \\kappa(\\xb, \\mub_d, \\lambda)\n",
    "}\n",
    "$$\n",
    "\n",
    "for instance, we can use\n",
    "$$\n",
    "\\kappa(\\xb, \\mub_i, \\lambda) = \n",
    "\\EXP{-\\half \\Norm{\\xb - \\mu_i}^2}\n",
    "$$\n",
    "\n",
    "Now\n",
    "$$\n",
    "\\hat{y}(\\xb_i) = \n",
    "\\sum_{d=0}^{D} \\thetab_d \\kappa(\\xb_i, \\mu_d, \\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a=[1,2,3]\n",
    "b=[100,200,300]\n",
    "print([x[0]*x[1] for x in zip(a,b)])\n",
    "print(ft.reduce(lambda x,y: x+y, [x[0]*x[1] for x in zip(a,b)]))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_rbf(x, mu, sigma):\n",
    "    exponent = (x-mu)**2/sigma\n",
    "    return math.exp(-exponent)\n",
    "\n",
    "def plot_rbf(mu, sigma):\n",
    "    x = np.linspace(mu-6*sigma, mu+6*sigma, 100)\n",
    "    y = [val_rbf(xx, mu, sigma) for xx in x]\n",
    "    plt.plot(x, y, label='mu={0}'.format(mu))\n",
    "\n",
    "def vals_rbfs(x, thetas, mus, sigma):\n",
    "    vals_rbf = [val_rbf(x, mu, sigma) for mu in mus]\n",
    "    vals = [ theta*val for theta, val in zip(thetas, vals_rbf)]\n",
    "    return ft.reduce(lambda x,y: x+y, vals)\n",
    "    \n",
    "def fiddler(sigma=.6, theta1=.3, theta2=1.8, theta3=-.8):\n",
    "    mus = [1,2,4]\n",
    "    thetas = [theta1, theta2, theta3]\n",
    "    [plot_rbf(mu, sigma) for mu in mus]\n",
    "    \n",
    "    x_min = min(mus) - 6*sigma\n",
    "    x_max = max(mus) + 6*sigma\n",
    "    x = np.linspace(x_min, x_max, 100)\n",
    "    y = [vals_rbfs(xx, thetas, mus, sigma) for xx in x]\n",
    "    \n",
    "    plt.plot(x, y, label='resultant curve')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "interact(fiddler, sigma=(0,3,0.2), \n",
    "         theta1=(-2,2,.1),\n",
    "         theta2=(-2,2,.1),\n",
    "         theta3=(-2,2,.1)\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* think of basis functions as bricks\n",
    "* give the right bricks, i can build the empire state building, hell, even the earth\n",
    "* \"give me a fulcrum and a place to stand and i shall move the earth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{y}$ will be a weighted combination of gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a complete spec of the problem would be to specify $\\lambda, ~\\left\\{\\mu_i\\right\\}_{i=1}^{D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interruption @ end of lecture 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\phib(\\xb_i) = \n",
    "\\mat{\n",
    "1 &\n",
    "\\kappa(\\xb_i, \\mu_1, \\lambda) &\n",
    "\\kappa(\\xb_i, \\mu_2, \\lambda) &\n",
    "\\kappa(\\xb_i, \\mu_3, \\lambda) &\n",
    "}\n",
    "$$\n",
    "\n",
    "* $\\phib(\\xb_i)$ is a vector with 4 entries\n",
    "  * there are 3 bases\n",
    "* the corresponding vector of parameters is $\\underl{\\thetab} = \\mat{\\theta_0 & \\theta_1 & \\theta_2 & \\theta_3}^T$\n",
    "\n",
    "$$\n",
    "\\hat{\\yb}_i = \\phib(\\xb_i) \\underl{\\thetab}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a data set of size N, let\n",
    "$$\n",
    "\\Yb = \\mat{y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N}\n",
    "~~~ ~~~\n",
    "\\Phib = \\mat{\n",
    "\\phi(\\xb_1) \\\\ \\phi(\\xb_2) \\\\ \\vdots \\\\ \\phi(\\xb_N)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "source": [
    "Then\n",
    "$$\n",
    "\\arrthree{\n",
    "\\hat{\\Yb} &= \\Phib \\thetab \\\\\n",
    "\\hat{\\thetab}_{LS} &=\n",
    "\\left( \\Phit \\Phib \\right)^{-1} \\Phit \\yb\n",
    "\\\\\n",
    "\\hat{\\thetab}_{\\text{ridge}} &=\n",
    "\\left( \\Phit \\Phib + \\delta^2 \\mathcal{I} \\right)^{-1} \\Phit \\yb\n",
    "}\n",
    "$$\n",
    "\n",
    "this is still linear regression with $\\Xb$ replaced by $\\Phib$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we can choose the locations $\\mub$ of the basis functions to be the inputs.\n",
    "* that is, $\\mub_i = \\xb_i$\n",
    "* these basis functions are known as **kernels**\n",
    "* the choice of width $\\lambda$ is tricky"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fiddler_yo()\n",
    "    x = np.linspace(x_min, x_max, 100)\n",
    "    y = [vals_rbfs(xx, thetas, mus, sigma) for xx in x]\n",
    "    \n",
    "    plt.plot(x, y, label='resultant curve')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# interact(fiddler_yo, sigma=(0,3,0.2), )\n",
    "    \n",
    "\n",
    "# x_gt = np.linspace(-10,10,11)\n",
    "# y_gt = [2,1,1,0,0,2,6,5,7,10,8]\n",
    "# plt.plot(x_gt, y_gt, '.b')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe big fucking question  \n",
    "* how do we choose,\n",
    "  * regularization coefficient\n",
    "  * width of the kernels\n",
    "  * polynomial order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution 1: cross-validation\n",
    "-----------------------------\n",
    "\n",
    "1. given training data $\\left\\{ \\xb_i, y_i \\right\\}_{i=1}^{N}$ and some $\\delta^2$ guess, compute $\\hat{\\thetab}$\n",
    "1. $\\hat{\\Yb}_{\\text{train}} = \\Xb_{\\text{train}} \\hat{\\thetab}$\n",
    "1. $\\hat{\\Yb}_{\\text{test}}  = \\Xb_{\\text{test}}  \\hat{\\thetab}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all research/modelling is based on induction. this is coz learning is an ill posed problem and there could be infinitely many solutions to the same problem. we have to iteratively fit a better model based on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there are two strategies to pick the right parameter values, given the training and testing errors.\n",
    "* first one is to find the max of the training and testing errors for each parameter value and take the parameter corresponding to the min of this.\n",
    "* the second one is to avg the train and test errors and pick the parameter value which results in the min error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross-validation\n",
    "---------------------------\n",
    "\n",
    "* split the training data into K folds\n",
    "* for each fold $k \\in \\left\\{ 1, \\cdots, K \\right\\}$\n",
    "  * train on all folds but $k^{th}$\n",
    "  * test on the $k^{th}$\n",
    "* it is common to use K=5, called 5-fold CV\n",
    "* if K=N, we get leave-one out cross-validation or LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: RR with polynomial of degree 14\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect of data when we have the right model\n",
    "-------------------------------------------\n",
    "\n",
    "$$\n",
    "y_i = \\theta_0 + x_i \\theta_1 + x_i^2 \\theta_2 + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "the model that we have\n",
    "$$\n",
    "\\hat{y}_i = \\hat{\\theta}_0 + x_i \\hat{\\theta}_1 + x_i^2 \\hat{\\theta}_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect of data when the model is too simple\n",
    "-------------------------------------------\n",
    "\n",
    "$$\n",
    "y_i = \\theta_0 + x_i \\theta_1 + x_i^2 \\theta_2 + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "the model that we have\n",
    "$$\n",
    "\\hat{y}_i = \\hat{\\theta}_0 + x_i \\hat{\\theta}_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect of data when the model is very complex\n",
    "-------------------------------------------\n",
    "\n",
    "$$\n",
    "y_i = \\theta_0 + x_i \\theta_1 + x_i^2 \\theta_2 + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "the model that we have\n",
    "$$\n",
    "\\hat{y}_i = \\hat{\\theta}_0 + x_i \\hat{\\theta}_1 + x_i^2 \\hat{\\theta}_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of test vs train errors as the size of the data set increases  \n",
    "* the test error, drops from above and tends to converge\n",
    "  * like $e^{-x}$\n",
    "* the train error, increases from below and tends to converge\n",
    "  * like log\n",
    "  \n",
    "this plot is similar to the plot in \"learning from data\" by mostafa\n",
    "* fig 2.3 [59]\n",
    "* section 2.3.2 [66]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More data improves the results, but only if the model has the right complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence in the predictions\n",
    "-----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what we essentially try to do is to reduce the uncertainty all over the data space.  \n",
    "but the right thing to do is to reduce the uncertainty in regions where we have observed some data and be more uncertain in the regions where we haven't observed anything"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
