{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$\n",
    "\\newcommand{\\fracrec}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\expb}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\LN}[1]{\\ln\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\invp}[1]{\\left({#1}\\right)^{-1}}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\mat}[1]{ \\left[ \\begin{matrix} #1 \\end{matrix} \\right] }\n",
    "\\newcommand{\\matp}[1]{ \\left( \\begin{matrix} #1 \\end{matrix} \\right)}\n",
    "\\newcommand{\\mats}[1]{ \\begin{matrix}#1\\end{matrix} }\n",
    "$\n",
    "\n",
    "$\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr} #1 \\end{array}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet bold,\n",
    "$\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\At}{\\Ab^T}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\fb}{\\mathbf{f}}\n",
    "\\newcommand{\\fp}{f^{\\prime}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\kb}{\\mathbf{k}}\n",
    "\\newcommand{\\Kb}{\\mathbf{K}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lt}{\\Lb^T}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "\\newcommand{\\vb}{\\mathbf{v}}\n",
    "\\newcommand{\\Vb}{\\mathbf{V}}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xp}{x^{\\prime}}\n",
    "\\newcommand{\\xbp}{\\xb^{\\prime}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math bold\n",
    "$\n",
    "\\newcommand{\\alphab}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\zerob}{\\pmb{0}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\middle | #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Basics\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a point cloud around origin within an unit circle, the mean is zero.\n",
    "\n",
    "there is no correlation between the axes or dimensions and the covar becmes diagonal. this is saying that a particular value of x doesnt gimme any new information about y.\n",
    "\n",
    "in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in a point cloud of an ellipse at 45 deg with elongation along 45 degee, the mean is still going to be zero.\n",
    "\n",
    "but x and y are correlated now, so the covar is no longer diagonal.\n",
    "\n",
    "that is to say, if i know a value of x, i gain some new information on the value of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets say we have a 2d gaussian. this will have the joint probability distribution in the z-axis. now, we take a slice of this, at a particular value of $x_1$, say $\\chi_1$.  \n",
    "\n",
    "Now the resulting distribution will be Gaussian as well.\n",
    "\n",
    "The most important thing here is the conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is GP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "say i ask a dude the question of how much they like each country.  \n",
    "here $x_i$ are the countries and each student has a function in his head for his likely preference and he gives us the value of the function for these countries ($x_i$).  \n",
    "this is what a functional is all about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Gaussian Theorem\n",
    "==============================\n",
    "\n",
    "Marginals and conditionals of an MVN  \n",
    "\n",
    "Suppose $\\xb = (\\xb_1, \\xb_2)$ is jointly Gaussian with parameters\n",
    "$$\n",
    "\\mub = \\matp{\\mub_1 \\\\ \\mub_2} \\\\\n",
    "\\Sigma =\n",
    "\\matp{\\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22}} \\\\\n",
    "\\Lambda = \\Sigma^{-1} = \n",
    "\\matp{\\Lambda_{11} & \\Lambda_{12} \\\\ \\Lambda_{21} & \\Lambda_{22}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginals are given by\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\xb_1) &= \\Nl{\\xb_1}{\\mub_1}{\\Sigma_{11}} \\\\\n",
    "p(\\xb_2) &= \\Nl{\\xb_2}{\\mub_2}{\\Sigma_{22}}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior conditional is given by\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\xb_1 \\mid \\xb_2) &=\n",
    "\\Nl{\\xb_1}{\\mub_{1 \\mid 2}}{\\Sigma_{1 \\mid 2}}\n",
    "\\\\\n",
    "\\mub_{1 \\mid 2} &=\n",
    "\\mub_1 + \\Sigma_{12} \\Sigma_{22}^{-1} (\\xb_2 - \\mub_2)\n",
    "\\\\\n",
    "&=\n",
    "\\mub_1 - \\Lambda_{11}^{-1} \\Lambda_{12}(\\xb_2 - \\mub_2)\n",
    "\\\\ &=\n",
    "\\Sigma_{1 \\mid 2}\n",
    "\\left(\n",
    "  \\Lambda_{11} \\mub_1\n",
    "  - \\Lambda_{12} (\\xb_2 - \\mub_2)\n",
    "\\right)\n",
    "\\\\\n",
    "\\Sigma_{1 \\mid 2} &=\n",
    "\\inv{\\Lambda_{11}} =\n",
    "\\Sigma_{11}\n",
    "- \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Basics\n",
    "================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model using a multivariate Gaussian\n",
    "$$\n",
    "\\mat{f_1 \\\\ f_2 \\\\ f_3} \\sim\n",
    "\\Nl{f}{\\mat{0 \\\\ 0\\\\ 0}}\n",
    "{\\mat{\n",
    "k_{11} & k_{12} & k_{13} \\\\\n",
    "k_{21} & k_{22} & k_{23} \\\\\n",
    "k_{31} & k_{32} & k_{33} \\\\\n",
    "}}\n",
    "\\\\ \\sim\n",
    "\\Nl{f}{\\mat{0 \\\\ 0\\\\ 0}}\n",
    "{\\mat{\n",
    "1 & 0.7 & 0.2 \\\\\n",
    "0.7 & 1 & 0.6 \\\\\n",
    "0.2 & 0.6 & 1\n",
    "}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Kb_{ij} = \\expb{-\\Norm{\\xb_i - \\xb_j}^2}\n",
    "=\n",
    "\\begin{cases}\n",
    "  0 & \\Norm{\\xb_i - \\xb_j} \\rightarrow \\infty \\\\\n",
    "  1 & \\xb_i == \\xb_j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "thie leads to an svd matrix [??]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\D = \\left\\{ (\\xb_1,f_1), (\\xb_2,f_2), (\\xb_3,f_3) \\right\\}$ and $\\xb_*$, what is $f_*?$\n",
    "\n",
    "X's are given and we ahve to model the function f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smoothness criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "say the original f is an exponential and you are given points $x_1, x_2, x_3$ and the corresponding function values $f_1, f_2, f_3$\n",
    "\n",
    "say we are given an $x_*$ and we want to predict $f_*$. we need to choose an $f_*$ so that it obeys the smoothness criterion. small changes in input should lead to small changes in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SInce the value of f in the training set seems to increase with x, we want the prediction to follow that trend as well.\n",
    "\n",
    "that is,\n",
    "$$\n",
    "\\mat{\\fb \\\\ f_*} \\sim\n",
    "\\Nl{f}{\\zerob}\n",
    "{\\mat{\n",
    "\\mat{\\Kb} & \\kb_* \\\\\n",
    "\\kb_*^T & k_{**}\n",
    "}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\kb_*[i] = \\kappa(\\xb_*, \\xb_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to find the conditional $p(f_* \\mid \\fb)$, we can appeal to the multivariate Gaussian theorem\n",
    "\n",
    "since $\\mub_{1\\mid 2} = \\mub_1 + \\Sigma_{1 2} \\Sigma_{22}^{-1} (\\xb_2 - \\mub_2)$,   \n",
    "we have $\\mub_*= \\kb_*^T \\Kb^{-1} \\fb$\n",
    "\n",
    "since $\\Sigma_{1 \\mid 2} = \\Sigma_{11}\n",
    "- \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n",
    "$,  \n",
    "we have\n",
    "$\\Sigma_* = k_{**} - \\kb_*^T \\Kb^{-1} \\kb_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember\n",
    "$$\n",
    "\\V{\\Ab \\Xb} = \\Ab \\V{\\Xb} \\At\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\Lb$ = cholesky$(\\Kb + \\sigma_y^2 \\I)$\n",
    "1. $\\alpha = \\Lt /\\ (\\Lb /\\ \\yb) $\n",
    "1. $\\E{f_*} = \\kb_*^T \\alpha$\n",
    "1. $\\vb = \\Lb /\\ \\kb_*$\n",
    "1. $\\V{f_*} = \\kappa(\\xb_*, \\xb_*) - \\vb^T \\vb$\n",
    "1. $\n",
    "\\log p(\\yb \\mid \\Xb) = \n",
    "-\\half ~\\yt \\alpha\n",
    "-\\sum_i \\log \\Lb_{ii}\n",
    "-\\frac{N}{2} ~\\log(2\\pi)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP: a distribution over functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GP is a Gaussian distribution over functions.\n",
    "$$\n",
    "\\arrthree{\n",
    "f(\\xb) &\\propto\n",
    "GP(m(\\xb), \\kappa(\\xb, \\xb^{\\prime}))\n",
    "\\\\\n",
    "m(\\xb) &= \\E{f(\\xb)}\n",
    "\\\\\n",
    "\\kappa(\\xb, \\xbp) &=\n",
    "\\E{(f(\\xb) - m(\\xb)) (f(\\xbp) - m(\\xbp))^T}\n",
    "\\\\\n",
    "\\kappa(\\xb, \\xbp) &=\n",
    "\\expb{-\\half (x-x^{\\prime})^2}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from P(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create $\\xb_{1:N}$\n",
    "* Create $\\mub = \\zerob_N, \\mats{\\kappa \\\\ N \\times N}$\n",
    "* $\\kappa = \\Lb \\Lt$\n",
    "* $\\fp \\sim \\mathcal{N}(\\zerob_N, \\kappa) \\sim \\Lb \\mathcal{N}(\\zerob, \\I) $\n",
    "* since $\\V{\\Ab \\Xb} = \\Ab \\V{\\Xb} \\At$\n",
    "* Hence $\\V{\\fp} = \\Lb \\Lt = \\kappa$\n",
    "* so sample from a std gaussian and left-multiply it by $\\Lb$, we can sample from $\\kappa$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def kernel(a,b):\n",
    "    \"\"\" GP squared exponential kernel \"\"\"\n",
    "    # t1: (n,1) t2: (1,n) t3: (n,n)\n",
    "    # t1 + t2 => (n, n) with sum of pairs like a grid\n",
    "    sqdist = a**2 + b.T**2 - 2. * a @ b.T\n",
    "    return np.exp(-.5 * sqdist)\n",
    "  \n",
    "#number of test points\n",
    "n=50\n",
    "# test points\n",
    "Xtest = np.linspace(-5, 5, n).reshape(-1, 1)\n",
    "# kernel at test points\n",
    "K_ = kernel(Xtest, Xtest)\n",
    "\n",
    "# draw samples from the prior at our test points\n",
    "# $K = L L^T$\n",
    "L = np.linalg.cholesky(K_ + 1e-6*np.eye(n))\n",
    "n_curves = 10\n",
    "f_prior = np.dot(L, np.random.normal(size=(n,n_curves)))\n",
    "print('xtest: ', Xtest.shape, ' f_p: ',f_prior.shape)\n",
    "\n",
    "plt.plot(Xtest, f_prior)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP Posterior\n",
    "===========\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\D &= \\left\\{\n",
    "(\\xb_i, f_i), i=1:N\n",
    "\\right\\}\n",
    "\\\\\n",
    "p(f \\mid \\D)\n",
    "&=\n",
    "\\frac{p(\\D \\mid f) ~p(f)}{p(\\D)}\n",
    "}\n",
    "$$\n",
    "\n",
    "the prior ensures that the fit/posterior is smooth.  \n",
    "this is like saying, hey, function predictor, please make sure the functions you predict are smooth, not just based on the data.\n",
    "\n",
    "the figure in the previous section are priors.  \n",
    "the posterior will be like prior in the regions where there is no data and in the regions containing data, it will all be squished together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active Learning with GPs\n",
    "==========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noiseless GP regression\n",
    "=======================\n",
    "\n",
    "* we observe a training set $\\D = \\left\\{\n",
    "(\\xb_i, f_i), i=1:N\n",
    "\\right\\}$ where $f_i = f(\\xb_i)$\n",
    "* Given a test set $\\Xb_*$ of size $N_* \\times D$, we want to predict the function outputs $\\fb_*$\n",
    "\n",
    "$$\n",
    "\\matp{\\fb \\\\ \\fb_*}\n",
    "\\sim\n",
    "\\mathcal{N}{\\matp{\\mub \\\\ \\mub_*}, \\matp{\\Kb & \\Kb_* \\\\ \\Kb_*^T & \\Kb_{**}}}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\arrthree{\n",
    "\\Kb &= \\kappa(\\Xb, \\Xb) &: (N \\times N)\n",
    "\\\\\n",
    "\\Kb_* &= \\kappa(\\Xb, \\Xb_*) &: (N \\times N_*)\n",
    "\\\\\n",
    "\\Kb_{* *} &= \\kappa(\\Xb_*, \\Xb_*) &: (N_*, N_*)\n",
    "\\\\\n",
    "\\kappa(x, \\xp) &=\n",
    "\\sigma_f^2\n",
    "\\expb{-\\fracone{2\\ell^2} (x-\\xp)^2}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\matp{\\fb \\\\ \\fb_*}\n",
    "\\sim\n",
    "\\mathcal{N}{\\matp{\\mub \\\\ \\mub_*}, \\matp{\\Kb & \\Kb_* \\\\ \\Kb_*^T & \\Kb_{**}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\fb_* \\mid \\Xb_*, \\Xb, \\fb)\n",
    "&=\n",
    "\\Nl{\\fb_*}{\\mub_*}{\\Sigma_*}\n",
    "\\\\\n",
    "\\mub_* &=\n",
    "\\mub(\\Xb_*) + \\Kb_*^T \\inv{\\Kb} (\\fb - \\mub(\\Xb))\n",
    "\\\\\n",
    "\\Sigma_* &=\n",
    "\\Kb_{* *} - \\Kb_*^T \\inv{\\Kb} \\Kb_*\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect of kernel width parameter\n",
    "=================================\n",
    "\n",
    "$$\n",
    "\\kappa(x, \\xp)\n",
    "=\n",
    "\\sigma_f^2\n",
    "\\expb{-\\fracone{2\\ell^2} (x-\\xp)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the length-scale is less, the predictions are very myopic. hence they become very noisy and wiggly. they also tend to overfit.   \n",
    "but most importantly, if the training data is sparse, then the predictions in regions where there was no training data, the distance becomes too high (coz of the presence of \\inv{\\Kb}) and the prediction tend to overshoot on either extreme ( up or down depending on the trend of the function)\n",
    "\n",
    "if the length scale, it is very smooth since it can take into account or give more weightage to far away points. but this might result in prediction errors for the predictor might take in suggestions from faraway points which aren't necessary and hence become noise.\n",
    "\n",
    "if the length-scale is infinite, then the predictions become a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i can use some gradient descent techniques to find the best parameters values. but since the likelihood function is non-convex, we end up having multiple maxima.\n",
    "\n",
    "so, a better way would be to use cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy GP regression\n",
    "====================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the uncertainty of the posterior is little different from the noiseless case.  \n",
    "we have very less regions where the uncertainty is zero (almost no such regions)  \n",
    "\n",
    "we have regions of more uncertainty and regions of moderate uncertainty.  \n",
    "\n",
    "* in regions w/ data, we have less uncertainty\n",
    "* in regions w/o data, there is more uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fyi, the inverse Wishart distribution in 1d is called the inverse gamma distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in type-2 ml, we are learning the parameters of the \"prior\", of the Kernel matrix, from the data. what a faggot eh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in hierarchial bayes, what we is that, instead of getting a point estimate of the hyperparameters, we induce a distribution over those.\n",
    "\n",
    "sth like hyper-prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_* = \\kb_*^T \\inv{\\Kb_y} \\yb\n",
    "$$\n",
    "Let\n",
    "$$\n",
    "\\alpha = \\inv{\\Kb_y} \\yb\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\arrthree{\n",
    "f_* \n",
    "&= \\sumiN \\alpha_i \\kappa(\\xb_i, \\xb_*)\n",
    "\\\\ &=\n",
    "\\Kb_*^T \\alphab\n",
    "\\\\ \n",
    "f_*(\\xb_*)&=\n",
    "\\sumiN \\alpha_i \\expb{-\\fracrec{\\lambda} \\Norm{\\xb_i - \\xb_*}^2}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nothing but a set of rbf basis functions each weighted by $\\alphab_i$.  \n",
    "A big fucking linear combination of nonlinear basis functions.  \n",
    "Given a new test point $\\xb_*$, we evaluate this function and this becomes the mean at that test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy GP regression and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\displaystyle \\argmin_{\\thetab \\in \\mathbb{R}^d}\n",
    "\\Norm{\\yb - \\Xb \\thetab}_2^2 + \\delta^2 \\Norm{\\thetab}_2^2\n",
    "~~~ ~~~ ~~~ ~~~ \\Xb \\in \\mathbb{R}^{n \\times d} ~~~ \\yb \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "on differentiating the above and equating it to zero, we get\n",
    "$$\n",
    "\\arrthree{\n",
    "(\\Xt \\Xb + \\delta^2 \\I_d) \\thetab &= \\Xt \\yb\n",
    "\\\\\n",
    "\\Xt \\Xb \\thetab + \\delta^2 \\I_d \\thetab &= \\Xt \\yb\n",
    "\\\\\n",
    "\\delta^2 \\I_d \\thetab &= \\Xt \\yb - \\Xt \\Xb \\thetab = \\Xt (\\yb - \\Xb \\thetab)\n",
    "\\\\\n",
    "\\thetab &= \\fracrec{\\delta^2} \\Xt (\\yb - \\Xb \\thetab)\n",
    "\\\\\n",
    "\\thetab &= \\Xt \\alphab\n",
    "}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\alphab = \\frac{(\\yb - \\Xb \\thetab)}{\\delta^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\alphab &= \\frac{1}{\\delta^2} (\\yb - \\Xb \\thetab)\n",
    "\\\\\n",
    "\\delta^2 \\alphab &= \\yb - \\Xb \\Xt \\alpha & \\commentgray{$\\thetab = \\Xt \\alpha$}\n",
    "\\\\\n",
    "(\\delta^2 \\I_n + \\Xt \\Xt) \\alphab &= \\yb\n",
    "\\\\\n",
    "\\alphab &= (\\delta^2 \\I_n + \\Xb \\Xt)^{-1} \\yb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which parametrization one has to use depends on the application which gives us n and d.\n",
    "\n",
    "In case of bioinformatics where you have n=20 patients and d=20k features, we are better off using $\\alphab$ where we have to invert $\\Xb \\Xt$ which is $n \\times n$, which is easy\n",
    "\n",
    "on the other hand, if the situation is the reverse, with small d and huge n, we are better off using $(\\Xt \\Xb + \\delta^2 \\I_d) \\thetab = \\Xt \\yb$ since we have to invert a smaller $d \\times d$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction given a new $\\xb_*$ is \n",
    "$$\n",
    "\\arrthree{\n",
    "\\yb_* &= \\thetab^T \\xb_*\n",
    "\\\\ &=\n",
    "\\xb_*^T \\thetab\n",
    "\\\\ &=\n",
    "\\xb_*^T \\Xt \\alphab\n",
    "\\\\ &=\n",
    "\\xb_*^T \\Xt (\\delta^2 \\I_n + \\Xb \\Xt)^{-1} \\yb\n",
    "}\n",
    "$$\n",
    "where we can use either $\\thetab$ or $\\alphab$, whichever is easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now \n",
    "$$\n",
    "\\Xb \\Xt = \n",
    "\\mat{\\xb_1^T \\\\ \\vdots \\\\ \\xb_n^T} \\mat{\\xb_1 & \\cdots & \\xb_n}\n",
    "=\n",
    "\\mat{\n",
    "\\xb_1^T \\xb_1 & \\cdots & \\xb_1^T \\xb_N\n",
    "\\\\\n",
    "\\vdots & \\ddots & \\vdots\n",
    "\\\\\n",
    "\\xb_N^T \\xb_1 & \\cdots & \\xb_N^T \\xb_N\n",
    "}\n",
    "$$\n",
    "\n",
    "each entry is a dot product of a data point with another, which gives us a similarity measure.  Thus, this outer product is a kernel matrix, using the linear kernel. That is\n",
    "$$\n",
    "\\Kb = \\Xb \\Xt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus\n",
    "$$\n",
    "\\arrthree{\n",
    "y^* &= \n",
    "\\xb_*^T \\Xt (\\delta^2 \\I_n + \\Xb \\Xt)^{-1} \\yb\n",
    "\\\\ &=\n",
    "\\kb_*^T \\inv{\\Kb_N} \\yb\n",
    "}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\kb_*^T = \n",
    "\\mat{\\xb_*^T \\xb_1 & \\cdots & \\xb_*^T \\xb_N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just note that, in GP, the covariance of the prior on **t** was\n",
    "$$\n",
    "\\Cb(\\xbn, \\xbm) = \\inv{\\beta} \\delta_{nm} + \\kappa(\\xbn, \\xbm)\n",
    "$$\n",
    "\n",
    "and in posterior, the mean was\n",
    "$$\n",
    "\\mb_*\n",
    "= \\kb_*^T \\inv{\\Cb_N} \\tb\n",
    "= \\kb_*^T \\left\\{ \\beta^{-1} \\mathcal{I} + \\Kb \\right\\}^{-1} \\tb\n",
    "= \\kb_*^T \\Kb_N^{-1} \\tb\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is a GP where you have the mean of the GP  using a linear kernel is in the same as we get from ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are two things which control the smoothness of the GP estimate\n",
    "* the length-scale or the kernel width parameter\n",
    "* the regularizer $\\delta^2$ of Ridge = precision of the input noise $\\beta^{-1}$ of GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression is also called Tikhonov regularization  \n",
    "Bishop says that Tikhonoc regularization is equivalent to adding noise to your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN and GP\n",
    "\n",
    "in the limit of n going to infinity, it becomes a gaussian process\n",
    "\n",
    "\\citeme{Introduction to Gaussian Processes, MacKay}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\chol}{\\text{cholesky}}$\n",
    "\n",
    "# GP Regression - pseudocode\n",
    "\n",
    "1. $\\Lb = \\chol(\\Kb + \\sigma_y^2 \\I)$ \n",
    "1. $\\alphab = \\Lt \\setminus (\\Lb \\setminus \\yb)$\n",
    "1. $\\E{f_*} = \\kb_*^T \\alphab$\n",
    "1. $\\vb = \\Lb \\setminus \\kb_*$\n",
    "1. $\\V{f_*} = \\kappa(\\xb_*, \\xb_*) - \\vb^T \\vb$\n",
    "1. $\\log p(\\yb \\mid \\Xb) = -\\half \\yt \\alphab - \\sum_i \\log \\Lb_{ii} - \\frac{N}{2} \\log(2\\pi)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the complexity of conjugate gradient for fiding the solution to a linear system, which usuallly involves inverting a matrix which amounts to $O(n^3)$, is just $O(n^2)$ times the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP in animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you have a parametrized model for the walk of a dino. for each parameter, a user rates how good the model is in mimicking the dino walk.  \n",
    "essentially, what is happening here is we are giving the user a data point and he uses the function he has in his head to evaluate the function at that point and gives us the result of the evaluation.  \n",
    "\n",
    "we can then use these evaluations to get a good model for a dinosaur walk\n",
    "\n",
    "http://dl.acm.org/citation.cfm?id=1921427.1921443\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deal is the software has a set of parameters (11 as she says).  \n",
    "We have to navigate this space efficiently so that we reach an optimal point.  \n",
    "\n",
    "there is a tradeoff between showing things that are similar to what the animator liked best and exploring different areas of the state space.  \n",
    "\n",
    "in order to exploit, we need to find the areas where the mean is high, where the good values are.   \n",
    "in order to explore, we need to find areas where the uncertainty is high, which are bassically areas which are to be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using GP for trading off exploitation vs exploration, which is fundamental problems of AI"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
