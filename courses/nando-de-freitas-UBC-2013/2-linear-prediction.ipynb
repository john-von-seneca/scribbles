{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$ \\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}$  \n",
    "$ \\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}$\n",
    "$ \\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}$\n",
    "$ \\newcommand{\\EXP}[1]{\\exp\\left(#1\\right)}$  \n",
    "$ \\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\mat}[1]{\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "#1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "}$\n",
    "$\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}$\n",
    "\n",
    "$\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr}\n",
    "#1\n",
    "\\end{array}\n",
    "}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet bold,\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\Abt}{\\Ab^T}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math bold\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inputs: covariates, predictors, features\n",
    "* Outputs: variates, labels\n",
    "\n",
    "\n",
    "Linear methods:  \n",
    "* lotta processes can be approximates with linear models\n",
    "* Linear regression: module of larger systems\n",
    "* can be solved analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation\n",
    "===========\n",
    "\n",
    "* n instances of input-output pairs $\\{\\xb_{1:n}, \\yb_{1:n} \\}$\n",
    "* $\\xb_i \\in \\mathbb{R}^{d}$, that is d-dimensional\n",
    "* $\\yb_i \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a training set, learn the model of how inputs affect outputs.  \n",
    "given a new $x_{n+1}$, find $\\hat{y}(\\xb_{n+1})$\n",
    "\n",
    "* The training phase involves learning the parameters of the model $\\hat{\\thetab}$\n",
    "* the prediction phase involves predicting $\\hat{y}_{n+1}$ for a new value $\\xb_{n+1}$ using the learning parameters $\\hat{\\thetab}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\hat{y}(\\xb_i) &= \\theta_1 + x_i \\theta_2 = \\hat{y}_i\n",
    "\\\\\n",
    "J(\\thetab) &=\n",
    "\\sumiN \\left( y_i - \\hat{y}_i \\right)^2\n",
    "\\\\ &=\n",
    "\\sumiN \\left( y_i - \\theta_1 - x_i \\theta_2 \\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "J: & \\text{objective, cost, loss, energy, error} \\\\\n",
    "\\theta_1:& \\text{y-intercept} \\\\\n",
    "\\theta_2:& \\text{slope}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the line resembles a rod and each point is connected to the line using a spring (vertical). if we let go of the rod at any orientation, the rod will oscillate and stabilize at a particular angle. this corresponds at the minimum energy configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there will always be a statistical error and there will always be uncertainty. we have to reduce this uncertainty\n",
    "\n",
    "when does this fail  \n",
    "* the line doesn't go thro any of the points.\n",
    "* points in a vertical line\n",
    "* points follow a high order polynomial\n",
    "* outliers: will pull the rod towards, increasing the error at other points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "i &= 1,2,\\cdots,N \\\\\n",
    "j &= 1,2,\\cdots,D \\\\\n",
    "\\hat{y}_i &= \\sumjD x_{ij} \\theta_j\n",
    "}\n",
    "$$\n",
    "\n",
    "* $x_{i1}=1$, hence $\\theta_1$ corresponds to the intercept of the line with the vertical axis.  \n",
    "* $\\theta_1$: bias, offset :: controls the vertical distance from the origin\n",
    "* $\\theta_2$: controls the slope of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix form\n",
    "=============\n",
    "\n",
    "$$\n",
    "\\mat{\\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_N}\n",
    "=\n",
    "\\mat{\n",
    "  x_{11} & \\cdots & x_{1D} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  x_{N1} & \\cdots & x_{ND}\n",
    "}\n",
    "\\mat{\n",
    "  \\theta_1 \\\\\n",
    "  \\vdots \\\\\n",
    "  \\theta_D\n",
    "}\n",
    "=\n",
    "\\mat{\n",
    "  \\underl{x}_1^T \\\\\n",
    "  \\underl{x}_2^T \\\\\n",
    "  \\vdots \\\\\n",
    "  \\underl{x}_N^T\n",
    "}\n",
    "\\mat{\n",
    "  \\theta_1 \\\\\n",
    "  \\vdots \\\\\n",
    "  \\theta_D\n",
    "}\n",
    "$$\n",
    "\n",
    "That is,\n",
    "$$\n",
    "\\arrthree{\n",
    "\\hat{\\yb} &= \\Xb \\thetab \\\\\n",
    "\\hat{\\yb} & \\in \\mathbb{R}^N \\\\\n",
    "\\hat{\\Xb} & \\in \\mathbb{R}^{N \\times D} \\\\\n",
    "\\hat{\\thetab} & \\in \\mathbb{R}^D \\\\\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first col of **X** is composed of 1's"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.matrix((5,25,22,18)).transpose()\n",
    "X = np.matrix(((1,100,2),\n",
    "               (1,50,42),\n",
    "               (1,45,31),\n",
    "               (1,60,35)))\n",
    "print(\"y=\")\n",
    "pprint(y)\n",
    "print(\"X=\")\n",
    "pprint(X)\n",
    "\n",
    "T = np.matrix((1,0,0.5)).transpose()\n",
    "print(\"T=\")\n",
    "pprint(T)\n",
    "\n",
    "y_hat = X*T\n",
    "print(\"y_hat= X * T = \")\n",
    "pprint(y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization approach\n",
    "======================\n",
    "\n",
    "$$\n",
    "J(\\thetab)\n",
    "=\n",
    "\\sumiN \\left( y_i - \\xb_i^T \\thetab \\right)^2\n",
    "=\n",
    "\\left( \\yb - \\Xb\\thetab \\right)^T\n",
    "\\left( \\yb - \\Xb\\thetab \\right)\n",
    "$$\n",
    "\n",
    "* contour plot: point in a single contour plot have the same function value\n",
    "* gradient is always perpendicular to the contour plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the minimum\n",
    "--------------------\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\thetab)}{\\partial \\theta_j}\n",
    "=\n",
    "\\sumiN 2(y_i - \\xb_{i}^{T} \\thetab) (-x_{ij})\n",
    "$$\n",
    "\n",
    "We have to equate this to zero and find the values of every parameter. quite painful you see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding the min, the easy way\n",
    "------------------------------\n",
    "\n",
    "$$\n",
    "J(\\thetab) = \\left( \\yb - \\Xb \\thetab \\right)^T \\left( \\yb - \\Xb \\thetab \\right)\n",
    "$$\n",
    "\n",
    "Results from matrix differentiation..\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{\\partial \\Ab \\thetab}{\\partial \\thetab}\n",
    "&=\n",
    "\\Abt\n",
    "\\\\\n",
    "\\frac{\\partial \\thetat \\Ab \\thetab}{\\partial \\thetab}\n",
    "&=\n",
    "2 \\Abt \\thetab\n",
    "}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{\\partial J(\\thetab)}{\\partial \\thetab}\n",
    "&= 2 \\left(\\yb - \\Xb \\thetab \\right)^T\n",
    "    \\left( -\\Xb \\right)\n",
    "\\\\\n",
    "&=\n",
    "2 \\left( \\Xb \\thetab - \\yb \\right)^T \\Xb\n",
    "}\n",
    "$$\n",
    "\n",
    "The little long way\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{\\partial J(\\thetab)}{\\partial \\thetab}\n",
    "&=\n",
    "\\frac{\\partial}{\\partial \\thetab}\n",
    "\\left[\n",
    "  \\yt \\yb - \\yt \\Xb \\thetab - \\thetat \\Xt \\yb \n",
    "  + \\thetat \\Xt \\Xb \\thetab\n",
    "\\right]\n",
    "\\\\\n",
    "&= -\\Xt \\yb - \\Xt \\yb + 2\\Xt \\Xb \\thetab\n",
    "\\\\\n",
    "&= -2 \\Xt \\yb + 2\\Xt \\Xb \\thetab\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal equations\n",
    "------------------\n",
    "\n",
    "Equating the derivative to zero, we get\n",
    "$$\n",
    "\\arrthree{\n",
    "\\Xt \\Xb \\thetab &= \\Xt \\yb\\\\\n",
    "\\thetabh &= \\left( \\Xt \\Xb \\right)^{-1} \\Xt \\yb\n",
    "}\n",
    "$$\n",
    "\n",
    "This equation is called the normal equations and $\\thetabh$ is called the least squares estimate\n",
    "\n",
    "$$\n",
    "\\hat{\\yb} = \\Xb \\thetabh\n",
    "= \\Xb \\left( \\Xt \\Xb \\right)^{-1} \\Xt \\yb\n",
    "= \\Hb \\yb\n",
    "$$\n",
    "\n",
    "$\\Hb = \\Xb \\left( \\Xt \\Xb \\right)^{-1} \\Xt$ is called the hat matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple outputs\n",
    "==================\n",
    "\n",
    "aka Multivariate linear regression\n",
    "\n",
    "\n",
    "$$\n",
    "\\mat{\n",
    "  \\hat{y}_{11} & \\hat{y}_{12} \\\\\n",
    "  \\vdots       & \\vdots       \\\\\n",
    "  \\hat{y}_{N1} & \\hat{y}_{N2} \\\\\n",
    "}\n",
    "=\n",
    "\\mat{\n",
    "  \\underl{x}_1^T \\\\\n",
    "  \\underl{x}_2^T \\\\\n",
    "  \\vdots \\\\\n",
    "  \\underl{x}_N^T\n",
    "}\n",
    "\\mat{\n",
    "  \\theta_{11} & \\theta_{12} \\\\\n",
    "  \\vdots      & \\vdots      \\\\\n",
    "  \\theta_{D1} & \\theta_{D2}\n",
    "}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
