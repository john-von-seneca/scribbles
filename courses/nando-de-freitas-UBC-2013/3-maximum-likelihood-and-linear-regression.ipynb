{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\LN}[1]{\\ln\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$\n",
    "\n",
    "$\\newcommand{\\mat}[1]{\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "#1\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "}\n",
    "$\n",
    "\n",
    "$\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr}\n",
    "#1\n",
    "\\end{array}\n",
    "}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "$\n",
    "\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet bold,\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\Abt}{\\Ab^T}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math bold\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood and Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian\n",
    "========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate Gaussian\n",
    "----------------------\n",
    "\n",
    "$$\n",
    "p(x) =\n",
    "\\fracone{\\sqrt{2\\pi\\sigma^2}}\n",
    "\\exp\n",
    "\\left\\{\n",
    "  -\\fracone{2\\sigma^2}\n",
    "  \\left(\n",
    "    x - \\mu\n",
    "  \\right)^2\n",
    "\\right\\}\n",
    "~~~ ~~~ ~~~ ~~~ \n",
    "x \\sim \\mathcal{N} \\left( \\mu, \\sigma^2 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling\n",
    "\n",
    "* sample x from $U(0,1)$\n",
    "* find $cum^{-1}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate\n",
    "--------------\n",
    "\n",
    "If $\\yb \\in \\mathbb{R}^n$, then\n",
    "$$\n",
    "p(\\yb)\n",
    "=\n",
    "\\left|\n",
    "  2 \\pi \\Sigma\n",
    "\\right|^{-1/2}\n",
    "\\exp\\left\\{\n",
    "  -\\half (\\yb-\\mub)^T \\Sigma^{-1} (\\yb-\\mub)\n",
    "\\right\\}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mub =\n",
    "\\mat{\n",
    "  \\mu_1 \\\\ \\vdots \\\\ \\mu_n\n",
    "}\n",
    "=\n",
    "\\mat{\n",
    "  \\E{y_1} \\\\ \\vdots \\\\ \\E{y_n}\n",
    "}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\Sigma = \n",
    "\\mat{\n",
    "  \\sigma_{11} & \\cdots & \\sigma_{1n} \\\\\n",
    "  \\vdots      & \\ddots & \\vdots      \\\\\n",
    "  \\sigma_{n1} & \\cdots & \\sigma_{nn}\n",
    "}\n",
    "=\n",
    "\\E{ (\\yb-\\mub) (\\yb-\\mub)^T }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bivariate: Example\n",
    "-------------------\n",
    "\n",
    "two independent Gaussian RV's.\n",
    "$$\n",
    "x_1 = \\mathcal{\\mu_1, \\sigma^2} ~~~ ~~~ ~~~\n",
    "x_2 = \\mathcal{\\mu_2, \\sigma^2}\n",
    "$$\n",
    "\n",
    "Joint distribution $p(x_1, x_2)$ is:\n",
    "\\begin{array}{rll}\n",
    "p(x_1, x_2) \n",
    "&=\n",
    "p(x_1) ~p(x_2)\n",
    "\\\\ &=\n",
    "\\frac{\\EXP{-\\fracone{2\\sigma^2} (x_1 - \\mu_1)^2}}\n",
    "{(2\\pi\\sigma^2)^{-1/2}}\n",
    "\\frac{\\EXP{-\\fracone{2\\sigma^2} (x_2- \\mu_2)^2}}\n",
    "{(2\\pi\\sigma^2)^{-1/2}}\n",
    "\\\\ &=\n",
    "\\frac{\n",
    "\\EXP{\n",
    "-\\half\n",
    "\\displaystyle \\sum_{i=1}^{2}\n",
    "\\left[\n",
    "  (x_i - \\mu_i)^T (\\sigma^2)^{-1} (x_i - \\mu_i)\n",
    "\\right]\n",
    "}\n",
    "}\n",
    "{2\\pi\\sigma^2}\n",
    "\\\\ &=\n",
    "\\frac{\n",
    "\\EXP{\n",
    "-\\half\n",
    "\\mat{ (x_2 - \\mu_1) & (x_2 - \\mu_2) }\n",
    "\\mat{\n",
    "  \\sigma^2 & 0 \\\\\n",
    "  0 & \\sigma^2\n",
    "}^{-1}\n",
    "\\mat{ (x_2 - \\mu_1) \\\\ (x_2 - \\mu_2) }\n",
    "}\n",
    "}\n",
    "{2\\pi\\sigma^2}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from a multivariate\n",
    "-----------------------------\n",
    "\n",
    "take the 1D case\n",
    "$$\n",
    "y \\sim \\mathcal{N}(0,\\sigma)\n",
    "\\sim \\sigma + \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "y_1 &\\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n",
    "y_2 &\\sim \\mu + \\sigma \\mathcal{N}(0, 1) \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "Remember the [properties](/notebooks/all-of-statistics/3-expectation.ipynb#Variance)\n",
    "$$\n",
    "\\arrthree{\n",
    "\\E{X_1 + X_2} &= \\E{X_1} + \\E{X_2}\\\\\n",
    "\\E{aX} &= a \\E{X}\\\\\n",
    "\\V{X + a} &= \\V{X}\\\\\n",
    "\\V{aX} &= a^2 \\V{X}\n",
    "}\n",
    "$$\n",
    "So\n",
    "$$\n",
    "\\E{y2} = \\E{\\mu} + \\sigma \\E{\\mathcal{N}(0,1)} = \\mu\n",
    "\\\\\n",
    "\\V{y2} = \\V{\\sigma \\mathcal{N}(0,1)} = \\sigma^2\n",
    "$$\n",
    "\n",
    "Similarly, for the multivariate case,\n",
    "$$\n",
    "\\underl{x} \\sim \\mathcal{N}(\\underl{\\mu}, \\Sigma)\n",
    "$$\n",
    "Using Cholesky decomposition, $\\Sigma = \\Bb \\Bt$, we have\n",
    "$$\n",
    "\\underl{x} \\sim \\underl{\\mu} + \\Bb \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "That is, sample D times from $\\mathcal{N}(0,1)$ and form a vector outta it, multiply it by **B** and then add $\\underl{\\mu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood\n",
    "============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "--------\n",
    "\n",
    "3 points $y_1 = 1, y_2 = 0.5, y_3 = 1.5$ which are independent and Gaussian with unknown mean $\\theta$ and variance 1.\n",
    "\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(\\theta, 1)\n",
    "=\n",
    "\\theta + \\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "Likelihood\n",
    "$$\n",
    "p(y_1 ~y_2 ~y_3 \\mid \\theta)\n",
    "=\n",
    "p(y_1 \\mid \\theta) ~p(y_2 \\mid \\theta) ~p(y_3 \\mid \\theta)\n",
    "$$\n",
    "\n",
    "Consider two guesses of $\\theta$, 1 and 2.5. which is more likely?\n",
    "\n",
    "ofc, $\\theta = 1$, since it is more likely.  \n",
    "Hence, in the light of this, we can define likelihood as the probability of seeing the data $\\D$ given the parameters of the model. tat is, the one that explains the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Linear regression\n",
    "----------------------\n",
    "\n",
    "Let\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(x_i^T \\theta, \\sigma^2)\n",
    "= x_i^T + \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\yb \\mid \\Xb, \\thetab, \\sigma)\n",
    "&=\n",
    "\\prodiN p(y_i \\mid \\xb_i \\thetab, \\sigma)\n",
    "\\\\ &=\n",
    "\\prodiN\n",
    "\\left( 2 \\pi \\sigma^2 \\right)^{-1/2}\n",
    "\\EXP{\n",
    "  -\\fracone{2\\sigma^2}\n",
    "  \\left( y_i - \\xb_i^T \\thetab \\right)^2\n",
    "}\n",
    "\\\\ &=\n",
    "\\left( 2 \\pi \\sigma^2 \\right)^{-N/2}\n",
    "\\EXP{\n",
    "  -\\fracone{2\\sigma^2}\n",
    "  \\sumiN\n",
    "  \\left( y_i - \\xb_i^T \\thetab \\right)^2\n",
    "}\n",
    "\\\\ &=\n",
    "\\left( 2 \\pi \\sigma^2 \\right)^{-N/2}\n",
    "\\EXP{\n",
    "  -\\fracone{2\\sigma^2}\n",
    "  \\left( \\yb - \\Xb \\thetab \\right)^T\n",
    "  \\left( \\yb - \\Xb \\thetab \\right)\n",
    "}\n",
    "}\n",
    "$$\n",
    "\n",
    "The exponent looks like the least squares objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function\n",
    "-------------\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "J(\\thetab)\n",
    "&=\n",
    "\\sumiN \\left(y_i - \\xb_i^T \\thetab \\right)^2\n",
    "\\\\\n",
    "p(\\yb \\mid \\Xb, \\thetab, \\sigma)\n",
    "&=\n",
    "\\left( 2 \\pi \\sigma^2 \\right)^{-N/2}\n",
    "\\EXP{\n",
    "  -\\fracone{2\\sigma^2}\n",
    "  \\sumiN\n",
    "  \\left( y_i - \\xb_i^T \\thetab \\right)^2\n",
    "}\n",
    "\\\\ &=\n",
    "\\prodiN\n",
    "\\left( 2 \\pi \\sigma^2 \\right)^{-1/2}\n",
    "\\EXP{\n",
    "  -\\fracone{2\\sigma^2}\n",
    "  \\left( y_i - \\xb_i^T \\thetab \\right)^2\n",
    "}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Likelihood\n",
    "==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there is a gaussian (say, of equal variance $\\sigma^2$) centered at the $\\hat{y}_i = \\xb_i^T \\thetab$, whose \"x\" axis is the axis of y or the label to be predicted.\n",
    "* now, the likelihood of $\\xb_i$ is given by $\\mathcal{N}(\\xb_i \\mid \\hat{y}_i, \\sigma^2)$\n",
    "* now, lin regression is all about increasing this likelihood for all the points in $\\D$\n",
    "* if the likelihood is increased, then the distance $\\Norm{\\hat{y}_i - y_i}$ decreases as well\n",
    "  * if likelihood increases, then the point $x_i$ moves closer to the mean, which in turn means the distance or the error decreases\n",
    "* the outliers screw up the fit, since the gaussian f tends to move the line closer to the outlier\n",
    "  * fix: use a [student's t](/notebooks/all-of-statistics/2-random-variables.ipynb#t-Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML\n",
    "---\n",
    "\n",
    "MLE of $\\thetab$ is obtained by taking the derivative of the log-likelihood, $\\log p(\\yb \\mid \\Xb, \\thetab, \\sigma)$. The goal is to maximize the likelihood of seeting the training data **y** by modifying the parameters $(\\thetab, \\sigma)$.\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "p(\\yb \\mid \\Xb, \\thetab, \\sigma)\n",
    "&= \n",
    "\\left( 2 \\pi \\sigma^2 \\right)^{-N/2}\n",
    "\\EXP{\n",
    "  -\\fracone{2\\sigma^2}\n",
    "  \\left( \\yb - \\Xb \\thetab \\right)^T\n",
    "  \\left( \\yb - \\Xb \\thetab \\right)\n",
    "}\n",
    "\\\\\n",
    "l(\\theta)\n",
    "&=\n",
    "-\\frac{N}{2} \\LN{2 \\pi \\sigma^2}\n",
    "-\\fracone{2\\sigma^2}\n",
    "\\left( \\yb - \\Xb \\thetab \\right)^T\n",
    "\\left( \\yb - \\Xb \\thetab \\right)\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Estimate of $\\theta$\n",
    "---------------------\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{\\partial \\ln(\\theta)}{\\partial \\thetab}\n",
    "&=\n",
    "-\\fracone{2\\sigma^2}\n",
    "\\left[\n",
    "  -2 \\Xb^T \\yb\n",
    "  + 2 \\Xb^T \\Xb \\thetab\n",
    "\\right]\n",
    "= 0 \n",
    "\\\\\n",
    "\\hat{\\thetab}_{ML}\n",
    "&=\n",
    "\\left( \\Xt \\Xb \\right)^{-1} \\Xt \\yb\n",
    "}\n",
    "$$\n",
    "\n",
    "same as that was obtained was using Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Estimate of $\\sigma$\n",
    "---------------------\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\frac{\\partial \\ln(\\theta)}{\\partial \\sigma^2}\n",
    "&=\n",
    "-\\frac{N}{2}\n",
    "\\fracone{\\sigma^2}\n",
    "+\n",
    "\\fracone{2\\sigma^4}\n",
    "\\left( \\yb - \\Xb \\thetab \\right)^T\n",
    "\\left( \\yb - \\Xb \\thetab \\right)\n",
    "= 0\n",
    "\\\\\n",
    "\\sigma^2 &=\n",
    "\\fracone{N}\n",
    "\\left( \\yb - \\Xb \\thetab \\right)^T\n",
    "\\left( \\yb - \\Xb \\thetab \\right)\n",
    "\\\\ &=\n",
    "\\fracone{N}\n",
    "\\sumiN\n",
    "\\left(\n",
    "  y_i - \\xb_i^T \\thetab\n",
    "\\right)^2\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions\n",
    "====================\n",
    "\n",
    "given training data $\\D = (\\Xb, \\yb)$, for a new input $\\xb_{*}$ and known $\\sigma^2$ is given by\n",
    "$$\n",
    "p(\\yb \\mid \\xb_{*}, \\D, \\hat{\\sigma}^2)\n",
    "= \\G \\left( \\yb \\mid \\xb_*^T \\thetab_{ML}, \\hat{\\sigma}^2 \\right)\n",
    "$$\n",
    "\n",
    "Risk management is all about reducing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequentist learning and ML\n",
    "=============================\n",
    "\n",
    "* Frequentist learning: assumes that there exists a true model, say, with parameters $\\thetab_0$\n",
    "* The estimate (learned value) will be denoted as $\\hat{\\thetab}$\n",
    "* Given $\\D = \\left\\{ \\xb_i \\right\\}_{i=1}^{N}$, we choose the  $\\theta$ which has more probability of generating the data. that is,\n",
    "$$\n",
    "\\hat{\\thetab} = \\argmax_{\\thetab} p(\\D \\mid \\thetab)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli: a model for coins\n",
    "-------------------------------\n",
    "\n",
    "great for text data\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = \n",
    "\\begin{cases}\n",
    "\\theta   & \\text{if } x=1 \\\\\n",
    "1-\\theta & \\text{if } x=0\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "That is,\n",
    "$$\n",
    "p(x \\mid \\theta) = \\theta^{x} (1-\\theta)^{1-x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy\n",
    "-------\n",
    "\n",
    "measure of uncertainty associated with a random variable. It is defined as,\n",
    "$$\n",
    "\\Hb(x) =\n",
    "-\\sum_x p(x \\mid \\theta) \\log p(x \\mid \\theta)\n",
    "$$\n",
    "\n",
    "For Bernoulli, entropy is\n",
    "$$\n",
    "\\arrthree{\n",
    "\\Hb(x) &=\n",
    "- \\displaystyle \\sum_{x=0}^{1}\n",
    "\\theta^x (1-\\theta)^{1-x}\n",
    "\\log \\left[\n",
    "  \\theta^x (1-\\theta)^{1-x}\n",
    "\\right]\n",
    "\\\\&=\n",
    "-\\left[\n",
    "  \\theta \\log \\theta + (1-\\theta) \\log(1-\\theta)\n",
    "\\right]\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02c6a88898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting entropy of bernoulli\n",
    "x = np.linspace(0+1e-10,1-1e-10,100)\n",
    "y = [-(1-xx)*math.log(1-xx) - xx*math.log(xx) for xx in x]\n",
    "plt.plot(x,y)\n",
    "plt.title('Entropy of Bernoulli')\n",
    "plt.ylabel('H(X)')\n",
    "plt.xlabel('Pr(X=1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entropy of unbiased coin is max, we are completely uncertain about the outcome\n",
    "* if the coin is biased, close to 0 or 1, then we will have a better idea about the outcome and we will get less information from the occurrence/event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Learning is about reducing the uncertainty about the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE Properties\n",
    "================\n",
    "\n",
    "Law of large numbers:  \n",
    "as $N \\rightarrow \\infty$, the sample average tends to expectation\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\hat{\\mathbb{E}}[X] &= \\fracone{N} \\sumiN \\xb_i & \\commentgray{for small N}\n",
    "\\\\\n",
    "\\E{X} &= \\int \\xb p(\\xb) ~d\\xb & \\commentgray{for large N}\n",
    "}\n",
    "$$\n",
    "when $\\xb_i \\sim p(\\xb)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for iid data from $p(x \\mid \\thetab_0)$, MLE minimizes the KL divergence:\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\hat{\\thetab} &=\n",
    "\\argmax_{\\thetab} \\prodiN p(\\xb_i \\mid \\thetab)\n",
    "\\\\ &=\n",
    "\\argmax_{\\thetab} \\sumiN \\log p(\\xb_i \\mid \\thetab)\n",
    "\\\\ &=\n",
    "\\argmax_{\\thetab}\n",
    "\\fracone{N} \\sumiN \\log(p(\\xb_i \\mid \\thetab))\n",
    "-\n",
    "\\fracone{N} \\sumiN \\log(p(\\xb_i \\mid \\thetab_o))\n",
    "\\\\ &=\n",
    "\\argmax_{\\thetab}\n",
    "\\fracone{N} \\sumiN \\log\n",
    "\\left(\n",
    " \\frac{p(\\xb_i \\mid \\thetab)}{p(\\xb_i \\mid \\thetab_o)}\n",
    "\\right)\n",
    "\\\\ & \\xrightarrow{N \\rightarrow \\infty}\n",
    "\\argmin_{\\thetab}\n",
    "\\int\n",
    "\\log\n",
    "\\left(\n",
    " \\frac{p(\\xb \\mid \\thetab)}{p(\\xb \\mid \\thetab_o)}\n",
    "\\right)\n",
    "p(\\xb \\mid \\thetab_0) ~d\\xb\n",
    "\\\\\n",
    "}\n",
    "$$\n",
    "This is KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we assumed $\\xb_i \\sim p(\\xb \\mid \\thetab_0)$. why do we do that? coz $\\thetab_0$ is the true model in frequentist paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\argmin_{\\thetab}\n",
    "\\int\n",
    "\\log\n",
    "\\left(\n",
    " \\frac{p(\\xb \\mid \\thetab)}{p(\\xb \\mid \\thetab_o)}\n",
    "\\right)\n",
    "p(\\xb \\mid \\thetab_0) ~d\\xb\n",
    "\\\\\n",
    "\\argmin_{\\thetab}\n",
    "\\underbrace{\n",
    "\\int p(\\xb \\mid \\thetab_0)\n",
    "\\log p(\\xb \\mid \\thetab_0) ~dx\n",
    "}_{\\text{information world}}\n",
    "-\n",
    "\\underbrace{\n",
    "\\int p(\\xb \\mid \\thetab_0)\n",
    "\\log p(\\xb \\mid \\thetab) ~dx\n",
    "}_{\\text{in model}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if the information in the real world matches the information in my model, then we have achieved the true minimum.\n",
    "* First term: negative entropy or information\n",
    "* Second term: cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theoretical statistics  \n",
    "by alex wishart, ruben zumar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "under smoothness and identifiability assumptions, the MLE is consistent:\n",
    "$$\n",
    "\\arrthree{\n",
    "\\hat{\\thetab} &\\xrightarrow{p} \\thetab_0\n",
    "\\\\\n",
    "plim(\\hat{\\thetab}) &= \\thetab_0\n",
    "\\\\\n",
    "\\Lim{N \\rightarrow \\infty}\n",
    "p( \\left| \\hat{\\thetab} - \\thetab_0 \\right| > \\alpha) &\\rightarrow 0\n",
    "} \n",
    "$$\n",
    "All the three formulations are equivalent and can be used interchangeably "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MLE is asymptotically normal\n",
    "* as $N \\rightarrow \\infty$, we have:\n",
    "$$\n",
    "(\\hat{\\thetab} - \\thetab_0) \\Rightarrow \\mathcal{N}(0, I^{-1})\n",
    "$$\n",
    "where I is the fisher information matrix.\n",
    "from All of statistics\n",
    "\n",
    "* It is asymptotically optimal or efficient.\n",
    "* asymptotically, it has the lowerst vriance among all well behaved estimators.\n",
    "* attains a lower bound on the CLT variance known as the Cramer-Rao lower bound\n",
    "* is MLE always the right option?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* But only for $N \\rightarrow \\infty$\n",
    "* problems posed in this setting tend to be intracable\n",
    "* belong to NP Hard or #P complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and Variance\n",
    "=================\n",
    "\n",
    "* estimator is a function of the data $\\hat{\\thetab} = g(\\D)$\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\thetab}) = \\mathbb{E}_{\\D \\mid \\thetab_0}(\\hat{\\thetab}) - \\thetab_0 = \\bar{\\thetab} - \\thetab_0\n",
    "$$\n",
    "\n",
    "$\\bar{\\thetab}$ is the average of all the estimates for $\\thetab$ for different data sets $\\D$ sampled from the model given by $\\thetab_0$\n",
    "$$\n",
    "\\bar{\\thetab} = \\int \\hat{\\thetab} ~p(\\D \\mid \\thetab_0) ~d\\D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forests used in kinect, lotta estimators which have a large variance and then average the estimates from these and you will get an estimate close to the true value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance is\n",
    "$$\n",
    "\\V{\\hat{\\thetab}}\n",
    "=\n",
    "\\mathbb{E}_{p(\\D \\mid \\thetab_0)}(\\hat{\\thetab} - \\bar{\\thetab})^2\n",
    "$$\n",
    "\n",
    "MSE is\n",
    "$$\n",
    "\\mathbb{E}_{p(\\D \\mid \\thetab_0)}(\\hat{\\thetab} - \\bar{\\thetab})^2\n",
    "=\n",
    "\\left(\n",
    "  \\bar{\\thetab} - \\thetab_0\n",
    "\\right)^2\n",
    "+\n",
    "\\mathbb{E}_{p(\\D \\mid \\thetab_0)}(\\hat{\\thetab} - \\bar{\\thetab})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
