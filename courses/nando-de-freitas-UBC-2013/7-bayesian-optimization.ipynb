{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rnd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pprint import pprint\n",
    "import functools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization\n",
    "$\n",
    "\\newcommand{\\Brace}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\Bracket}[1]{\\left[#1\\right]}\n",
    "\\newcommand{\\cases}[1]{\\begin{cases}#1\\end{cases}}\n",
    "\\newcommand{\\cov}[1]{\\text{cov} \\sigma\\left[#1\\right]}\n",
    "\\newcommand{\\diff}[2]{\\frac{d #1}{d #2}}\n",
    "\\newcommand{\\difftwo}[2]{\\frac{d^2 #1}{d {#2}^2}}\n",
    "\\newcommand{\\diffn}[2]{{#1}^{\\prime}(#2)}\n",
    "\\newcommand{\\E}[1]{\\mathbb{E}\\left[ #1 \\right]}\n",
    "\\newcommand{\\expb}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\EXP}[1]{\\exp\\left\\{#1\\right\\}} \n",
    "\\newcommand{\\frachalf}[1]{\\frac{#1}{2}~}\n",
    "\\newcommand{\\fracone}[1]{\\frac{1}{#1}~}\n",
    "\\newcommand{\\fracrec}[1]{\\frac{1}{#1}~}\n",
    "\\newcommand{\\half}{\\fracone{2}}\n",
    "\\newcommand{\\H}[1]{\\mathbb{H}\\left[#1\\right]}\n",
    "\\newcommand{\\Int}[2]{\\displaystyle \\int_{#1}^{#2}~}\n",
    "\\newcommand{\\intinfinf}{\\Int{-\\infty}{\\infty}}\n",
    "\\newcommand{\\inv}[1]{#1^{-1}}\n",
    "\\newcommand{\\invp}[1]{\\left({#1}\\right)^{-1}}\n",
    "\\newcommand{\\KL}[2]{\\text{KL}\\left(#1 \\Vert #2\\right)}\n",
    "\\newcommand{\\Lim}[1]{\\displaystyle \\lim_{#1}}\n",
    "\\newcommand{\\Ln}[1]{\\ln \\left\\(#1\\right\\)}\n",
    "\\newcommand{\\Lnb}[1]{\\ln \\left\\{#1\\right\\} }\n",
    "\\newcommand{\\Mod}[1]{\\left|#1\\right|}\n",
    "\\newcommand{\\Norm}[1]{\\left\\lVert #1 \\right\\rVert}\n",
    "\\newcommand{\\Normsqr}[1]{\\Norm{#1}^2}\n",
    "\\newcommand{\\map}[1]{#1_{\\text{MAP}}}\n",
    "\\newcommand{\\ml}[1]{#1_{\\text{ML}}}\n",
    "\\newcommand{\\MI}[1]{\\mathcal{I}\\left(#1\\right)}\n",
    "\\newcommand{\\P}{\\mathbb{P}}\n",
    "\\newcommand{\\Paran}[1]{\\left(#1\\right)}\n",
    "\\newcommand{\\Partial}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\sqrbrkt}[1]{\\Bracket{#1}^2}\n",
    "\\newcommand{\\sqrbrc}[1]{\\Brace{#1}^2}\n",
    "\\newcommand{\\trace}[1]{\\text{Tr}\\left( #1 \\right)}\n",
    "\\newcommand{\\traceb}[1]{\\text{Tr}\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\underl}[1]{\\text{$\\underline{#1}$}}\n",
    "\\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}\n",
    "$\n",
    "$\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\mat}[1]{ \\left[ \\begin{matrix} #1 \\end{matrix} \\right] }\n",
    "\\newcommand{\\matp}[1]{ \\left( \\begin{matrix} #1 \\end{matrix} \\right)}\n",
    "\\newcommand{\\mats}[1]{ \\begin{matrix}#1\\end{matrix} }\n",
    "\\newcommand{\\arrthree}[1]{\n",
    "\\begin{array}{rlr} #1 \\end{array}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Ca}{\\mathcal{C}}\n",
    "\\newcommand{\\Caone}{\\Ca_1}\n",
    "\\newcommand{\\Catwo}{\\Ca_2}\n",
    "\\newcommand{\\Cak}{\\Ca_k}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\G}{\\mathcal{G}}\n",
    "\\newcommand{\\I}{\\mathcal{I}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Ra}{\\mathcal{R}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\commentgray}[1]{\\color{gray}{\\text{#1}}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum, product\n",
    "$\n",
    "\\newcommand{\\sumi}{\\displaystyle \\sum_i}\n",
    "\\newcommand{\\sumiD}{\\displaystyle \\sum_{i=1}^{D}}\n",
    "\\newcommand{\\sumiL}{\\displaystyle \\sum_{i=1}^{L}}\n",
    "\\newcommand{\\sumiN}{\\displaystyle \\sum_{i=1}^{N}}\n",
    "\\newcommand{\\sumjD}{\\displaystyle \\sum_{j=1}^{D}}\n",
    "\\newcommand{\\sumjK}{\\displaystyle \\sum_{j=1}^{K}}\n",
    "\\newcommand{\\sumjMl}{\\sum_{j=1}^{M-1}}\n",
    "\\newcommand{\\sumkK}{\\displaystyle \\sum_{k=1}^{K}}\n",
    "\\newcommand{\\sumkM}{\\displaystyle \\sum_{k=1}^{M}}\n",
    "\\newcommand{\\sumkMl}{\\sum_{k=1}^{M-1}}\n",
    "\\newcommand{\\summN}{\\displaystyle \\sum_{m=1}^{N}}\n",
    "\\newcommand{\\sumnN}{\\displaystyle \\sum_{n=1}^{N}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\prodi}{\\displaystyle \\prod_i}\n",
    "\\newcommand{\\prodiD}{\\displaystyle \\prod_{i=1}^{D}}\n",
    "\\newcommand{\\prodiL}{\\displaystyle \\prod_{i=1}^{L}}\n",
    "\\newcommand{\\prodiN}{\\displaystyle \\prod_{i=1}^{N}}\n",
    "\\newcommand{\\prodjK}{\\displaystyle \\prod_{j=1}^{K}}\n",
    "\\newcommand{\\prodkK}{\\displaystyle \\prod_{k=1}^{K}}\n",
    "\\newcommand{\\prodmN}{\\displaystyle \\prod_{m=1}^{N}}\n",
    "\\newcommand{\\prodnN}{\\displaystyle \\prod_{n=1}^{N}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphabet shortcuts\n",
    "$\n",
    "\\newcommand{\\ab}{\\mathbf{a}}\n",
    "\\newcommand{\\at}{\\ab^T}\n",
    "\\newcommand{\\Ab}{\\mathbf{A}}\n",
    "\\newcommand{\\At}{\\Ab^T}\n",
    "\\newcommand{\\Ai}{\\inv{\\Ab}}\n",
    "\\newcommand{\\Abjk}{\\Ab_{jk}}\n",
    "\\newcommand{\\bb}{\\mathbf{b}}\n",
    "\\newcommand{\\bt}{\\bb^T}\n",
    "\\newcommand{\\Bb}{\\mathbf{B}}\n",
    "\\newcommand{\\Bt}{\\Bb^T}\n",
    "\\newcommand{\\Cb}{\\mathbf{C}}\n",
    "\\newcommand{\\Cn}{\\Cb_{N}}\n",
    "\\newcommand{\\Db}{\\mathbf{D}}\n",
    "\\newcommand{\\fb}{\\mathbf{f}}\n",
    "\\newcommand{\\fp}{f^{\\prime}}\n",
    "\\newcommand{\\Hb}{\\mathbf{H}}\n",
    "\\newcommand{\\hx}{h(\\xb)}\n",
    "\\newcommand{\\Jb}{\\mathbf{J}}\n",
    "\\newcommand{\\kb}{\\mathbf{k}}\n",
    "\\newcommand{\\kt}{\\kb^T}\n",
    "\\newcommand{\\Kb}{\\mathbf{K}}\n",
    "\\newcommand{\\Lb}{\\mathbf{L}}\n",
    "\\newcommand{\\Lt}{\\Lb^T}\n",
    "\\newcommand{\\Lbi}{\\Lb^{-1}}\n",
    "\\newcommand{\\mb}{\\mathbf{m}}\n",
    "\\newcommand{\\mt}{\\mb^T}\n",
    "\\newcommand{\\mbn}{\\mb_N}\n",
    "\\newcommand{\\mbnt}{\\mbn^T}\n",
    "\\newcommand{\\mbN}{\\mb_N}\n",
    "\\newcommand{\\mbNt}{\\mbn^T}\n",
    "\\newcommand{\\Mb}{\\mathbf{M}}\n",
    "\\newcommand{\\Qb}{\\mathbf{Q}}\n",
    "\\newcommand{\\Rb}{\\mathbf{R}}\n",
    "\\newcommand{\\sb}{\\mathbf{s}}\n",
    "\\newcommand{\\Sb}{\\mathbf{S}}\n",
    "\\newcommand{\\tb}{\\mathbf{t}}\n",
    "\\newcommand{\\tbnn}{\\tb_{N}}\n",
    "\\newcommand{\\tbnp}{\\tb_{N+1}}\n",
    "\\newcommand{\\tt}{\\tb^T}\n",
    "\\newcommand{\\Tb}{\\mathbf{T}}\n",
    "\\newcommand{\\Tt}{\\Tb^T}\n",
    "\\newcommand{\\ub}{\\mathbf{u}}\n",
    "\\newcommand{\\Ub}{\\mathbf{U}}\n",
    "\\newcommand{\\Ut}{\\Ub^T}\n",
    "\\newcommand{\\vb}{\\mathbf{v}}\n",
    "\\newcommand{\\Vb}{\\mathbf{V}}\n",
    "\\newcommand{\\wb}{\\mathbf{w}}\n",
    "\\newcommand{\\wnr}[1]{\\wb^{(\\text{#1})}}\n",
    "\\newcommand{\\wt}{\\wb^T}\n",
    "\\newcommand{\\Wb}{\\mathbf{W}}\n",
    "\\newcommand{\\Wt}{\\Wb^T}\n",
    "\\newcommand{\\Wtilde}{\\widetilde{\\Wb}}\n",
    "\\newcommand{\\Wtildet}{\\Wtilde^T}\n",
    "\\newcommand{\\Xb}{\\mathbf{X}}\n",
    "\\newcommand{\\Xt}{\\Xb^T}\n",
    "\\newcommand{\\Xtilde}{\\widetilde{\\Xb}}\n",
    "\\newcommand{\\Xtildet}{\\Xtilde^T}\n",
    "\\newcommand{\\xb}{\\mathbf{x}}\n",
    "\\newcommand{\\xt}{\\xb^T}\n",
    "\\newcommand{\\xtilde}{\\widetilde{\\xb}}\n",
    "\\newcommand{\\xtilden}{\\xtilde_n}\n",
    "\\newcommand{\\xtildent}{\\xtilden^T}\n",
    "\\newcommand{\\xp}{x^{\\prime}}\n",
    "\\newcommand{\\xbp}{\\xb^{\\prime}}\n",
    "\\newcommand{\\xbm}{\\xb_m}\n",
    "\\newcommand{\\xbn}{\\xb_n}\n",
    "\\newcommand{\\xbnp}{\\xb_{N+1}}\n",
    "\\newcommand{\\xab}{\\mathbf{x_a}}\n",
    "\\newcommand{\\xabt}{\\mathbf{x_a}^T}\n",
    "\\newcommand{\\xbb}{\\mathbf{x_b}}\n",
    "\\newcommand{\\xbbt}{\\mathbf{x_b}^T}\n",
    "\\newcommand{\\yb}{\\mathbf{y}}\n",
    "\\newcommand{\\yt}{\\yb^T}\n",
    "\\newcommand{\\yx}{y(\\xb)}\n",
    "\\newcommand{\\zb}{\\mathbf{z}}\n",
    "\\newcommand{\\zt}{\\zb^T}\n",
    "\\newcommand{\\zbm}{\\zb_m}\n",
    "\\newcommand{\\zbn}{\\zb_n}\n",
    "\\newcommand{\\zbnp}{\\zb_{n-1}}\n",
    "\\newcommand{\\znk}{\\zb_{nk}}\n",
    "\\newcommand{\\znpj}{\\zb_{n-1,j}}\n",
    "\\newcommand{\\Zb}{\\mathbf{Z}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "math shortcuts\n",
    "$\n",
    "\\newcommand{\\chib}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\etab}{\\pmb{\\eta}}\n",
    "\\newcommand{\\etat}{\\eta^T}\n",
    "\\newcommand{\\etabt}{\\etab^T}\n",
    "\\newcommand{\\Lambdab}{\\pmb{\\Lambda}}\n",
    "\\newcommand{\\laa}{\\Lambda_{aa}}\n",
    "\\newcommand{\\laai}{\\Lambda_{aa}^{-1}}\n",
    "\\newcommand{\\lab}{\\Lambda_{ab}}\n",
    "\\newcommand{\\lba}{\\Lambda_{ba}}\n",
    "\\newcommand{\\lbb}{\\Lambda_{bb}}\n",
    "\\newcommand{\\lbbi}{\\Lambda_{bb}^{-1}}\n",
    "\\newcommand{\\li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\Li}{\\Lambda^{-1}}\n",
    "\\newcommand{\\mub}{\\pmb{\\mu}}\n",
    "\\newcommand{\\mut}{\\mub^T}\n",
    "\\newcommand{\\muab}{\\pmb{\\mu}_a}\n",
    "\\newcommand{\\mubb}{\\pmb{\\mu}_b}\n",
    "\\newcommand{\\Phib}{\\pmb{\\Phi}}\n",
    "\\newcommand{\\Phibt}{\\Phib^T}\n",
    "\\newcommand{\\pib}{\\pmb{\\pi}}\n",
    "\\newcommand{\\sigmasqr}{\\sigma^2}\n",
    "\\newcommand{\\sigmanoise}{\\sigma_{\\text{noise}}^2}\n",
    "\\newcommand{\\saa}{\\Sigma_{aa}}\n",
    "\\newcommand{\\sab}{\\Sigma_{ab}}\n",
    "\\newcommand{\\sba}{\\Sigma_{ba}}\n",
    "\\newcommand{\\sbb}{\\Sigma_{bb}}\n",
    "\\newcommand{\\Sigmai}{\\inv{\\Sigma}}\n",
    "\\newcommand{\\thetab}{\\pmb{\\theta}}\n",
    "\\newcommand{\\thetat}{\\thetab^T}\n",
    "\\newcommand{\\thetabh}{\\hat{\\thetab}}\n",
    "\\newcommand{\\thetaold}{\\thetab^{\\text{old}}}\n",
    "$\n",
    "$\n",
    "\\newcommand{\\zerob}{\\pmb{0}}\n",
    "\\newcommand{\\ed}{\\mathbb{E}_{\\D}}\n",
    "\\newcommand{\\edyx}{\\ed\\left[y(\\xb ; \\D)\\right]}\n",
    "\\newcommand{\\dx}{~dx}\n",
    "\\newcommand{\\dxb}{~d\\xb}\n",
    "\\newcommand{\\pxdxb}{p(\\xb) \\dxb}\n",
    "\\newcommand{\\dwb}{~d\\wb}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aliases for distributions\n",
    "$\\newcommand{\\multivarcoeff}{\\frac{1}{(2\\pi)^{D/2}}\n",
    "\\frac{1}{\\left| \\mathbf{\\Sigma}\\right|^{1/2}}}$\n",
    "$\\newcommand{\\multivarexp}[2]\n",
    "{\n",
    "\\left\\{\n",
    " -\\frac{1}{2} \n",
    " {#1}^T \n",
    " #2\n",
    " {#1}\n",
    "\\right\\}\n",
    "}$\n",
    "$\\newcommand{\\multivarexpx}[1]{\\multivarexp{#1}{\\Sigma^{-1}}}$\n",
    "$\\newcommand{\\multivarexpstd}{\\multivarexpx{(\\xb-\\mub)}}$\n",
    "$\\newcommand{\\gam}{\\operatorname{Gam}}$\n",
    "$\n",
    "\\newcommand{\\Nl}[3]{\\mathcal{N}\\left(#1 \\mid #2, #3\\right)}\n",
    "\\newcommand{\\Nstdx}{\\Nl{\\mathbf{x}}{\\mathbf{\\mu}}{\\Sigma}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Action $\\Brace{1, \\dots, K}$  \n",
    "* rewards $\\xb(t) \\in [0, 1]^K$\n",
    "* sequence of trials $t = 1, \\dots, T$\n",
    "\n",
    "* Trade-off between exploration and exploitation\n",
    "* Regret = Player reward - Reward of best action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the x-axis represents the set of all bandits and we make two observations.  \n",
    "\n",
    "that is, we choose two bandits and evaluate them and get two data points.  \n",
    "\n",
    "from these, we can fit a GP that \"passes\" thro the two observations we have made.   \n",
    "\n",
    "the goal is to find the bandit which has the max. obj. value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is huge uncertainty in unexplored areas, which are good for exploration.  \n",
    "\n",
    "there is less uncertainty in explored areas and the area where the objective is high is good for exploitation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use an auxiliary function called the **acquisision** function which tells us wher to sample next. in this context, which bandit to try.  \n",
    "\n",
    "the arg max of the acquisition functions gives us the next sample point.  \n",
    "\n",
    "after we sample, we recompute the GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **for** t = 1, .. **do**\n",
    "  1. find $\\xb_t$ by combining attributes of the posterior distribution in a utility function u and maximizing\n",
    "  $$\n",
    "  \\xb_t = \\argmax_{\\xb} u(\\xb \\mid \\D_{1:t-1})\n",
    "  $$\n",
    "  1. sample the objective function:\n",
    "  $$\n",
    "  y_t = f(\\xb_t) + \\epsilon_t\n",
    "  $$\n",
    "  1. augmet the data $$\\D_{1:t} = \\Brace{\\D_{1:t-1}; (\\xb_t, y_t)}$$\n",
    "  1. update the GP\n",
    "1. **end for**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploration-exploitation trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the expressions for GP prediction\n",
    "$$\n",
    "\\arrthree{\n",
    "p(y_{t+1} \\mid |D_{1:t}, \\xb_{t+1})\n",
    "&=\n",
    "\\Nl{y}{\\mu_t(\\xb_{t+1})}{\\sigma_t^2(\\xb_{t+1}) + \\sigmanoise}\n",
    "\\\\\n",
    "\\mu_t(\\xb_{t+1})\n",
    "&=\n",
    "\\kt \\inv{\\Bracket{\\Kb + \\sigmanoise \\I}} \\yb_{1:t}\n",
    "\\\\ \n",
    "\\sigma_t^2(\\xb_{t+1})\n",
    "&=\n",
    "\\kappa(\\xb_{t+1}, \\xb_{t+1}) - \\kt \\inv{\\Bracket{\\Kb + \\sigmanoise \\I}} \\kb\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we shoud choose the next point $\\color{green}{\\xb}$ where\n",
    "* the mean is high (**exploitation**)\n",
    "* the variance is high (**exploration**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could balance this tradeoff with an **acquisition function** as\n",
    "$$\n",
    "\\mu(\\xb) + \\kappa \\sigma(\\xb)\n",
    "$$\n",
    "where $\\kappa$ is a trade-off parameter between exploration($\\sigma$) and exploitation($\\mu$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* aka **infill, figure of merit**\n",
    "* guides the optimization by determining which $\\xb_{t+1}$ to observe next\n",
    "* uses predictive posterior to combine \n",
    "  * exploration (high-variance regions)\n",
    "  * exploitation (high-mean regions)\n",
    "* optimize to find sample point, can be done cheaply/approximately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example - probability of improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\mathbf{PI}(\\xb)\n",
    "&=\n",
    "P(f(\\xb) \\ge \\mu^+ + \\xi)\n",
    "\\\\ &=\n",
    "\\Phib\\Paran{\\frac{\\mu(\\xb) - \\mu_+ - \\xi}{\\sigma(\\xb)}}\n",
    "}\n",
    "$$\n",
    "where $\\mu_+$ is the best observed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People as Bayesian reasoners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes and decision theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Utilitarian view\n",
    "  * we need models to make the right decisions under uncertainty\n",
    "  * inference and decision making are intertwined\n",
    "* Learned posterior\n",
    "\\begin{cases}\n",
    "P(x = \\text{healthy} &\\mid ~\\D) &= 0.9 \\\\\n",
    "P(x = \\text{cancer}  &\\mid ~\\D) &= 0.1 \\\\\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost/reward model\n",
    "\n",
    "\\begin{array}{|l|c|c|}\n",
    "\\hline\n",
    " & \\ab = \\text{no treatment} & \\ab = \\text{treatment}\n",
    " \\\\ \\hline\n",
    "\\xb = \\text{healthy} & 0 &  -30\n",
    "\\\\ \\hline\n",
    "\\xb = \\text{cancer} & -100 & -20\n",
    "\\\\ \\hline\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we choose the action that maximizes the expected utility or equivalently which minimizes the expected cost\n",
    "$$\n",
    "\\mathbf{EU}(\\ab) = \\displaystyle \\sum_\\xb u(\\xb, \\ab) ~P(\\xb \\mid \\D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let h = healthy, cancer = c, treatment = t, notreatment = nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\arrthree{\n",
    "\\mathbf{EU}(a=t)\n",
    "&= \n",
    "\\overbrace{u(h, t)}^{-30} ~\\overbrace{p(x = h \\mid \\D)}^{0.9}\n",
    "+\n",
    "\\overbrace{u(c, t)}^{-20} ~\\overbrace{p(x = c \\mid \\D)}^{0.1}\n",
    "&= -29\n",
    "\\\\\n",
    "\\mathbf{EU}(a=nt)\n",
    "&=\n",
    "\\underbrace{u(h, nt)}_{0} ~\\underbrace{p(x = h \\mid \\D)}_{0.9}\n",
    "+\n",
    "\\underbrace{u(c, nt)}_{-100} ~\\underbrace{p(x = c \\mid \\D)}_{0.1}\n",
    "&= -10\n",
    "}\n",
    "$$\n",
    "\n",
    "SO you do not treat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An expected utility criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At iteration n+1, choose the point that minimizes the distance to the objective evaluated at the maximum $\\xb_*$\n",
    "$$\n",
    "\\arrthree{\n",
    "\\xb_{n+1}\n",
    "&=\n",
    "\\displaystyle \\argmin_{\\xb}\n",
    "\\E{\\Norm{f_{n+1}(\\xb) - f(\\xb_*)} \\mid \\D}\n",
    "\\\\ &=\n",
    "\\argmin_{\\xb}\n",
    "\\Int{}{} \\Norm{f_{n+1}(\\xb) - f(\\xb_*)} ~p(f_{n+1} \\mid \\D) ~df_{n+1}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we donno the true objective at the maximum. to overcome this, Mockus proposed the following acquisition function\n",
    "$$\n",
    "\\xb = \\argmax_{\\xb} ~\\E{\\max\\Brace{0, f_{n+1}(\\xb) - f^{\\text{max}}} \\mid \\D_n }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\xb = \\argmax_{\\xb} ~\\E{\\max\\Brace{0, f_{n+1}(\\xb) - f^{\\text{max}}} \\mid \\D_n }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this acquisition, we can obtain an analytical expression for expected improvement (**EI**)\n",
    "$$\n",
    "\\arrthree{\n",
    "\\mathbf{EI}(\\xb) &=\n",
    "\\cases{\n",
    "(\\mu(\\xb) - \\mu^+ - \\xi) ~\\Phi(Z) + \\sigma(\\xb) \\phi(Z) \n",
    "& \\text{if} ~\\sigma(\\xb) \\gt 0\n",
    "\\\\\n",
    "0 \n",
    "&\n",
    "\\text{if} ~\\sigma(\\xb) = 0\n",
    "}\n",
    "\\\\\n",
    "Z &=\n",
    "\\frac{\\mu(\\xb) - \\mu^+ - \\xi}{\\sigma(\\xb)}\n",
    "}\n",
    "$$\n",
    "where\n",
    "* $\\phi(.)$ - pdf of standard Normal\n",
    "* $\\Phi(.)$ - cdf of standard Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A third criterion: GP-UCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Srinivas et al, 2010  \n",
    "\n",
    "\n",
    "define the regret and cumulative regret as follows\n",
    "$$\n",
    "\\arrthree{\n",
    "r(\\xb) &=\n",
    "f(\\xb_*) - f(\\xb)\n",
    "\\\\\n",
    "R_T &=\n",
    "r(\\xb_1) + \\cdots + r(\\xb_T)\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Te GP-UCB criterion is as follows:\n",
    "$$\n",
    "\\text{GP-UCB}(\\xb) = \\mu(\\xb) + \\sqrt{\\nu \\beta_t} ~\\sigma(\\xb)\n",
    "$$\n",
    "$\\beta$ is set using a simple concentration bound:\n",
    "> with $\\nu = 1$ and $\\beta_t = 2 \\log\\Paran{\\frac{\\pi^2 t^{d/2+2}}{3\\delta}}$\n",
    "> it can be shown with high probability that this method is no regret, that is,\n",
    "> $$\\Lim{T \\rightarrow \\infty} R_T / T = 0$$ \n",
    "> This in turn implies a lower-bound on the convergence rate for the optmization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fourth criterion: Thompson sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets say i have 3 data points and we need to optimize over an interval.  \n",
    "\n",
    "we fit a GP to these points and get the confidence intervals ($\\mu \\pm \\sigma$)  \n",
    "\n",
    "if i sample from this, i would get a function inside these confidence intervals with a very high probability\n",
    "\n",
    "this would go very near the data points we have already seen since the confidence intervals get squished near the data points.\n",
    "\n",
    "this approach takes well or prefers exploitation over exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu^+ = \\displaystyle \\argmax_{\\xb_i \\in \\xb_{1:t}} \\mu(\\xb_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of improvement\n",
    "\n",
    "Kushner 1964\n",
    "$$\n",
    "\\mathbf{PI}(\\xb) = \n",
    "\\Phi \\Paran{\n",
    "\\frac{\\mu(\\xb) - \\mu^+ - \\xi}{\\sigma(\\xb)}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Improvement\n",
    "\n",
    "Mockus 1978\n",
    "\n",
    "$$\n",
    "\\arrthree{\n",
    "\\mathbf{EI}(\\xb) &=\n",
    "\\cases{\n",
    "(\\mu(\\xb) - \\mu^+ - \\xi) ~\\Phib(Z) + \\sigma(\\xb) \\phi(Z) \n",
    "& \\text{if} ~\\sigma(\\xb) \\gt 0\n",
    "\\\\\n",
    "0 \n",
    "&\n",
    "\\text{if} ~\\sigma(\\xb) = 0\n",
    "}\n",
    "\\\\\n",
    "Z &=\n",
    "\\frac{\\mu(\\xb) - \\mu^+ - \\xi}{\\sigma(\\xb)}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper confidence bound\n",
    "\n",
    "Srinivas et al 2010\n",
    "\n",
    "$$\n",
    "\\text{GP-UCB}(\\xb)\n",
    "=\n",
    "\\mu(\\xb) + \\sqrt{\\nu \\tau_t} ~\\sigma(\\xb)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolios of acquisition functions help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why BO works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the following four curves\n",
    "* actual function f\n",
    "* mean function $\\mu$\n",
    "* upper confidence bound $\\mu + \\beta \\sigma$\n",
    "* lower confidence bound $\\mu - \\beta \\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the max of the lower bound, $LB_{\\max}$.\n",
    "\n",
    "the upper confidence bound says this is the best i can do.  \n",
    "the lower confidence bound says this is the worst i can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, if there is any point for which the upper confidence bound is less than $LB_{\\max}$, then there is no point in trying that point\n",
    "\n",
    "its not worth your salt to explore such areas which are below $LB_{\\max}$\n",
    "\n",
    "so discard all the regions for which the upper confidence bound is lower than $LB_{\\max}$.\n",
    "\n",
    "This is called the branch-and-bound heuristic. here it is probabilistic branch-and-bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent user interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Tuning NP hard problem solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why random tuning works sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Tuning random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Tuning Hybrid Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical policy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **High-level** model-based learning for deciding when to navigate, park, pickup and dropoff passengers\n",
    "* **Mid-level** active path learning for navigating a toplological map\n",
    "* **Low-level** active policy optimizer to learn control of continuous non-linear vehicle dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active path finding in middle level\n",
    "\n",
    "Mid-level *Navigate* policy generates sequences of waypoints on a topological map to navigate from a location to a destination. $V(\\theta)$ value function represents the path length from the current node to the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level: Trajectory following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical systems apply to many robot tasks - key to build large systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaze planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
