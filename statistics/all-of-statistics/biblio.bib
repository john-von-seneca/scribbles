Automatically generated by Mendeley Desktop 1.15
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Rusu2015,
abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
archivePrefix = {arXiv},
arxivId = {1511.06295},
author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
eprint = {1511.06295},
file = {:Users/dragon/neo-human/papers/Rusu et al. - 2015 - Policy Distillation.pdf:pdf},
month = {nov},
title = {{Policy Distillation}},
url = {http://arxiv.org/abs/1511.06295},
year = {2015}
}
@article{Fischer2012,
abstract = {Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided.},
author = {Fischer, Asja and Igel, Christian},
doi = {10.1007/978-3-642-33275-3{\_}2},
file = {:Users/dragon/neo-human/papers/Fischer, Igel - 2012 - An Introduction to Restricted Boltzmann Machines.pdf:pdf},
isbn = {978-3-642-33274-6},
issn = {1875-7855},
journal = {Lecture Notes in Computer Science: Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
pages = {14--36},
pmid = {24309266},
title = {{An Introduction to Restricted Boltzmann Machines}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33275-3{\_}2},
volume = {7441},
year = {2012}
}
@misc{Robertson1977,
abstract = {The principle that, for optimal retrieval, documents should be ranked in order of the probability of relevance or usefulness has been brought into question by Cooper. It is shown that the principle can be justified under certain assumptions, but that in cases where these assumptions do not hold, the principle is not valid. The major problem appears to lie in the way the principle considers each document independently of the rest. The nature of the information on the basis of which the system decides whether or not to retrieve the documents determines whether the document-by-document approach is valid.},
author = {Robertson, S.E.},
booktitle = {Journal of Documentation},
doi = {10.1108/eb026647},
file = {:Users/dragon/neo-human/papers/Robertson - 1977 - the Probability Ranking Principle in Ir.pdf:pdf},
isbn = {1558604545},
issn = {0022-0418},
number = {4},
pages = {294--304},
title = {{the Probability Ranking Principle in Ir}},
volume = {33},
year = {1977}
}
@article{Belkin1992,
abstract = {Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented.},
author = {Belkin, Nj and Croft, Wb},
doi = {10.1145/138859.138861},
file = {:Users/dragon/neo-human/papers/Belkin, Croft - 1992 - Information filtering and information retrieval two sides of the same coin.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {1--10},
title = {{Information filtering and information retrieval: two sides of the same coin?}},
url = {http://dl.acm.org/citation.cfm?id=138861},
volume = {29},
year = {1992}
}
@article{Zhai2010,
abstract = {Developing effective retrieval models is a long-standing central challenge in information retrieval research. In order to develop more effective models, it is necessary to understand the deficiencies of the current retrieval models and the relative strengths of each of them. In this article, we propose a general methodology to analytically and experimentally diagnose the weaknesses of a retrieval function, which provides guidance on how to further improve its performance. Our methodology is motivated by the empirical observation that good retrieval performance is closely related to the use of various retrieval heuristics. We connect the weaknesses and strengths of a retrieval function with its implementations of these retrieval heuristics, and propose two strategies to check how well a retrieval function implements the desired retrieval heuristics. The first strategy is to formalize heuristics as constraints, and use constraint analysis to analytically check the implementation of retrieval heuristics. The second strategy is to define a set of relevance-preserving perturbations and perform diagnostic tests to empirically evaluate how well a retrieval function implements retrieval heuristics. Experiments show that both strategies are effective to identify the potential problems in implementations of the retrieval heuristics. The performance of retrieval functions can be improved after we fix these problems.},
author = {Zhai, Chengxiang},
doi = {10.1145/1961209.1961210},
file = {:Users/dragon/neo-human/papers/Zhai - 2010 - Diagnostic Evaluation of Information Retrieval Models.pdf:pdf},
issn = {10468188},
journal = {October},
number = {October},
pages = {1--46},
title = {{Diagnostic Evaluation of Information Retrieval Models}},
url = {http://portal.acm.org/citation.cfm?id=1961210},
volume = {V},
year = {2010}
}
@article{Preiss2008,
author = {Preiss, D. and Tolsa, X. and Toro, T.},
doi = {10.1007/s00526-008-0208-z},
file = {:Users/dragon/neo-human/papers/Preiss, Tolsa, Toro - 2008 - On the smoothness of H{\"{o}}lder doubling measures.pdf:pdf},
issn = {0944-2669},
journal = {Calculus of Variations and Partial Differential Equations},
month = {oct},
number = {3},
pages = {339--363},
title = {{On the smoothness of H{\"{o}}lder doubling measures}},
url = {http://link.springer.com/10.1007/s00526-008-0208-z},
volume = {35},
year = {2008}
}
@article{Mammen1999,
author = {Mammen, Enno and Tsybakov, Alexandre B.},
file = {:Users/dragon/neo-human/papers/Mammen, Tsybakov - 1999 - Smooth discrimination analysis.pdf:pdf},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Bayes risk,Discrimination analysis,empirical risk,optimal rates,sieves},
month = {dec},
number = {6},
pages = {1808--1829},
publisher = {Institute of Mathematical Statistics},
title = {{Smooth discrimination analysis}},
url = {http://projecteuclid.org/euclid.aos/1017939240},
volume = {27},
year = {1999}
}
@inproceedings{Lasserre,
author = {Lasserre, J.A. and Bishop, C.M. and Minka, T.P.},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)},
doi = {10.1109/CVPR.2006.227},
file = {:Users/dragon/neo-human/papers/Lasserre, Bishop, Minka - Unknown - Principled Hybrids of Generative and Discriminative Models.pdf:pdf},
isbn = {0-7695-2597-0},
issn = {1063-6919},
keywords = {Application software,Computer vision,Hybrid power systems,Labeling,Large-scale systems,Machine learning,Object recognition,Parametric statistics,Predictive models,Training data,prml},
language = {English},
mendeley-tags = {prml},
pages = {87--94},
publisher = {IEEE},
title = {{Principled Hybrids of Generative and Discriminative Models}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1640745},
volume = {1}
}
@article{Gale1993,
abstract = {Researchers in both machine translation (e.g., Brown et al., 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann, 1990) have recently become interested in studying parallel texts, texts such as the Canadian Hansards (parliamentary proceedings) which are available in multiple languages (French and English). This paper describes a method for aligning sentences in these parallel texts, based on a simple statistical model of character lengths. The method was developed and tested on a small trilingual sample of Swiss economic reports. A much larger sample of 90 million words of Canadian Hansards has been aligned and donated to the ACL/DCI.},
author = {Gale, William a and {Church, Kenneth}, W},
doi = {10.3115/981344.981367},
file = {:Users/dragon/neo-human/papers/Gale, Church, Kenneth - 1993 - A Program for Aligning Sentences in Bilingual Corpora.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
pages = {177--184},
title = {{A Program for Aligning Sentences in Bilingual Corpora}},
volume = {19},
year = {1993}
}
@article{Donoho1998,
author = {Donoho, David L. and Johnstone, Iain M.},
issn = {2168-8966},
journal = {The Annals of Statistics},
keywords = {Besov,H{\"{o}}lder,Minimax decision theory,Sobolev,Triebel spaces,minimax Bayes estimation,nonlinear estimation,nonparametric regression,orthonormal bases of compactly supported wavelets,renormalization,white noise approximation,white noise model},
month = {jun},
number = {3},
pages = {879--921},
publisher = {Institute of Mathematical Statistics},
title = {{Minimax estimation via wavelet shrinkage}},
url = {http://projecteuclid.org/euclid.aos/1024691081},
volume = {26},
year = {1998}
}
@article{Wasserman2010,
author = {Wasserman, Larry},
isbn = {1441923225, 9781441923226},
month = {dec},
publisher = {Springer Publishing Company, Incorporated},
title = {{All of Statistics: A Concise Course in Statistical Inference}},
url = {http://dl.acm.org/citation.cfm?id=1965575},
year = {2010}
}
@article{Rice1944,
abstract = {THIS paper deals with the mathematical analysis of noise obtained by passing random noise through physical devices. The random noise considered is that which arises from shot effect in vacuum tubes or from thermal agitation of electrons in resistors. Our main interest is in the statistical properties of such noise and we leave to one side many physical results of which Nyquist's law may be given as an example.1},
author = {Rice, S. O.},
doi = {10.1002/j.1538-7305.1944.tb00874.x},
issn = {00058580},
journal = {Bell System Technical Journal},
month = {jul},
number = {3},
pages = {282--332},
shorttitle = {Bell System Technical Journal, The},
title = {{Mathematical Analysis of Random Noise}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6771565},
volume = {23},
year = {1944}
}
@article{DeBoer2005b,
abstract = {The cross-entropy (CE) method is a new generic approach to combinatorial and multi-extremal optimization and rare event simulation. The purpose of this tutorial is to give a gentle introduction to the CE method. We present the CE methodology, the basic algorithm and its modifications, and discuss applications in combinatorial optimization and machine learning. Many everyday tasks in operations research involve solving complicated optimization problems. The travelling salesman problem (TSP), the quadratic assignment problem (QAP) and the max-cut problem are a representative sample of combinatorial optimiza-tion problems (COP) where the problem being studied is completely known and static. In contrast, the buffer allocation problem (BAP) is a noisy estimation problem where the objective function needs to be estimated since it is unknown. Discrete event simulation is one method for estimating an unknown objective function. The purpose of this tutorial is to show that the CE method provides a simple, efficient and general method for solving such problems. Moreover, we wish to show that the CE method is also valuable for rare event simulation, where very small probabilities need to be accurately estimated – for example in reliability analysis, or performance analysis of telecommunication systems. This tutorial is intended for a broad audience of operations research specialists, computer scientists, mathematicians, statisticians and engineers. Our aim is to explain the foundations of the CE method and consider various applications.},
author = {{De Boer}, Pieter-Tjerk and Kroese, Dirk P and Rubinstein, Reuven Y},
doi = {10.1007/s10479-005-5724-z},
file = {:Users/dragon/neo-human/papers/A Tutorial on the Cross-Entropy Method - de Boer et al. - 2005.pdf:pdf},
isbn = {0254-5330},
issn = {02545330},
journal = {Annals of Operations Research},
keywords = {Monte-Carlo simulation,cross-entropy method,machine learning,randomized optimization,rare events},
pages = {19--67},
title = {{A Tutorial on the Cross-Entropy Method}},
volume = {134},
year = {2005}
}
@article{Myung2006,
abstract = {The Minimum Description Length (MDL) principle is an information theoretic approach to inductive inference that originated in algorithmic coding theory. In this approach, data are viewed as codes to be compressed by the model. From this perspective, models are compared on their ability to compress a data set by extracting useful information in the data apart from random noise. The goal of model selection is to identify the model, from a set of candidate models, that permits the shortest description length (code) of the data. Since Rissanen originally formalized the problem using the crude ‘two-part code’ MDL method in the 1970s, many significant strides have been made, especially in the 1990s, with the culmination of the development of the refined ‘universal code’ MDL method, dubbed Normalized Maximum Likelihood (NML). It represents an elegant solution to the model selection problem. The present paper provides a tutorial review on these latest developments with a special focus on NML. An application example of NML in cognitive modeling is also provided.},
author = {Myung, Jay I. and Navarro, Daniel J. and Pitt, Mark A.},
doi = {10.1016/j.jmp.2005.06.008},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Cognitive modeling,Inductive inference,Minimum Description Length,Model complexity},
month = {apr},
number = {2},
pages = {167--179},
title = {{Model selection by normalized maximum likelihood}},
url = {http://www.sciencedirect.com/science/article/pii/S0022249605000532},
volume = {50},
year = {2006}
}
@article{Wasserman2000,
abstract = {This paper reviews the Bayesian approach to model selection and model averaging. In this review, I emphasize objective Bayesian methods based on noninformative priors. I will also discuss implementation details, approximations, and relationships to other methods. Copyright 2000 Academic Press.},
author = {Wasserman, L},
doi = {10.1006/jmps.1999.1278},
issn = {0022-2496},
journal = {Journal of mathematical psychology},
keywords = {AIC,BIC,Bayes factors,Markov chain Monte Carlo,consistency,default Bayes methods},
month = {mar},
number = {1},
pages = {92--107},
pmid = {10733859},
title = {{Bayesian Model Selection and Model Averaging.}},
url = {http://www.sciencedirect.com/science/article/pii/S0022249699912786},
volume = {44},
year = {2000}
}
@article{Jebara2003,
annote = {compares the discriminative and generative models and tries to fuse em?},
author = {Jebara, Tony},
file = {:Users/dragon/neo-human/papers/Jebara - 2003 - Machine Learning Discriminative and Generative (Kluwer International Series in Engineering and Computer Science).pdf:pdf},
isbn = {1402076479},
keywords = {book,prml},
mendeley-tags = {book,prml},
month = {nov},
publisher = {Kluwer Academic Publishers},
title = {{Machine Learning: Discriminative and Generative (Kluwer International Series in Engineering and Computer Science)}},
url = {http://dl.acm.org/citation.cfm?id=983108},
year = {2003}
}
@article{Montgomery1985,
author = {Montgomery, Peter L.},
doi = {10.1090/S0025-5718-1985-0777282-X},
file = {:Users/dragon/neo-human/papers/Montgomery - 1985 - Modular multiplication without trial division.pdf:pdf},
issn = {0025-5718},
journal = {Mathematics of Computation},
keywords = {Modular arithmetic,multiplication},
month = {may},
number = {170},
pages = {519--519},
title = {{Modular multiplication without trial division}},
url = {http://www.ams.org/mcom/1985-44-170/S0025-5718-1985-0777282-X/},
volume = {44},
year = {1985}
}
@article{Church1990,
abstract = {The term word association is used in a very particular sense in the$\backslash$npsycholinguistic literature. {\{}(Generally{\}} speaking, subjects respond$\backslash$nquicker than normal to the word nurse if it follows a highly associated$\backslash$nword such as doctor. ) We will extend the term to provide the basis$\backslash$nfor a statistical description of a variety of interesting linguistic$\backslash$nphenomena, ranging from semantic relations of the doctor/nurse type$\backslash$n(content word/content word) to lexico-syntactic co-occurrence constraints$\backslash$nbetween verbs and prepositions (content word/function word). This$\backslash$npaper will propose an objective measure based on the information$\backslash$ntheoretic notion of mutual information, for estimating word association$\backslash$nnorms from computer readable corpora. {\{}(The{\}} standard method of obtaining$\backslash$nword association norms, testing a few thousand subjects on a few$\backslash$nhundred words, is both costly and unreliable.) The proposed measure,$\backslash$nthe association ratio, estimates word association norms directly$\backslash$nfrom computer readable corpora, making it possible to estimate norms$\backslash$nfor tens of thousands of words.},
author = {Church, Kenneth Ward and Hanks, Patrick},
doi = {10.3115/981623.981633},
file = {:Users/dragon/neo-human/papers/Church, Hanks - 1990 - Word association norms, mutual information, and lexicography.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Comput. Linguist.},
number = {1},
pages = {22--29},
title = {{Word association norms, mutual information, and lexicography}},
url = {http://portal.acm.org/citation.cfm?id=89095{\&}dl=},
volume = {16},
year = {1990}
}
@article{VanderPlas2014a,
abstract = {This paper presents a brief, semi-technical comparison of the essential features of the frequentist and Bayesian approaches to statistical inference, with several illustrative examples implemented in Python. The differences between frequentism and Bayesianism fundamentally stem from differing definitions of probability, a philosophical divide which leads to distinct approaches to the solution of statistical problems as well as contrasting ways of asking and answering questions about unknown parameters. After an example-driven discussion of these differences, we briefly compare several leading Python statistical packages which implement frequentist inference using classical methods and Bayesian inference using Markov Chain Monte Carlo.},
archivePrefix = {arXiv},
arxivId = {1411.5018},
author = {VanderPlas, Jake},
eprint = {1411.5018},
file = {:Users/dragon/neo-human/papers/VanderPlas - 2014 - Frequentism and Bayesianism A Python-driven Primer.pdf:pdf},
number = {Scipy},
pages = {1--9},
title = {{Frequentism and Bayesianism: A Python-driven Primer}},
url = {http://arxiv.org/abs/1411.5018},
year = {2014}
}
@article{Shewchuk1994b,
abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written with neither illustrations nor intuition, and their victims can be found to this day babbling senselessly in the corners of dusty libraries. For this reason, a deep, geometric understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-six illustrations are provided. Dense prose is avoided. Concepts are explained in several differentways. Most equations are coupled with an intuitive interpretation.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Shewchuk, Jonathan Richard},
doi = {10.1.1.110.418},
eprint = {1102.0183},
file = {:Users/dragon/neo-human/papers/An Introduction to the Conjugate Gradient Method Without the Agonizing Pain - Shewchuk - 1994.pdf:pdf},
isbn = {9781479928934},
issn = {14708728},
journal = {Science},
keywords = {1,2,5,agonizing pain,conjugate gradient method,convergence analysis,eigen do,eigenvalues,i try,jacobi iterations,preconditioning,thinking with eigenvectors},
number = {CS-94-125},
pages = {64},
pmid = {17348934},
title = {{An Introduction to the Conjugate Gradient Method Without the Agonizing Pain}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.418{\&}amp;rep=rep1{\&}amp;type=pdf$\backslash$nhttp://www.cs.cmu.edu/{~}quake-papers/painless-conjugate-gradient.pdf},
volume = {49},
year = {1994}
}
@article{TURING1950,
author = {TURING, A. M.},
doi = {10.1093/mind/LIX.236.433},
issn = {0026-4423},
journal = {Mind},
month = {oct},
number = {236},
pages = {433--460},
title = {{I.—COMPUTING MACHINERY AND INTELLIGENCE}},
url = {http://mind.oxfordjournals.org/cgi/content/long/LIX/236/433},
volume = {LIX},
year = {1950}
}
@misc{Charniak1997,
abstract = {We present a language model in which the probability of a sentence is the sum of the individual parse probabilities, and these are calculated using a probabilistic context-free grammar (PCFG) plus statistics on individual words and how they fit into parses. We have used the model to improve syntactic disambiguation. After training on Wall Street Journal (WSJ) text we tested on about 200 WSJ sentence restricted to the 5400 most common words from our training. We observed a 41{\%} reduction in bracket-crossing errors compared to the performance of our PCFG without the use of the word statistics.},
author = {Charniak, Eugene},
booktitle = {Wall Street Journal},
file = {:Users/dragon/neo-human/papers/Charniak - 1997 - Statistical parsing with a context-free grammar and word statistics.pdf:pdf},
number = {CS-95-28},
pages = {598--603},
title = {{Statistical parsing with a context-free grammar and word statistics}},
url = {http://bllip.cs.brown.edu/papers/aaai97.pdf},
year = {1997}
}
@misc{MehdiHassani,
abstract = {In this paper we introduce some formulas for the number of derangements. Then we define the derangement function and use the software package MAPLE to obtain some integrals related to the incomplete gamma function and also to some hypergeometric summations.},
author = {Hassani, Mehdi},
booktitle = {Journal of Integer Sequences},
file = {:Users/dragon/neo-human/papers/Derangements and Applications - Mehdi Hassani.pdf:pdf},
title = {{Derangements and Applications}},
url = {https://cs.uwaterloo.ca/journals/JIS/VOL6/Hassani/hassani5.html},
urldate = {2015-11-15},
year = {2003}
}
@article{Turner2012,
abstract = {This tutorial explains the foundation of approximate Bayesian computation (ABC), an approach to Bayesian inference that does not require the specification of a likelihood function, and hence that can be used to estimate posterior distributions of parameters for simulation-based models. We discuss briefly the philosophy of Bayesian inference and then present several algorithms for ABC. We then apply these algorithms in a number of examples. For most of these examples, the posterior distributions are known, and so we can compare the estimated posteriors derived from ABC to the true posteriors and verify that the algorithms recover the true posteriors accurately. We also consider a popular simulation-based model of recognition memory (REM) for which the true posteriors are unknown. We conclude with a number of recommendations for applying ABC methods to solve real-world problems.},
author = {Turner, Brandon M. and {Van Zandt}, Trisha},
doi = {10.1016/j.jmp.2012.02.005},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Approximate Bayesian computation,Bayesian estimation,Population Monte Carlo,Tutorial},
month = {apr},
number = {2},
pages = {69--85},
title = {{A tutorial on approximate Bayesian computation}},
url = {http://www.sciencedirect.com/science/article/pii/S0022249612000272},
volume = {56},
year = {2012}
}
@article{Myung2003,
abstract = {In this paper, I provide a tutorial exposition on maximum likelihood estimation (MLE). The intended audience of this tutorial are researchers who practice mathematical modeling of cognition but are unfamiliar with the estimation method. Unlike least-squares estimation which is primarily a descriptive tool, MLE is a preferred method of parameter estimation in statistics and is an indispensable tool for many statistical modeling techniques, in particular in non-linear modeling with non-normal data. The purpose of this paper is to provide a good conceptual explanation of the method with illustrative examples so the reader can have a grasp of some of the basic principles.},
author = {Myung, In Jae},
doi = {10.1016/S0022-2496(02)00028-7},
file = {:Users/dragon/neo-human/papers//Myung - 2003 - Tutorial on maximum likelihood estimation.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
month = {feb},
number = {1},
pages = {90--100},
title = {{Tutorial on maximum likelihood estimation}},
url = {http://www.sciencedirect.com/science/article/pii/S0022249602000287},
volume = {47},
year = {2003}
}
