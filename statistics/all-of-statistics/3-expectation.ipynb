{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-30T13:48:56.807618Z",
     "start_time": "2017-10-30T13:48:56.783408Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "from IPython.core.pylabtools import figsize as i_figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "$ \\newcommand{\\E}{\\mathbb{E}}$  \n",
    "$ \\newcommand{\\V}{\\mathbb{V}}$\n",
    "$ \\newcommand{\\P}{\\mathbb{P}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation\n",
    "======\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance\n",
    "========\n",
    "[[wiki](http://www.wikiwand.com/en/Variance)]\n",
    "\n",
    "* $\\operatorname{Var}(X) = \\operatorname{E}\\left[(X - \\mu)^2 \\right] = \n",
    "\\int (X - \\mu)^2 dF(x)$\n",
    "\n",
    "* $\\operatorname{Var}(X) = \\operatorname{Cov}(X, X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $$\n",
    "\\begin{align}\n",
    "\\operatorname{Var}(X) &= \\operatorname{E}\\left[(X - \\operatorname{E}[X])^2\\right] \\\\\n",
    "&= \\operatorname{E}\\left[X^2 - 2X\\operatorname{E}[X] + (\\operatorname{E}[X])^2\\right] \\\\\n",
    "&= \\operatorname{E}\\left[X^2\\right] - 2\\operatorname{E}[X]\\operatorname{E}[X] + (\\operatorname{E}[X])^2 \\\\\n",
    "&= \\operatorname{E}\\left[X^2 \\right] - (\\operatorname{E}[X])^2\n",
    "\\end{align}\n",
    "$$\n",
    "That is, $E\\left[X^2\\right] - (E[X])^2$, which can be remembered as \"mean of square minus square of mean\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For continuous random variables,  \n",
    "$\\operatorname{Var}(X) =\\sigma^2 =\\int (x-\\mu)^2 \\, f(x) \\, dx\\, =\\int x^2 \\, f(x) \\, dx\\, - \\mu^2$\n",
    "If expected value doesn't exist, so does the variance.  \n",
    "Ex. \n",
    "  * Cauchy distribution\n",
    "  * Pareto distribution with $k \\in (1, 2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For discrete random variables,  \n",
    "$$\n",
    "\\operatorname{Var}(X) = \\sum_{i=1}^n p_i\\cdot(x_i - \\mu)^2 = \\sum_{i=1}^n p_i x_i ^2- \\mu^2,\n",
    "$$\n",
    "  * If the $X_i \\sim F$ for some F, then  \n",
    "  $$ \\operatorname{Var}(X) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2. \\text{where } \\mu = \\frac{1}{n}\\sum_{i=1}^n x_i $$\n",
    "  * The mean in the formula can be avoided as:  \n",
    "  $$ \\operatorname{Var}(X) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\frac{1}{2}(x_i - x_j)^2 = \\frac{1}{n^2}\\sum_i \\sum_{j>i} (x_i-x_j)^2.$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Offset: ** $\\operatorname{Var}(X+a) = \\operatorname{Var}(X)$\n",
    "* **Scaling**: $\\operatorname{Var}(aX) = a^2 \\operatorname{Var}(X)$\n",
    "* $\\operatorname{Var}(aX+bY)=a^2\\operatorname{Var}(X)+b^2\\operatorname{Var}(Y)+2ab\\, \\operatorname{Cov}(X,Y)$\n",
    "* $\\operatorname{Var}(aX-bY)=a^2\\operatorname{Var}(X)+b^2\\operatorname{Var}(Y)-2ab\\, \\operatorname{Cov}(X,Y)$\n",
    "* **Sum**: $\\operatorname{Var}\\left(\\sum_{i=1}^N X_i\\right)=\\sum_{i,j=1}^N\\operatorname{Cov}(X_i,X_j)\\\\=\\sum_{i=1}^N\\operatorname{Var}(X_i)+\\sum_{i\\ne j}\\operatorname{Cov}(X_i,X_j).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Linear combination**: $$\n",
    "    \\begin{align}\n",
    "\\operatorname{Var}\\left( \\sum_{i=1}^N a_iX_i\\right) &=\\sum_{i,j=1}^{N} a_ia_j\\operatorname{Cov}(X_i,X_j) \\\\\n",
    "&=\\sum_{i=1}^N a_i^2\\operatorname{Var}(X_i)+\\sum_{i\\not=j}a_ia_j\\operatorname{Cov}(X_i,X_j)\\\\\n",
    "& =\\sum_{i=1}^N a_i^2\\operatorname{Var}(X_i)+2\\sum_{1\\le i<j\\le N}a_ia_j\\operatorname{Cov}(X_i,X_j).\n",
    "\\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**linear transformation**\n",
    "\n",
    "Let $\\mathbf{X}$ be a random vector with covariance matrix $\\Sigma(\\mathbf{X})$, and let A be a matrix that can act on $\\mathbf{X}$. The covariance matrix of the vector $A\\mathbf{X}$ is:\n",
    "\n",
    "$$\n",
    "\\Sigma(A\\mathbf{X}) = A\\, \\Sigma(\\mathbf{X})\\, A^\\mathrm{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uncorrelated RVs:  \n",
    "  * If $\\operatorname{Cov}(X_i,X_j)=0\\ ,\\ \\forall\\ (i\\ne j)$, then $\\{X_i\\}$ are uncorrelated. This then leads to  \n",
    "  $\\operatorname{Var}\\left(\\sum_{i=1}^N X_i\\right)=\\sum_{i=1}^N\\operatorname{Var}(X_i)$\n",
    "  * If RVs are independent, then they are uncorrelated. Note that, independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances.\n",
    "  * $$\\operatorname{Var}\\left(\\overline{X}\\right) = \\operatorname{Var}\\left(\\frac {1} {n}\\sum_{i=1}^n X_i\\right) = \\frac {1} {n^2}\\sum_{i=1}^n \\operatorname{Var}\\left(X_i\\right) = \\frac {\\sigma^2} {n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sum of correlated variables  \n",
    "  * If the RVs have equal variance $\\sigma^2$ and the average correlation of the distinct variables is $\\rho$, then,  \n",
    "  $$\\operatorname{Var}(\\overline{X}) = \\frac {\\sigma^2} {n} + \\frac {n-1} {n} \\rho \\sigma^2$$\n",
    "  * Thus, variance of the mean increases with the average of the correlations.\n",
    "  * In other words, additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean.\n",
    "  * Convergence:  \n",
    "    * $\\lim_{n \\to \\infty} \\operatorname{Var}(\\overline{X}) = \\rho \\sigma^2$\n",
    "    * This makes clear that the sample mean of correlated variables does not generally converge to the population mean, even though the [Law of large numbers](http://www.wikiwand.com/en/Law_of_large_numbers) states that the sample mean will converge for independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bessel's Correction\n",
    "\n",
    "\n",
    "Sample Variance = $\\displaystyle \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu)^2$\n",
    "\n",
    "## Caveats\n",
    "\n",
    "* It does not yield an unbiased estimator of standard deviation.\n",
    "  * while the sample variance (using Bessel's correction) is an unbiased estimate of the population variance, its square root, the sample standard deviation, is a biased estimate of the population standard deviation; because the square root is a concave function, the bias is downward, by Jensen's inequality. There is no general formula for an unbiased estimator of the population standard deviation, though there are correction factors for particular distributions, such as the normal; see unbiased estimation of standard deviation for details. An approximation for the exact correction factor for the normal distribution is given by using n − 1.5 in the formula: the bias decays quadratically (rather than linearly, as in the uncorrected form and Bessel's corrected form).\n",
    "  \n",
    "* The corrected estimator often has worse (higher) mean squared error (MSE) than the uncorrected estimator, and never has the minimum MSE\n",
    "  * A different scale factor can always be chosen to minimize MSE. The optimal value depends on excess kurtosis, as discussed in [mean squared error: variance](http://www.wikiwand.com/en/Mean_squared_error#/Variance). For the normal distribution this is optimized by dividing by n + 1 (instead of n − 1 or n).\n",
    "* It is only necessary when the population mean is unknown (and estimated as the sample mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation 1\n",
    "\n",
    "One can understand Bessel's correction intuitively as the degrees of freedom in the residuals vector (residuals, not errors, because the population mean is unknown):\n",
    "\n",
    "$$(x_1-\\overline{x},\\,\\dots,\\,x_n-\\overline{x})$$\n",
    "\n",
    "where $\\overline{x}$ is the sample mean. While there are n independent samples, there are only n − 1 independent residuals, as they sum to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation 2\n",
    "\n",
    "* Comes from [Jensen's inequality](http://www.wikiwand.com/en/Jensen%27s_inequality)\n",
    "\n",
    "* Say, the population mean, $\\mu$ is 2050\n",
    "* Samples: $x_i$ = 2051, 2053, 2055, 2050, 2051\n",
    "* Sample mean $\\hat{\\mu} = \\frac{\\sum_{i} x_i}{5} = 2052$\n",
    "* If we use the population mean (2050), the variance is,\n",
    "$var(x; \\mu) = \\frac{\\sum_{i=1}^{5} (x_i - \\mu)^2}{5} = \\frac{\\sum_{i=1}^{5} (x_i - 2050)^2}{5}  = \\frac{36}{5} = 7.2$\n",
    "* If we use the sample mean (2052), the variance is,\n",
    "$var(x; \\hat{\\mu}) = \\frac{\\sum_{i=1}^{5} (x_i - \\hat{\\mu})^2}{5} = \\frac{\\sum_{i=1}^{5} (x_i - 2052)^2}{5}  = \\frac{16}{5} = 3.2$\n",
    "\n",
    "* Is the variance computed using sample mean always smaller than if we used the population mean?\n",
    "\n",
    "* We will see what happens at a single point. Say we use the population mean (2050) to compute the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    " [ \\underbrace{2053-2050}_{\n",
    "             \\begin{smallmatrix}\n",
    "                 \\text{Deviation from}\\\\\n",
    "                 \\text{population mean}\n",
    "             \\end{smallmatrix}\n",
    "        }\n",
    " ]^2 \n",
    " &=  [ \\overbrace{\n",
    "             \\underbrace{(2053 - 2052)}_{\n",
    "                \\begin{smallmatrix}\n",
    "                    \\text{Deviation from} \\\\ \n",
    "                    \\text{sample mean}\n",
    "                 \\end{smallmatrix}\n",
    "              }\n",
    "          }^{\\text{This is a.}}  + \n",
    " \\overbrace{(2052 - 2050)}^{\\text{This is }b.}\\,]^2 \\\\\n",
    " & = \\overbrace{(2053 - 2052)^2}^{\\text{This is }a^2.} +\n",
    "     \\overbrace{2(2053 - 2052)(2052 - 2050)}^{\\text{This is }2ab.} +\n",
    "     \\overbrace{(2052 - 2050)^2}^{\\text{This is }b^2.}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now extending to all points  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\overbrace{(2051 - 2052)^2}^{\\text{This is }a^2.} &\n",
    "  +\\  \\overbrace{2(2051 - 2052)(2052 - 2050)}^{\\text{This is }2ab.} &\n",
    "  +\\  \\overbrace{(2052 - 2050)^2}^{\\text{This is }b^2.} \\\\\n",
    "  (2053 - 2052)^2\\  &+\\  2(2053 - 2052)(2052 - 2050)\\  &+\\ (2052 - 2050)^2 \\\\\n",
    "  (2055 - 2052)^2\\  &+\\  2(2055 - 2052)(2052 - 2050)\\  &+\\ (2052 - 2050)^2 \\\\\n",
    "  (2050 - 2052)^2\\  &+\\  2(2050 - 2052)(2052 - 2050)\\  &+\\ (2052 - 2050)^2 \\\\\n",
    "  (2051 - 2052)^2\\  &\n",
    "  +\\  \\underbrace{2(2051 - 2052)(2052 - 2050)}_{\n",
    "          \\begin{smallmatrix}\n",
    "              \\text{The sum of the entries in this} \\\\\n",
    "              \\text{middle column must be 0.}\n",
    "           \\end{smallmatrix}}\\ &\n",
    "   +\\  (2052 - 2050)^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the second column, notice that the term (2052-2050) is a constant. Hence, the sum of second column is the sum of its first terms times this constant. But 2052 is the sample mean. Hence, the sum of the second column becomes zero\n",
    "* The first column is the variance computed using the sample mean\n",
    "* The third column is the residue, each of which is $(\\mu - \\hat{\\mu})^2$\n",
    "* Thus $var(x; \\mu) = var(x; \\hat{\\mu}) + 5 (\\mu - \\hat{\\mu})^2$. The second term will vanish only if the sample mean is equal to the population mean. Else, it is always a positive value (since it is a sum of squares) and leads to underestimation of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof for correction**  \n",
    "\n",
    "Idea:\n",
    "\n",
    "In the biased estimator, by using the sample mean instead of the true mean, you are underestimating each $x_i − \\mu$ by $\\overline{x} − \\mu$. We know that the variance of a sum is the sum of the variances (for uncorrelated variables). So, to find the discrepancy between the biased estimator and the true variance, we just need to find the variance of $\\overline{x} − \\mu$. This is just the [variance of the sample mean](http://www.wikiwand.com/en/Variance#/Sum_of_uncorrelated_variables_.28Bienaym.C3.A9_formula.29), which is $\\sigma^2/n$. So, we expect that the biased estimator underestimates $\\sigma^2$ by $\\sigma^2/n$, and so the biased estimator = (1 − 1/n) × the unbiased estimator = (n − 1)/n × the unbiased estimator.\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "E \\left[ \\sigma^2 - s_{biased}^2 \\right] &\n",
    "= E\\left[ \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2 - \\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{x})^2 \\right] \\\\\n",
    "&= \\frac{1}{n} E\\left[ \\sum_{i=1}^n\\left((x_i^2 - 2 x_i \\mu + \\mu^2) - (x_i^2 - 2 x_i \\overline{x} + \\overline{x}^2)\\right) \\right] \\\\\n",
    "&= \\frac{1}{n} E\\left[ \\sum_{i=1}^n\\left( - 2 x_i \\mu + \\mu^2 + 2 x_i \\overline{x} - \\overline{x}^2\\right) \\right] \\\\\n",
    "&= E\\left[ - 2 \\overline{x} \\mu + \\mu^2 + 2 \\overline{x}^2 - \\overline{x}^2 \\right] \\\\\n",
    "&= E\\left[  \\mu^2 - 2 \\overline{x} \\mu + \\overline{x}^2 \\right] \\\\\n",
    "&= E\\left[  (\\overline{x}   - \\mu)^2 \\right] \\\\\n",
    "&= \\text{Var} (\\overline{x}) \\\\\n",
    "&= \\frac{\\sigma^2}{n}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence,  \n",
    "$\n",
    "\\operatorname{E} \\left[ s^2_{\\text{biased}} \\right] = \n",
    "\\sigma^2 - \\frac{\\sigma^2}{n} =\n",
    "\\frac{n-1}{n} \\sigma^2 \n",
    "$\n",
    "\n",
    "So, an unbiased estimator should be given by\n",
    " $ s_{\\text{unbiased}}^2 = \\frac{n}{n-1} s_{\\text{biased}}^2 = \\frac{1}{n-1} \\sum (x_i - \\overline{x})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Variance\n",
    "==========\n",
    "\n",
    "Unbiased estimate:\n",
    "$$\n",
    "\\sigma^2 = \\displaystyle\\frac {\\sum_{i=1}^n x_i^2 - (\\sum_{i=1}^n x_i)^2/n}{n-1}.\n",
    "$$\n",
    "\n",
    "the terms in the numerator can be similar numbers, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-30T13:51:29.593049Z",
     "start_time": "2017-10-30T13:51:29.264968Z"
    },
    "code_folding": [],
    "locked": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist [3 2 2 0 1 2 1 3 0 1]\n",
      "edges [  611.25        1366.83207071  2122.41414141  2877.99621212  3633.57828283\n",
      "  4389.16035354  5144.74242424  5900.32449495  6655.90656566  7411.48863636\n",
      "  8167.07070707]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAssAAAE1CAYAAAAReK0vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11068b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 10^5 # Number of trials\n",
    "def compute_diff():\n",
    "    N = np.random.randint(low=10, high=100) # no. of samples each time\n",
    "    x = np.random.randint(low=1, high=10^5, size=(N, 1))\n",
    "    \n",
    "    x_sum_of_sqrs = np.square(x).sum()\n",
    "    x_sum = x.sum()\n",
    "    x_sqr_of_sum = (x_sum)^2\n",
    "    \n",
    "    diff = abs(x_sum_of_sqrs - (x_sqr_of_sum/(1.*N)))\n",
    "    return diff\n",
    "    \n",
    "diffs = [compute_diff() for t in range(T)]\n",
    "hist, edges = np.histogram(diffs, bins=10)\n",
    "print('hist', hist)\n",
    "print('edges', edges)\n",
    "i_figsize(12,5)\n",
    "plt.hist(diffs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-30T13:50:54.993568Z",
     "start_time": "2017-10-30T13:50:54.919809Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for computing variance\n",
    "==================\n",
    "[wiki](http://www.wikiwand.com/en/Algorithms_for_calculating_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# algo 1\n",
    "def variance_algo_1(x):\n",
    "    n, sum1, sum_sqrs = 0, 0, 0\n",
    "    for xx in x:\n",
    "        n += 1\n",
    "        sum1     += xx\n",
    "        sum_sqrs += xx*xx\n",
    "    print('method 1: sum-sqrs %.2f\\nmethod 1: sqr-sum1  %.2f' % (sum_sqrs, sum1*sum1/(1.*n)))\n",
    "    print('method 1: sqr-sum1: %.2f' % (sum1*sum1))\n",
    "    print('method 1: diff: {0}'.format((sum_sqrs - sum1*sum1/(1.*n))))\n",
    "    variance = (sum_sqrs - sum1*sum1/(1.*n))/(n-1.)\n",
    "    print('method 1: variance: %.8f' % (variance))\n",
    "\n",
    "# algo 2: 2-pass variance\n",
    "def variance_algo_2(x):\n",
    "    n, sum1, sum2=0, 0, 0\n",
    "    for xx in x:\n",
    "        n+=1\n",
    "        sum1 += xx\n",
    "    mean1 = sum1/(1.*n)\n",
    "    for xx in x:\n",
    "        sum2 += (xx-mean1)*(xx-mean1)\n",
    "    variance1 = sum2/(n - 1.)\n",
    "    print('method 2: variance: %.8f' % (variance))\n",
    "    \n",
    "x_original = [4,7,13,16]\n",
    "offsets = [0, 1e8, 1e9]\n",
    "for offset1 in offsets:\n",
    "    x = np.asarray([xx+offset1 for xx in x_original])\n",
    "    print('\\ndata with offset {1}: {0}'.format(x, offset1))\n",
    "    variance_algo_1(x)\n",
    "    variance_algo_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance & Correlation\n",
    "=============\n",
    "\n",
    "* Let X, Y be RV with means $\\mu_X \\text{ and } \\mu_Y$ and standard deviations $\\sigma_X \\text{ and } \\sigma_Y$. Then  \n",
    "$\\operatorname{Cov}(X, Y) = \\sigma(X, Y) = \\mathbb{E} \\left[ (X - \\mu_X) ~ (Y - \\mu_Y) \\right]$\n",
    "\n",
    "* Correlation $\\rho$ is defined as  $\\rho_{X,Y} = \\rho(X,Y) = \\frac{\\sigma(X, Y)}{\\sigma_X \\sigma_Y}$\n",
    "\n",
    "* **Alt-Forms**: $\\sigma(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y)$\n",
    "\n",
    "* **Range**: $\\rho(X, Y) \\in [-1, 1]$\n",
    "\n",
    "* If Y = aX + b, then   \n",
    "$\n",
    "\\rho(X, Y) = \n",
    "\\begin{cases}\n",
    "    +1 & \\text{a > 0} \\\\\n",
    "    -1 & \\text{a < 0} \\\\\n",
    "     0 & \\text{X and Y are independent}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Expectation\n",
    "================\n",
    "\n",
    "**Definition**  \n",
    "The conditional expectation of X given Y=y is  \n",
    "$$\n",
    "\\mathbb{E}(X | Y=y) = \n",
    "\\begin{cases}\n",
    "    \\sum ~ x ~ f_{X | Y}~(x|y) ~dx & \\text{discrete case}\\\\\n",
    "    \\int ~ x ~ f_{X | Y}~(x|y) ~dx & \\text{continuous case}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Some more**\n",
    "* Let X take values from $\\mathcal{S}$ and Y from $\\mathcal{T} \\subseteq \\mathbb{R}$\n",
    "\n",
    "* **Regression function**  \n",
    "The function $v: S \\rightarrow \\mathbb{R}$, defined by $v(x) = \\mathbb{E}(Y | X=x)$ is called the *regression function* of Y based on X.\n",
    "\n",
    "* It should be note that $r(X) = \\mathbb{E}(Y|X)$ is not a value, but rather a function in Y.\n",
    "\n",
    "** Fundamental Property**\n",
    "1. $\\mathbb{E}[r(X) \\mathbb{E}(Y|X)] = \\mathbb{E}[r(X)Y]$ for every function $r: \\mathcal{S} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "**Properties**\n",
    "1. **Linearity**  \n",
    "  1. $\\E \\bigl[ (Y+Z) \\mid X \\bigr] = \\E[Y \\mid X] + \\E[Z \\mid X]$\n",
    "  1. $\\E \\bigl[ aY \\mid X \\bigr] = a \\E [ Y \\mid X] $\n",
    "1. **Monotonicity**  \n",
    "  1. if $Y \\ge 0, \\text{ then } \\mathbb{E}(Y|X) \\ge 0$\n",
    "  1. if $Y \\le Z, \\text{ then } \\mathbb{E}(Y|X) \\le \\mathbb{E}(Z|X)$\n",
    "  1. $ \\bigl| ~ \\E[Y \\mid X] ~ \\bigr| \\le \\E \\left[ ~|Y|~ \\mid X \\right]$\n",
    "\n",
    "**Of Transformations of RVs**  \n",
    "If r(x,y) is a function of x and y, then  \n",
    "$$\n",
    "\\mathbb{E}(r(X,Y)| Y=y) = \n",
    "\\begin{cases}\n",
    "    \\sum ~ r(X,Y) ~ f_{X | Y}(x|y) dx & \\text{discrete case}\\\\\n",
    "    \\int ~ r(X,Y) ~ f_{X | Y}(x|y) dx & \\text{continuous case}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Iterated Expectations**  \n",
    "\n",
    "* **Equal means Property**   \n",
    "For RVs X and Y, if the expectations exist, then $\\E \\bigl[ \\E [Y \\mid X] \\bigr] = \\E[Y]$.  \n",
    "That is, the mean of Y and Y|X are the same.\n",
    "\n",
    "* For any function r(x,y), we have  \n",
    "$$\n",
    "\\E \\bigl[ \\E [r(X,Y) \\mid X] \\bigr] = \\E \\bigl[ r(X,Y) \\bigr]\n",
    "$$\n",
    "\n",
    "**Some results**\n",
    "\n",
    "* if $r: \\mathcal{S} \\rightarrow \\mathbb{R}, \\text{ then } Y - \\mathbb{E}(Y|X)$ and r(X) are uncorrelated.  \n",
    "Proof:  \n",
    "  * $\\E \\bigl[ Y - \\E[Y \\mid X] \\bigr] = \\E[Y] - \\E \\bigl[ \\E[Y \\mid X] \\bigr] = \\E[Y] - \\E[Y] = 0$\n",
    "  * \\begin{array}{llr}\n",
    "      \\sigma \\bigl( Y−\\E[Y \\mid X],r(X) \\bigr)\n",
    "       &= \\E \\bigl[ \\left( Y−\\E(Y \\mid X) \\right) r(X) \\bigr]\n",
    "           - \\E \\bigl[ Y - \\E[Y \\mid X ] \\bigr] \\E[r(X)]\n",
    "      & \\color{gray}{\\text{deftn. of covariance}}\\\\\n",
    "      & = \\E \\bigl[ \\left( Y−\\E(Y \\mid X) \\right) r(X) \\bigr] - 0\n",
    "      & \\color{gray}{\\text{from the prev result}}\\\\\n",
    "      &= \\E[Yr(X)] − \\E \\bigl[ \\E [Y \\mid X ] r(X) \\bigr]\n",
    "      & \\color{gray}{\\text{fundamental property}}\\\\\n",
    "      &=0&\n",
    "    \\end{array}\n",
    "\n",
    "* $If s:\\mathcal{S} \\rightarrow \\mathbb{R} \\text{ then } \\E [s(X)~ Y \\mid X] = s(X)~ E[Y \\mid X]$  \n",
    "That is, a deterministic functions acts like a constant [?].\n",
    "\n",
    "* **Substitution Rule**  \n",
    "if $s:\\mathcal{S} \\times \\mathcal{T} \\rightarrow \\mathbb{R}$, then\n",
    "$\\E \\bigl[ s(X, Y) \\mid X=x \\bigr] = \\E \\bigl[ s(x, Y) \\mid X=x \\bigr] $\n",
    "\n",
    "* if X, Y are independent, then $\\E[Y \\mid X] = \\E[Y]$\n",
    "\n",
    "* **Consistency**[?]  \n",
    "  * $\\E \\bigl[ \\E [Z \\mid X,Y] \\mid X   \\bigr] = \\E[Z \\mid X]$  \n",
    "  * $\\E \\bigl[ \\E [Z \\mid X]   \\mid X,Y \\bigr] = \\E[Z \\mid X]$\n",
    "\n",
    "* $\\sigma(X, \\mathbb{E}(Y ~|~ X)) = \\sigma(X, Y)$  \n",
    "$\\mathbb{E}(X \\mid Y)$ behaves just like Y in its relationship wit X.  \n",
    "Proof:  \n",
    "\\begin{array}{lllr}\n",
    "\\sigma(X, \\mathbb{E}(Y ~|~ X))\n",
    "&= \\mathbb{E}\\bigl[X ~ \\mathbb{E}(Y ~|~ X)\\bigr]\n",
    "& - \\mathbb{E}[X] ~ \\mathbb{E}\\bigl[\\mathbb{E}(Y ~|~ X))\\bigr] \\hspace{1pt}\n",
    "&  \\color{gray}{\\text{ deftn. of covariance}} \\\\\n",
    "&= \\mathbb{E}[XY]\n",
    "& - \\mathbb{E}[X] ~ \\mathbb{E}\\bigl[\\mathbb{E}(Y ~|~ X))\\bigr] \\hspace{1pt}\n",
    "&  \\color{gray}{\\text{ (fundamental property)}}\\\\\n",
    "&= \\mathbb{E}[XY]\n",
    "& - \\mathbb{E}[X] ~ \\mathbb{E}[Y]\n",
    "&  \\color{gray}{\\text{ (Equal means property)}} \\\\\n",
    "&= \\sigma(X, Y) & & \n",
    "\\end{array}  \n",
    "\n",
    "\n",
    "* **Of conditional probability**\n",
    "  * The conditional probability of an event A, given a RV X. \n",
    "  * $\\mathbb{P}(A \\mid X) = \\mathbb{E}(\\text{1}_{A} \\mid X)$\n",
    "  * Then, $\\mathbb{E} \\bigl[ r(X) ~ \\mathbb{P}(A \\mid X) \\bigr] = \n",
    "       \\mathbb{E} \\bigl[ r(X) 1_A \\bigr] ~~ \\forall r:\\mathcal{S} \\rightarrow  \\mathbb{R}$\n",
    "\n",
    "* todo: [consistency](http://www.math.uah.edu/stat/expect/Conditional.html#prp9) featuring [cond exp revisited](http://www.math.uah.edu/stat/expect/Conditional2.html),\n",
    "[cov and cor](http://www.math.uah.edu/stat/expect/Covariance.html#blp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Variance\n",
    "===========\n",
    "* $\\mathbb{V}(Y \\mid X=x) = \\int (y - \\mu(x))^2 ~ f(y|x) ~ dx$  \n",
    "where $\\mu(x) = \\mathbb{E}(Y \\mid X=x)$\n",
    "* other deftn,  \n",
    "    $\\mathbb{V}(Y \\mid X=x) = \n",
    "            \\mathbb{E} \\left( \\left[Y - \\mathbb{E}(Y \\mid X) \\right]^2 \\biggm| X \\right)$\n",
    "* Variance in terms of exp:  \n",
    "$\\V(Y \\mid X) = \\mathbb{E} \\left( Y^2 \\mid X \\right) - \n",
    "                            \\left[\\mathbb{E}(Y \\mid X)\\right]^2$  \n",
    "  Proof:  \n",
    "  \\begin{array}{ll}\n",
    "\t\\V(Y \\mid X) &= \\E \\left( Y^2 - \n",
    "                              2 Y \\E(Y \\mid X) +\n",
    "                              \\left[\\E(Y \\mid X)\\right]^2 \\biggm| X \\right)\\\\\n",
    "    &= \\E(Y^2 \\mid X)\n",
    "        - 2 \\E \\left[Y \\E(Y \\mid X) \\mid X\\right] \n",
    "        + \\E\\left(\\left[\\E(Y \\mid X)\\right]^2 \\mid X\\right) \\\\\n",
    "\t& = \\E\\left(Y^2 \\mid X\\right)\n",
    "        - 2 \\E(Y \\mid X) \\E(Y \\mid X)\n",
    "        + \\left[\\E(Y \\mid X)\\right]^2 \\\\\n",
    "    &= \\E\\left(Y^2 \\mid X\\right)\n",
    "        - \\left[\\E(Y \\mid X)\\right]^2\n",
    "\\end{array}\n",
    "\n",
    "* **Variance**  \n",
    "$\\V(Y) = \\E\\V(Y|X) + \\V\\E(Y|X)$  \n",
    "That is, Variance of Y is the sum of expected variance and the variance of expected conditional. This helps in compute $\\V(Y)$ if we know $\\P(Y \\mid X)$   \n",
    "*Proof*  \n",
    "  \\begin{array}{llr}\n",
    "    \\text{We know } \\V[Y] &= \\E[Y^2] - \\E[Y]^2\\\\\n",
    "    \\Rightarrow \\E[Y^2]   &= \\V[Y] + \\E[Y]^2\\\\\n",
    "    \\text{Let } Z &= \\E[Y \\mid X]&\\\\\n",
    "    \\Rightarrow \\E[Z] &= \\E \\left[ \\E[Y \\mid X] \\right] = \\E[Y]\\\\\n",
    "    \\V(Y \\mid X) &= \\E[Y^2 \\mid X] - Z^2 \\\\\n",
    "    \\E \\bigl[ \\V(Y \\mid X) \\bigr] \n",
    "    &= \\E\\left(Y^2\\right) - \\E\\left( Z^2\\right)\n",
    "    & \\color{gray}{\\text{ Var in terms of exp}}\\\\\n",
    "    &= \\bigl(\\V(Y) + \\left[\\E(Y)\\right]^2 \\bigr)\n",
    "    - \\E\\left(Z^2\\right)\n",
    "    & \\color{gray}{\\text{mean of sqr - sqr of mean}}\\\\\n",
    "    &= \\V(Y) + \\left[\\E(Y)\\right]^2\n",
    "    & \\\\\n",
    "    & \\hspace{20pt} - \\left[ \\V[Z] + \\E[Z]^2 \\right] \n",
    "    & \\color{gray}{\\text{same as above}}\\\\\n",
    "    & = \\V(Y) + \\left[\\E(Y)\\right]^2\n",
    "    - (\\V [ Z ] + (\\left[\\E(Y)\\right]^2))&\\\\\n",
    "    &= \\V(Y) - \\V [ Z ]\\\\\n",
    "    & = \\V(Y) - \\V \\bigl[ \\E(Y \\mid X) \\bigr] & \\color{green}{\\text{There you go}}\n",
    "  \\end{array}\n",
    "  \n",
    "* **MSE**  \n",
    "The mean squared error if we use $\\E(Y \\mid X)$ in place of Y is  \n",
    "\\begin{array}{llr}\n",
    "\\E \\left( \\left[Y - \\E(Y \\mid X) \\right]^2 \\right)\n",
    "& = \\E \\left[ \\V(Y \\mid X) \\right] &\n",
    "\\color{gray}{\\text{Conditional variance: other deftn}}\\\\\n",
    "& = \\V(Y) - \\V\\left[ \\E(Y \\mid X) \\right] &\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independence\n",
    "=======\n",
    "\n",
    "* Two events are independent if $\\P(AB) = \\P(A) \\P(B)$.\n",
    "* A set of events $\\{A_i\\}_{i}$ are independent if  \n",
    "$$\n",
    "\\P \\left( \\bigcap_{i} A_i \\right) = \\prod_{i} \\P(A_i)\n",
    "$$\n",
    "* Suppose that A and B are disjoint events, each with positive probability. Can they be independent? No. This follows since $\\P(A)\\P(B) \\gt 0$ yet $\\P(AB) = \\P(\\phi) = 0$. Except in this special case, there is no way to judge independence by looking at the sets in a Venn diagram.\n",
    "* A, B are independent events $\\iff \\P \\left( A \\mid B \\right) = \\P(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations\n",
    "==============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Transformation  \n",
    "\\begin{array}{llr}\n",
    "\\E \\left[ \\bf{a}^T \\bf{X} \\right] &= \\E \\bigl[ \\sum_{i=1}^{n} a_i ~ X_i \\bigr]\\\\\n",
    "&=  \\sum_{i=1}^{n} a_i ~ \\E \\left[ X_i \\right]\\\\\n",
    "&=  \\sum_{i=1}^{n} a_i ~ \\mu_i\\\\\n",
    "&= \\bf{a}^T \\bf{\\mu}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{llr}\n",
    "\\V \\left[ \\bf{a}^T \\bf{X} \\right] &= \\E \\bigl[ \\left(\\bf{a}^T \\bf{X} - \\bf{a}^T \\bf{\\mu} \\right)^2 \\bigr]\\\\\n",
    " &= \\E \\bigl[ \\bf{a}^T \\left( \\bf{X} - \\bf{\\mu} \\right)^2 \\bf{a}\\bigr]\\\\\n",
    " &= \\bf{a}^T \\E \\bigl[ \\left( \\bf{X} - \\bf{\\mu} \\right)^2 \\bigr] \\bf{a}\\\\\n",
    "  &= \\bf{a}^T \\Sigma \\bf{a}\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Transformation     \n",
    "\\begin{array}{llr}\n",
    "Y = \\bf{A} \\bf{X}\n",
    "&= \\left[\n",
    "\\begin{matrix}\n",
    "\\bf{a_1}^T\\\\\n",
    "\\bf{a_2}^T\\\\\n",
    "\\cdots\\\\\n",
    "\\bf{a_n}^T\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\cdots\\\\\n",
    "x_n\\\\\n",
    "\\end{matrix}\n",
    "\\right]\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{llr}\n",
    "\\E \\bigl[Y_1\\bigr] = \\E \\left[ a_1^T X \\right] &= a_1^T \\mu\\\\\n",
    "\\Rightarrow \\E [A ~ X] = A \\mu\\\\\n",
    "\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{llr}\n",
    "\\text{Let } Z = X - \\mu \\\\\n",
    "\\sigma(Y_i, Y_j) &= \\E \\bigl[ \\left(Y_i - \\E[Y_i] \\right) \\left(Y_j - \\E[Y_j] \\right) \\bigr]\\\\\n",
    "&= \\E \\bigl[ a_i^T \\left(X - \\mu \\right) a_j^T \\left(X - \\mu \\right) \\bigr]\\\\\n",
    "&= \\E \\bigl[ a_i^T Z a_j^T Z \\bigr]\\\\\n",
    "a_i^T Z a_j^T Z &=  \\left( \\sum_{k=1}^{n} a_{ik}Z_k \\right)\n",
    "                    \\left( \\sum_{m=1}^{n} a_{jm}Z_m \\right)\\\\\n",
    "&= \\sum_{k=1}^{n} \\sum_{m=1}^{n} a_{ik} a_{jm} Z_k Z_m\\\\\n",
    "&= a_i^T ~ Z ~ Z^T ~ a_j\\\\\n",
    "=> \\E \\bigl[ a_i^T Z a_j^T Z \\bigr] &= \\E \\left[ a_i^T ~ Z ~ Z^T ~ a_j \\right]\\\\\n",
    "&= a_i^T ~ \\E \\left[ Z ~ Z^T \\right] ~ a_j\\\\\n",
    "&= a_i^T ~ \\Sigma ~ a_j\\\\\n",
    "\\text{Thus  } \\V[Y] &= A ~ \\Sigma ~ A^T\\\\\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Problems\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $X_1, \\cdots, X_n \\sim$ Uniform(0,1). $Y_n = \\operatorname{max}\\{X_1, \\cdots, X_n\\}$. Find $\\E[Y_n]$\n",
    "    * $\\P(Y_n \\le y) = F(y) = \\P(X \\le y)^n = y^n$\n",
    "    * $\\P(Y_n = y) = f(y) = n y^{n-1}$\n",
    "    * $\\E[Y_n] = \\int_{0}^{1} y ~ n ~ y^{n-1} ~ dy = n \\int_{0}^{1} y^n ~ dy = \\frac{n}{n+1} $\n",
    "    * \\begin{array}{ll}\n",
    "        \\V[Y_n] &= \\int_{0}^{1} (y-\\frac{n}{n+1})^2 ~ n ~ y^{n-1} ~ dy\\\\\n",
    "        &= \\int_{0}^{1} \\bigl( y^2 - \\frac{2n}{n+1} y + \\left( \\frac{n}{n+1} \\right)^2 \\bigr) ~ n ~ y^{n-1} ~ dy\\\\\n",
    "        &= n \\int_{0}^{1} \\bigl( y^{n+1} - \\frac{2n}{n+1} y^n + \\left( \\frac{n}{n+1} \\right)^2 y^{n-1} \\bigr) ~ dy\\\\\n",
    "        &= n \\left( \\frac{1}{n+2} - \\frac{2n}{(n+1)^2} +\\frac{n}{(n+1)^2} \\right)\\\\\n",
    "        &= n \\left( \\frac{1}{n+2} - \\frac{n}{(n+1)^2} \\right)\\\\\n",
    "        &= n \\left( \\frac{n^2+2n+1 - n^2 - 2n}{(n+2)(n+1)^2} \\right)\\\\\n",
    "        &= \\frac{n}{(n+2)(n+1)^2}\n",
    "      \\end{array}\n",
    "    * $\\E[Y_n^2] = \\int_{0}^{1} y^2 ~ n ~ y^{n-1} ~ dy = n \\int_{0}^{1} y^{n+1} ~ dy = \\frac{n}{n+2}$\n",
    "    * \\begin{array}{llr}\n",
    "        \\V[Y_n] &= \\E[Y_n^2] - \\E[Y_n]^2 &\\\\\n",
    "                &= \\frac{n}{n+2} - \\frac{n^2}{(n+1)^2} &\\\\\n",
    "                &= n \\left( \\frac{1}{n+2} - \\frac{n}{(n+1)^2} \\right) & \\color{gray}{\\text{same as above}}\n",
    "      \\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_hist(data, label, in_bins=10, plt_object=plt):\n",
    "    n = data.shape[0]\n",
    "    hist, edges = np.histogram(data, bins=in_bins)\n",
    "    bin_centers = (edges[1:]+edges[:-1])/2.\n",
    "    plt_object.plot(bin_centers, hist, label=label)\n",
    "\n",
    "n = 9\n",
    "T1, T2 = 10**3, 10**3\n",
    "grand_total_x, grand_total_x_sqr = 0, 0\n",
    "arr_total_x, arr_total_x_sqr = np.zeros((T1, 1)), np.zeros((T1, 1))\n",
    "for t1 in range(T1):\n",
    "    total_x, total_x_sqr = 0, 0\n",
    "    for t2 in range(T2):\n",
    "        x = np.random.rand(9, 1)\n",
    "        y = np.max(x)\n",
    "        total_x += y\n",
    "        total_x_sqr += (y*y)\n",
    "    arr_total_x[t1]    = (total_x    *1./T2)\n",
    "    arr_total_x_sqr[t1]= (total_x_sqr*1./T2)\n",
    "    grand_total_x     += (total_x    *1./T2)\n",
    "    grand_total_x_sqr += (total_x_sqr*1./T2)\n",
    "    \n",
    "print('E[y]  : expected: {0}, actual: {1} '.format((n*1./(n+1)),(grand_total_x/T1)))\n",
    "print('E[y^2]: expected: {0}, actual: {1} '.format((n*1./(n+2)),(grand_total_x_sqr/T1)))\n",
    "show_hist(arr_total_x    , 'E[x]'  , 30)\n",
    "show_hist(arr_total_x_sqr, 'E[x^2]', 30)\n",
    "plt.axvline((n*1./(n+1)),c='r',label='exp(x)')\n",
    "plt.axvline((n*1./(n+2)),c='m',label='exp(x^2)')\n",
    "plt.legend(loc='upper center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**  \n",
    "A fair coin is tossed until a head is obtained. What is the expected number of tosses that will be required?\n",
    "  * p = 0.5\n",
    "  * $f(x) = p (1-p)^{x-1}$\n",
    "  * \\begin{array}{lll}\n",
    "      S_{\\infty}  & = \\sum_{n=1}^{\\infty} n ~ r^{n-1} &= 1 + 2r + 3r^2 + 4r^3 + \\cdots\\\\\n",
    "      rS_{\\infty} & = \\sum_{n=1}^{\\infty} n ~ r^{n}   &= 0 +  r + 2r^2 + 3r^3 + \\cdots\\\\\n",
    "      S_{\\infty} - rS_{\\infty} & &= 1 + r + r^2 + r^3 + \\cdots\\\\\n",
    "      (1-r) S_{\\infty} &= \\frac{1}{1-r} &\\\\\n",
    "      S_{\\infty} & \\frac{1}{(1-r)^2} &\n",
    "    \\end{array}\n",
    "  * \\begin{array}{llr}\n",
    "      \\E[X] &= \\sum_{n=1}^{\\infty} n ~ p ~ (1-p)^{n-1} &\\\\\n",
    "            &= p ~ \\sum_{n=1}^{\\infty} n ~ (1-p)^{n-1} &\\\\\n",
    "            &= p ~ \\frac{1}{(1-(1-p))^2} &\\\\\n",
    "            &= p ~ \\frac{1}{p^2} &\\\\\n",
    "            &= \\frac{1}{p} &\n",
    "    \\end{array}\n",
    "   * So in case of a fair coin, $\\E[X] = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**   \n",
    "Let $X_1, X_2 , \\cdots,X_n \\sim \\mathcal{N}(0,1)$.  \n",
    "Let $\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$.  \n",
    "Plot $\\overline{X}_n$ versus n for n = 1,...,10,000.  \n",
    "Repeat for $X_1, X_2 , \\cdots,X_n \\sim \\text{Cauchy}$.  \n",
    "Explain why there is such a difference.  \n",
    "****************************************\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**3\n",
    "x_n_bar_normal, x_n_bar_cauchy = np.zeros((N,1)), np.zeros((N,1))\n",
    "for n in range(N):\n",
    "    x = np.random.normal(0,1,(n+1,1))\n",
    "    x_n_bar_normal[n] = np.average(x)\n",
    "    \n",
    "    x = np.random.standard_cauchy(n+1)\n",
    "    x_n_bar_cauchy[n] = np.average(x)\n",
    "\n",
    "f, axarr = plt.subplots(2, 2)\n",
    "\n",
    "axarr[0, 0].plot(x_n_bar_normal)\n",
    "axarr[0, 0].set_title('Normal: X_bar_n')\n",
    "\n",
    "axarr[0, 1].plot(x_n_bar_cauchy)\n",
    "axarr[0, 1].set_title('Cauchy: X_bar_n')\n",
    "\n",
    "show_hist(x_n_bar_normal, label='Normal', in_bins=30, plt_object=axarr[1,0])\n",
    "axarr[1, 0].set_title('Normal: Histogram')\n",
    "\n",
    "show_hist(x_n_bar_cauchy, label='Cauchy', in_bins=30, plt_object=axarr[1,1])\n",
    "axarr[1, 1].set_title('Cauchy: Histogram')\n",
    "\n",
    "# Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "\n",
    "plt.title('́Effects of tail')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Exponential distribution](2-random-variables.ipynb#Exponential-distribution)\n",
    "\n",
    "$f(x) = \\lambda e^{-\\lambda x}$\n",
    "\n",
    "\\begin{array}{ll}\n",
    "              u = x  & dv = e^{-\\lambda x} dx\n",
    "         \\end{array}\n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\E[X] &= \\int_{0}^{\\infty} x \\lambda  e^{-\\lambda x} dx =\\lambda \\int_{0}^{\\infty} x e^{-\\lambda x} dx &\\\\\n",
    "u = x  &  dv = e^{-\\lambda x} dx & \\\\\n",
    "du =dx & v  = -\\frac{1}{\\lambda} e^{-\\lambda x} &\\\\\n",
    "\\E[X] &=  \\lambda \\left(   \\left[ x \\left( -\\lambda e^{-\\lambda x} \\right) \\right]_{0}^{\\infty} -\n",
    "                        \\left( \\int_{0}^{\\infty} -\\frac{1}{\\lambda} e^{-\\lambda x} dx \\right)\n",
    "                   \\right) & \\\\\n",
    "      &=  \\int_{0}^{\\infty} e^{-\\lambda x} dx &\\\\\n",
    "      &= \\left[  \\frac{e^{-\\lambda x}}{-\\lambda} \\right]_{0}^{\\infty} & \\\\\n",
    "      &= \\frac{1}{\\lambda}&\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**\n",
    "\n",
    "$X \\sim \\mathcal{N}(0,1)$  $\\E[X] = 0, \\V[X] = 1$  \n",
    "\n",
    "$Y = e^X$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{ll}\n",
    "\\E[Y] = \\E[e^X] &= \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{\\infty} e^x ~ e^{-x^2/2} dx \n",
    "= \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{\\infty} e^{x-x^2/2} dx\\\\\n",
    "&= e^{1/2} \\frac{1}{\\sqrt{2 \\pi}}\\int_{-\\infty}^{\\infty} e^{(x-1)^2/2} dx\\\\\n",
    "&= e^{1/2} = \\sqrt{e}\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\color{gray}{x - \\frac{x^2}{2}\n",
    "= -\\frac{1}{2} \\left( x^2 - x \\right)\n",
    "= -\\frac{1}{2} \\left( x^2 - x + 1 - 1 \\right)\n",
    "= -\\frac{1}{2} \\left( x - 1 \\right)^2 + \\frac{1}{2}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{array}{ll}\n",
    "\\V[Y] = \\V[e^X] &= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} (e^x - e^{1/2})^2 e^{-x^2/2} dx\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}} \\bigl(\n",
    "\\int_{-\\infty}^{\\infty} e^{2x - x^2/2} dx\n",
    "- 2 e^{1/2} \\int_{-\\infty}^{\\infty} e^{x-x^2/2} dx\n",
    "+ e \\int_{-\\infty}^{\\infty} e^{-x^2/2} dx\n",
    "\\bigr)\\\\\n",
    "&= \n",
    "e^2\n",
    "- 2 e^{1/2} (e^{1/2})\n",
    "+ e (1)\\\\\n",
    "&= e^2 - e\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\color{gray}{\n",
    "2x - x^2/2 = -\\frac{1}{2} (x^2 - 4x) = -\\frac{1}{2} (x-2)^2 + 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second way**  \n",
    "\n",
    "From [stack overflow](http://stats.stackexchange.com/a/89973)\n",
    "\n",
    "Using moment generating functions.  \n",
    "$\\phi_X(t) = \\E \\left[ e^{Xt} \\right] = \\int e^{xt} dF(x)$.  \n",
    "Also, $\\phi_X^{(k)}(x) = \\E[X^k]$  \n",
    "\n",
    "For normal distribution, $\\phi_X(t) = \\exp \\left( \\mu t + \\frac{\\sigma^2 t^2}{2} \\right)$\n",
    "\n",
    "For $\\mathcal{N}(0,1), ~ \\phi_X(t) = \\exp \\left( \\frac{t^2}{2} \\right)$\n",
    "\n",
    "We need to find $\\E[e^X]$ and $\\V[e^X]$.\n",
    "Now $\\E[e^X] = \\phi_X(1) =  \\exp \\left( \\frac{1}{2} \\right) = \\sqrt{e}$  \n",
    "$\\E[Y^2] = \\E[e^{2X}] = \\phi_X(2) = e^2$  \n",
    "Thus, \n",
    "$\\V[e^X] = \\E[X^2] - \\E[X]^2 = e^2 - e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**\n",
    "\n",
    "Let $Y$ be such that $\\P(Y=1) = \\P(Y=-1) = \\frac{1}{2}$. Let $X = \\sum_{i=1}^{N} Y_i$. Find $\\E[X], \\V[X]$.\n",
    "\n",
    "--------------------------\n",
    "\n",
    "$\\E[Y]   = \\frac{1}{2}(+1) + \\frac{1}{2}(-1) = 0$  \n",
    "$\\E[Y^2] = \\frac{1}{2}(+1) + \\frac{1}{2}(+1) = 1$  \n",
    "\n",
    "$\\E[X] = \\E[\\sum_{i=1}^{n} Y_i] = \\sum_{i=1}^{n} \\E [Y_i] = \\sum_{i=1}^{n} \\E[Y] = 0$  \n",
    "$\\E[X^2] = \\E[ \\left( \\sum_{i=1}^{n} Y_i \\right)^2]\n",
    "= n \\E\\left[ Y^2 \\right] + 2 \\frac{n(n-1)}{2} \\E[Y]^2\n",
    "= n$   \n",
    "\n",
    "$\\V[X] = \\E \\left[ X^2 \\right] - \\E[X]^2 = n - 0 = n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1, T2 = 10**1, 10**1\n",
    "N, n_step = 10**4, 25\n",
    "exp_x, exp_x_sqr = np.zeros((int(N/n_step),1)), np.zeros((int(N/n_step),1))\n",
    "for n in range(0, N, n_step):\n",
    "    ix_n = int(n/n_step)\n",
    "    for t1 in range(T1):\n",
    "        y = np.random.randint(2, size=(T2,n))*2 - 1\n",
    "        #exp_x_t1 = np.sum(y, axis=1)\n",
    "        #tmp = np.sum(exp_x_t1)\n",
    "        tmp_x, tmp_x_sqr = np.sum(y), np.sum(np.square(y))\n",
    "        exp_x[ix_n] += (tmp_x/T2)\n",
    "        exp_x_sqr[ix_n] += (tmp_x_sqr/T2)\n",
    "\n",
    "exp_x /= T1\n",
    "exp_x_sqr /= T1\n",
    "x = range(0, N, n_step)\n",
    "plt.plot(x, exp_x)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, exp_x_sqr, label='actual')\n",
    "plt.plot(x, x, label='expected')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feynman's Restaurant problem\n",
    "-----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "T = 10\n",
    "y_exp, y_max = np.zeros((N,1)), np.zeros((N,1))\n",
    "for M in range(1, N):\n",
    "    y = np.random.randint(low=1,high=N,size=(T,M))\n",
    "    y_sum = np.sum(y, axis=0)/(1.*T)\n",
    "    #print(y)\n",
    "    y_mx = np.max(y, axis=1)\n",
    "    #print(y_max)\n",
    "    y_exp[M] = np.sum(y_sum/M)\n",
    "    y_max[M] = np.average(y_mx)\n",
    "\n",
    "#print(y_max[:-10].transpose())\n",
    "\n",
    "plt.plot(y_exp)\n",
    "plt.axhline((N/2.),c='m',label='exp(x^2)')\n",
    "plt.plot(y_max)\n",
    "plt.xlabel('M')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_exp_y_1(n, d):\n",
    "    numtr = d*(d**(d+1) - 1)\n",
    "    dentr = (d+1) * (n**(d-1))\n",
    "    return numtr*1.0/dentr\n",
    "\n",
    "def show_exp_y_2(n, d):\n",
    "    factor1 = d*1.0/(n**(d-1))\n",
    "    series1 = [y**d for y in range(1,d)]\n",
    "    return factor1 * sum(series1)\n",
    "n=40\n",
    "d=range(1,n)\n",
    "plt.plot(d, [show_exp_y_2(n, dd) for dd in d])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(50, size=(7,4))\n",
    "print(x)\n",
    "print('')\n",
    "print(np.sum(x, axis=0))\n",
    "print(np.max(x, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Exp, Variance\n",
    "---------------------------\n",
    "Suppose we generate a random variable X in the following way. First we flip a fair coin. If the coin is heads, take X to have a Unif(0,1) distribution. If the coin is tails, take X to have a Unif(3,4) distribution.\n",
    "(a) Find the mean of X.\n",
    "(b) Find the standard deviation of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:  \n",
    "\n",
    "(a)\n",
    "\\begin{array}{llr}\n",
    "\\E[X \\mid Y=y] &= \\sum_{x_i \\in R_{X}} x_i \\P_{X \\mid Y} ~ (x_i \\mid y)&\\\\\n",
    "\\E[X] &= \\E \\bigl[ \\E \\left[ X \\mid Y \\right] \\bigr]&\\\\\n",
    "      &= \\sum_{y_i} \\E \\left[ X \\mid Y \\right] ~ \\P(y_i) &\\\\\n",
    "\\E[X \\mid Y=H] &= \\E[U(0,1)] = 0.5 &\\\\\n",
    "X^{\\prime} &= U(3,4) = 3 + U(0,1)&\\\\\n",
    "\\E[X \\mid Y=T] &= \\E[U(3,4)] = 3.5&\\\\\n",
    "\\E[X] &= \\sum_{Y} ~\\E[X,Y] &\\\\\n",
    "      &= \\sum_{Y} ~\\E[X \\mid Y] ~\\P[Y]  &\\\\\n",
    "      &= \\frac{1}{2} \\E[U(0,1)] + \\frac{1}{2} \\E[U(3,4)] &\\\\\n",
    "      &= 2.0 &\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)  \n",
    "\\begin{array}{llr}\n",
    "\\V[X] &= \\E \\V [X \\mid Y] + \\V \\E [X \\mid Y] &\\\\\n",
    "\\V[X \\mid Y] &= 0.5 & \\color{gray}{\\text{irrespective of whether Y is H or T}}\\\\\n",
    "\\Rightarrow \\E \\V[X \\mid Y] = \\frac{1}{12}\\\\\n",
    "\\V \\E[X \\mid Y] &= \\E \\left[ \\left( \\E[X \\mid Y] - \\E[\\E[X \\mid Y]] \\right)^2  \\right]\\\\\n",
    "&= \\E \\left[ \\left( \\E[X \\mid Y] - \\E[X] \\right)^2  \\right]\\\\\n",
    "&=  P(Y=H)\\left( \\E[X \\mid Y] - 2.0 \\right)_{Y=H}^2+ \n",
    "    P(Y=T)\\left( \\E[X \\mid Y] - 2.0 \\right)_{Y=H}^2  \\\\\n",
    "&= \\frac{1}{2} \\left( 0.5 - 2 \\right)^2 +\n",
    "   \\frac{1}{2} \\left( 3.5 - 2 \\right)^2 \\\\\n",
    "&= \\left( \\frac{3}{2} \\right)^2 = 2.25\\\\\n",
    "\\V[X] = 2.25 + .083  = 2.33\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = T2 = 5*10**3\n",
    "x_mean_t1 = np.zeros((T1,1))\n",
    "x_var_t1 = np.zeros((T1,1))\n",
    "for t1 in range(T1):\n",
    "    x = np.random.randint(low=0, high=2,size=(T2,1))\n",
    "    in_heads = np.sum(x)\n",
    "    in_tails = T2 - in_heads\n",
    "    \n",
    "    x_heads = np.random.rand(in_heads)\n",
    "    x_tails = np.random.rand(in_tails)+3.\n",
    "    \n",
    "    mean_heads = np.average(x_heads)\n",
    "    mean_tails = np.average(x_tails)\n",
    "    \n",
    "    mean_t2 = (mean_heads+mean_tails)/2.\n",
    "    \n",
    "    x_diff_heads = np.square(x_heads - mean_t2)\n",
    "    x_diff_tails = np.square(x_tails - mean_t2)\n",
    "    var_t2 = (np.sum(x_diff_heads) + np.sum(x_diff_tails))/(T2-1)\n",
    "    \n",
    "    x_mean_t1[t1] = mean_t2\n",
    "    x_var_t1[t1] = var_t2\n",
    "    \n",
    "show_hist(x_mean_t1,label='hist: E[X]',in_bins=10**2)\n",
    "plt.axvline(2.0,color='m',label='expected: E[X]')\n",
    "show_hist(x_var_t1,label='hist: V[X]',in_bins=10**2)\n",
    "plt.axvline(2.33,color='c',label='expected: V[X]')\n",
    "plt.legend(loc='upper center')\n",
    "print('avg E[X] :: expected:', 2.0, ' actual: ', np.average(x_mean_t1))\n",
    "print('avg E[X] :: expected:', 2.33\n",
    "      , ' actual: ', np.average(x_var_t1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question is to help you understand the idea of a sampling distribution. Let $X_1, \\cdots, X_n$ be IID with mean $\\mu$ and variance $\\sigma^2$. Let $\\overline{X_n} = 1/n \\sum_{i=1}^{n} X_i$. Then $\\overline{X}_n$ is a statistic, that is, a function of the data. Since $\\overline{X}_n$ is a random variable, it has a distribution. This distribution is called the sampling distribution of the statistic. Recall from Theorem 3.17 that $\\E(\\overline{X}_n) = μ \\text{ and } \\V(\\overline{X}_n) = \\sigma^2/n$. Don’t confuse the distribution of the data $f_X$ and the distribution of the statistic $f_{\\overline{X}_n}$ . To make this clear, let $X_1, \\cdots, X_n ∼\\sim$ Uniform(0, 1). Let $f_X$ be the density of the Uniform(0,1). Plot $f_X$. Now let $X_n = n^{-1} \\sum_{i=1}^{n} X_i$. Find $\\E(\\overline{X}_n) \\text{ and } \\V(\\overline{X}_n)$. Plot them as a function of n. Interpret. Now simulate the distribution of $\\overline{X}_n$ for n = 1, 5, 25, 100. Check that the simulated values of $\\E(\\overline{X}_n) \\text{ and } \\V(\\overline{X}_n)$ agree with your theoretical calculations. What do you notice about the sampling distribution of Xn as n increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "650px",
    "left": "0px",
    "right": "auto",
    "top": "134px",
    "width": "152px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "370px",
   "left": "681px",
   "right": "34px",
   "top": "129px",
   "width": "241px"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
