{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# required for interactive plotting\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "import numpy.polynomial as np_poly\n",
    "\n",
    "from IPython.display import Math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialization  \n",
    "\n",
    "$ \\newcommand{\\E}[1]{\\mathbb{E}\\left[#1\\right]}$  \n",
    "$ \\newcommand{\\V}[1]{\\mathbb{V}\\left[#1\\right]}$\n",
    "$ \\newcommand{\\P}{\\mathbb{P}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias-Variance Decomposition\n",
    "===============\n",
    "\n",
    "$$\n",
    "\\operatorname{MSE}(\\hat{\\theta})=\n",
    "\\V{\\hat{\\theta}}+\n",
    "\\left(\\operatorname{Bias}(\\hat{\\theta},\\theta)\\right)^2.\n",
    "$$\n",
    "\n",
    "Proof:\n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\operatorname{MSE}(\\hat{\\theta})\n",
    "&\\equiv \\E{(\\hat{\\theta}-\\theta)^2}\n",
    "\\\\\n",
    "&=\n",
    " \\E{\\left(\\hat{\\theta}-\\E{\\hat\\theta}+\\E{\\hat\\theta}-\\theta\\right)^2}\\\\\n",
    "&= \\E{ \\left(\n",
    "            \\hat{\\theta}-\\E{\\hat\\theta}\n",
    "        \\right)^2\n",
    "        +2 \\left( (\\hat{\\theta}-\\E{\\hat\\theta})(\\E{\\hat\\theta}-\\theta) \n",
    "           \\right)\n",
    "        +\\left( \n",
    "            \\E{\\hat\\theta}-\\theta\n",
    "         \\right)^2\n",
    "     }\n",
    "\\\\\n",
    "&=  \\E{\\left( \n",
    "          \\hat{\\theta}-\\E{\\hat\\theta}\n",
    "        \\right)^2\n",
    "     }\n",
    "     +2 \\mathbb{E}\n",
    "        \\Big[\n",
    "            (\\hat{\\theta}-\\mathbb{E}(\\hat\\theta))\n",
    "            \\overbrace{ (\\E{\\hat\\theta}-\\theta)}^\n",
    "                      {\\begin{smallmatrix}\n",
    "                            \\text{This is} \\\\\n",
    "                            \\text{a constant,} \\\\\n",
    "                            \\text{so it can be} \\\\ \n",
    "                            \\text{pulled out.}\n",
    "                       \\end{smallmatrix}\n",
    "                      }\n",
    "         \\Big]\n",
    "       + \\mathbb{E}\n",
    "         \\Big[\n",
    "            \\overbrace{ \\left(\\E{\\hat\\theta}-\\theta\\right)^2}^\n",
    "                      {\n",
    "                         \\begin{smallmatrix}\n",
    "                             \\text{This is a} \\\\\n",
    "                             \\text{constant, so its} \\\\\n",
    "                             \\text{expected value} \\\\\n",
    "                             \\text{is itself.}\n",
    "                         \\end{smallmatrix}\n",
    "                       }\n",
    "         \\Big]\n",
    "\\\\\n",
    "& = \\E{\n",
    "        \\left( \n",
    "            \\hat{\\theta}-\\E{\\hat\\theta}\n",
    "         \\right)^2\n",
    "     }\n",
    "     +2( \\overbrace\n",
    "             {\\E{\\hat\\theta}-\\theta}^\n",
    "             {\\begin{smallmatrix}\n",
    "                 \\text{That first} \\\\\n",
    "                 \\text{constant, now} \\\\\n",
    "                 \\text{pulled out.}\n",
    "               \\end{smallmatrix}\n",
    "              }\n",
    "        )\n",
    "        \\underbrace{\\E{\n",
    "                        \\hat{\\theta}-\\E{\\hat\\theta}\n",
    "                        }\n",
    "                   }_\n",
    "                   {=\\E{\\hat\\theta}-\\E{\\hat\\theta}=0}\n",
    "     +\\left(\n",
    "          \\E{\\hat\\theta}-\\theta\n",
    "       \\right)^2\n",
    "\\\\\n",
    "& = \\E{\n",
    "        \\left(\n",
    "            \\hat{\\theta}-\\mathbb{E}(\\hat\\theta)\n",
    "        \\right)^2\n",
    "     }\n",
    "     +\\left(\n",
    "         \\E{\\hat\\theta}-\\theta\n",
    "      \\right)^2\n",
    "\\\\\n",
    "& = \\V{\\hat\\theta}+ \\operatorname{Bias}(\\hat\\theta,\\theta)^2\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression\n",
    "======\n",
    "\n",
    "Mean Squred error =\n",
    "$ \\frac{\\text{Residual Sum of squares}}\n",
    "       {\\text{#Degrees of Freedom}}$\n",
    "                \n",
    "[#Degrees of Freedom][steeltorrie1960] = \n",
    "\\begin{cases}\n",
    "    n-p   & \\text{for p regressors}\\\\\n",
    "    n-p-1 & \\text{if an intercept is used}\\\\\n",
    "\\end{cases}\n",
    "\n",
    "[steeltorrie1960]: google.com \"Steel, R.G.D, and Torrie, J. H., Principles and Procedures of Statistics with Special Reference to the Biological Sciences., McGraw Hill, 1960, page 288.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: \n",
    "http://www.wikiwand.com/en/Errors_and_residuals_in_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean\n",
    "===\n",
    "\n",
    "\\begin{array}{llr}\n",
    "\\overline{X} &= \\frac{1}{n} \\sum_{i=1}^{n} X_i\\\\\n",
    "\\E{\\overline{X}} &= \\mu &\\color{gray}{\\text{  True Mean}}\\\\\n",
    "\\operatorname{MSE}\\left(\\overline{X}\\right)\n",
    "&= \\E{\\left( \n",
    "        \\overline{X} - \\mu\n",
    "      \\right)^2\n",
    "   }\n",
    "\\\\\n",
    "&= \\left(\n",
    "     \\frac{\\sigma}{\\sqrt{n}}\n",
    "   \\right)^2\n",
    "\\\\\n",
    "&= \\frac{\\sigma^2}{n}\n",
    "\\end{array}\n",
    "\n",
    "For Gaussian distribution, this is the best unbiased estimator; that is, it has the lowest MSE among all unbiased estimators but not for the Uniform distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:  \n",
    "http://www.wikiwand.com/en/Best_unbiased_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance\n",
    "=====\n",
    "\n",
    "Corrected Sampled Variance  \n",
    "\\begin{array}{llr}\n",
    "S^2_{n-1}\n",
    "&= \\frac{1}{n-1}\n",
    "   \\sum_{i=1}^n\n",
    "   \\left(\n",
    "       X_i-\\overline{X}\\,\n",
    "   \\right)^2\n",
    "\\\\\n",
    "&= \\frac{1}{n-1}\n",
    "   \\sum_{i=1}^n\n",
    "   \\left(\n",
    "       X_i^2 - 2X_i\\overline{X} + \\overline{X}^2\n",
    "   \\right)\n",
    "\\\\\n",
    "&= \\frac{1}{n-1}\n",
    "   \\left(\n",
    "       \\sum_{i=1}^n X_i^2\n",
    "       - 2\\overline{X} \\sum_{i=1}^n X_i\n",
    "       + \\sum_{i=1}^n \\overline{X}^2\n",
    "   \\right).\n",
    "\\\\\n",
    "&= \\frac{1}{n-1}\n",
    "   \\left(\n",
    "       \\sum_{i=1}^n X_i^2\n",
    "       - 2\\overline{X} \\left( n \\overline{X} \\right)\n",
    "       + n \\overline{X}^2\n",
    "   \\right).\n",
    "\\\\\n",
    "&= \\frac{1}{n-1}\n",
    "   \\left(\n",
    "       \\sum_{i=1}^n\n",
    "       X_i^2-n\\overline{X}^2\n",
    "   \\right).\n",
    "\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unbiased since its expected value is $\\sigma^2$ [?].  \n",
    "Hence it is also called the unbiased sample variance.  \n",
    "Its [MSE][mood1974] is:  \n",
    "\\begin{array}{ll}\n",
    "\\operatorname{MSE}(S^2_{n-1})\n",
    "&= \\frac{1}{n}\n",
    "   \\left(\n",
    "       \\mu_4-\\frac{n-3}{n-1}\\sigma^4\n",
    "   \\right)\n",
    "\\\\\n",
    "&= \\frac{1}{n}\n",
    "   \\left(\n",
    "       \\gamma_2+\\frac{2n}{n-1}\n",
    "   \\right)\\sigma^4\n",
    "\\end{array}\n",
    "where  \n",
    "\\begin{array}{ll}\n",
    "\\mu_4 & \\text{Fourth central moment}\\\\\n",
    "\\gamma_2 = \\mu_4/\\sigma^4 -3 & \\text{excess kurtosis}\n",
    "\\end{array}\n",
    "\n",
    "when n is large,\n",
    "\\begin{array}{llr}\n",
    "\\operatorname{MSE}(S^2_{n-1})\n",
    "&\\approx\n",
    "\\frac{1}{n}\n",
    "\\left(\n",
    "    \\gamma_2^{\\prime} - 1\n",
    "\\right)\n",
    "\\sigma^4\\\\\n",
    "\\text{where }\n",
    "& \\gamma_2^{\\prime}\n",
    "& \\text{ is kurtosis}\n",
    "\\end{array}\n",
    "\n",
    "[mood1974]: google.com \"Mood, A.; Graybill, F.; Boes, D. (1974). Introduction to the Theory of Statistics (3rd ed.). McGraw-Hill. p. 229.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other estimators\n",
    "-----------------\n",
    "\n",
    "\\begin{array}\n",
    "\\text{If  } S^2_a\n",
    "&=\\frac{n-1}{a} S^2_{n-1}\n",
    "\\\\\n",
    "&=\n",
    "\\frac{1}{a}\n",
    "\\sum_{i=1}^n\n",
    "\\left(\n",
    "    X_i-\\overline{X}\n",
    "\\right)^2\n",
    "\\end{array}\n",
    "\n",
    "Then the MSE is\n",
    "\n",
    "\\begin{array}{lll}\n",
    "\\operatorname{MSE}(S^2_a)\n",
    "&=\\operatorname{E}\n",
    "\\left(\n",
    "    \\left(\n",
    "        \\frac{n-1}{a}\n",
    "        S^2_{n-1}-\\sigma^2\n",
    "    \\right)^2 \n",
    "\\right) \\\\\n",
    "&=\n",
    "\\frac{n-1}{n a^2}\n",
    "\\left[\n",
    "    (n-1)\\gamma_2+n^2+n\n",
    "\\right]\\sigma^4\n",
    "\\\\\n",
    "& \\hspace{30pt}\n",
    "-\\frac{2(n-1)}{a}\n",
    "\\sigma^4+\\sigma^4\n",
    "\\end{array}\n",
    "\n",
    "This is minimized when\n",
    "\\begin{array}{ll}\n",
    "a\n",
    "&=\n",
    "\\frac{(n-1)\\gamma_2+n^2+n}{n}\n",
    "\\\\\n",
    "&= n+1+\\frac{n-1}{n}\\gamma_2.\n",
    "\\end{array}\n",
    "\n",
    "* For Normal distribution\n",
    "  * $\\gamma_2=0$\n",
    "  * MSE is minimized when dividing by a=n+1\n",
    "* For Bernoulli\n",
    "  * with p=1/2, $\\gamma_2 = -2$\n",
    "  * MSE is minimized when a = n - 1 + 2/n\n",
    "* So irrespective of the value of $\\gamma_2$, we can get a better estimate (having lower MSE) by scaling down the unbiased estimator\n",
    "* This is an example of the shrinkage estimator, where one shrinks the estimator towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:  \n",
    "http://www.wikiwand.com/en/Shrinkage_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:  \n",
    "http://www.wikiwand.com/en/Analysis_of_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications\n",
    "=======\n",
    "\n",
    "* Minimizing MSE is a key criterion in selecting estimators: \n",
    "  * see [minimum mean-square error](http://www.wikiwand.com/en/Minimum_mean-square_error)\n",
    "  * Among unbiased estimators, minimizing the MSE is equivalent to minimizing the variance\n",
    "  * The estimator that does this is the [minimum variance unbiased estimator][mvue].\n",
    "  * However, a biased estimator may have lower MSE; see [estimator bias](http://www.wikiwand.com/en/Estimator_bias).\n",
    "  \n",
    "* In statistical modelling\n",
    "  * MSE represents the difference between the actual observations and the observation values predicted by the model\n",
    "  * It is used to determine the extent to which the model fits the data\n",
    "  * Also, whether the removal or some explanatory variables, simplifying the model, is possible without significantly harming the model's predictive ability.\n",
    "\n",
    "[mvue]: http://www.wikiwand.com/en/Minimum_variance_unbiased_estimator \"Minimum Variance Unbiased Estimator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:  \n",
    "1. http://www.wikiwand.com/en/Minimum_mean-square_error\n",
    "1. http://www.wikiwand.com/en/Minimum_variance_unbiased_estimator\n",
    "1. http://www.wikiwand.com/en/Estimator_bias\n",
    "1. Lehmann, E. L.; Casella, George (1998). Theory of Point Estimation (2nd ed.). New York: Springer. ISBN 0-387-98502-6. MR 1639875."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function\n",
    "========\n",
    "\n",
    "* Squared error loss is one of the most widely used loss functions in statistics, though its widespread use stems more from mathematical convenience than considerations of actual loss in applications.\n",
    "* Carl Friedrich Gauss, who introduced the use of mean squared error, was aware of its arbitrariness and was in agreement with objections to it on these grounds [ref][lehmann1998]\n",
    "\n",
    "[lehmann1998]: google.com \"Lehmann, E. L.; Casella, George (1998). Theory of Point Estimation (2nd ed.). New York: Springer. ISBN 0-387-98502-6. MR 1639875.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criticisms\n",
    "----------\n",
    "\n",
    "1. [Berger][berger1985]:\n",
    "  * Mean squared error is the negative of the expected value of one specific utility function, \n",
    "    the quadratic utility function,\n",
    "    which may not be the appropriate utility function to use under a given set of circumstances.\n",
    "  * There are, however,\n",
    "    some scenarios where MSE can serve as a good approximation to a \n",
    "    loss function occurring naturally in an application\n",
    "\n",
    "1. \n",
    "  * Like variance, MSE has the disadvantage of heavily weighting outliers.This is a result of the squaring of each term, which effectively weights large errors more heavily than small ones. [Sergio][sergio2001]\n",
    "   * This property, undesirable in many applications, has led researchers to use alternatives such as the [mean absolute error][MAE], or those based on the median.\n",
    "\n",
    "\n",
    "[berger1985]: http://www.ams.org/mathscinet-getitem?mr=0804611 \"Berger, James O. (1985). \"2.4.2 Certain Standard Loss Functions\". Statistical decision theory and Bayesian Analysis (2nd ed.). New York: Springer-Verlag. p. 60. ISBN 0-387-96098-8. MR 0804611\"\n",
    "\n",
    "[sergio2001]: google.com \"Sergio Bermejo, Joan Cabestany (2001) \"Oriented principal component analysis for large margin classifiers\", Neural Networks, 14 (10), 1447–1461.\"\n",
    "\n",
    "[MAE]: http://www.wikiwand.com/en/Mean_absolute_error \"wiki: Mean Absolute Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:  \n",
    "http://www.wikiwand.com/en/Mean_absolute_error"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
