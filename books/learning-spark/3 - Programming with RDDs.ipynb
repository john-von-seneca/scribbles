{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD: Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Github repo of the book](https://github.com/databricks/learning-spark)\n",
    "\n",
    "Clone URL: git@github.com:databricks/learning-spark.git\n",
    "\n",
    "path: ~/repos/databricks/learning-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An RDD in Spark is simply an immutable distributed collection of objects.  \n",
    "* Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "* RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Users create RDDs in two ways:\n",
    "  * by loading an external dataset, or \n",
    "  * by distributing a collection of objects (e.g., a list or set) in their driver program"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README.md\")\n",
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once created, RDDs offer two types of operations: transformations and actions.  \n",
    "\n",
    "* Transformations construct a new RDD from a previous one.\n",
    "  * For example, one common transformation is filtering data that matches a predicate.\n",
    "  * In our text file example, we can use this to create a new RDD holding just the strings that       contain the word Python"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_lines = lines.filter(lambda line: \"Python\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Actions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS). \n",
    "* One example of an action we called earlier is first() , which returns the first element in an RDD and is demonstrated in Example 3-3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first() action, Spark scans the file only until it finds the first matching line; it doesn’t even read the whole file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using \n",
    "\n",
    "```python\n",
    "RDD.persist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you will often use persist() to load a subset of your data into memory and query it repeatedly.  \n",
    "For example, if we knew that we wanted to compute multiple results about the README lines that contain Python, we could write the script as\n",
    "\n",
    "```python\n",
    "python_lines.persist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_lines.persist\n",
    "print(python_lines.count())\n",
    "print(python_lines.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*cache()* is the same as calling *persist()* with the default storage level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides two ways to create RDDs  \n",
    "  * loading an external dataset\n",
    "  * parallelizing a collection in your driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to create RDDs is to take an existing collection in your program and pass it to SparkContext’s parallelize() method, as shown below.  \n",
    "\n",
    "This approach is very useful when you are learning Spark, since you can quickly create your own RDDs in the shell and perform operations on them.   \n",
    "\n",
    "Keep in mind, however, that outside of prototyping and testing, this is not widely used since\n",
    "it requires that you have your entire dataset in memory on one machine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['pandas','i like pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common way is to load from external storage.  \n",
    "\n",
    "```python\n",
    "lines = sc.textFile(\"/path/to/README.md\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transformation: RDD to RDD  \n",
    "* Action: RDD to some other return type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed RDDs are computed lazily, only when used in an action"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_lines = \" warn: yes it is\\ninfo: this is info \\n error: this is error\\n warn: another warning\"\n",
    "with open(\"log.txt\",\"w\") as f: f.write(str_lines)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rdd = sc.textFile(\"log.txt\")\n",
    "errors_rdd = input_rdd.filter(lambda x: \"error\" in x)\n",
    "errors_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the filter() operation does not mutate the existing inputRDD.  \n",
    "Instead, it returns a pointer to an entirely new RDD. inputRDD can still be reused later in the\n",
    "program—for instance, to search for other words."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_rdd = input_rdd.filter(lambda x: \"error\" in x)\n",
    "warn_rdd = input_rdd.filter(lambda x: \"warn\" in x)\n",
    "bad_lines_rdd = errors_rdd.union(warn_rdd)\n",
    "print(\"type of errors_rdd:\", type(errors_rdd), id(errors_rdd))\n",
    "print(\"type of warn_rdd:\", type(warn_rdd), id(warn_rdd))\n",
    "print(\"type of bad_lines_rdd:\", type(bad_lines_rdd), id(bad_lines_rdd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note  \n",
    "A better way to accomplish the same result would be to simply filter the inputRDD once, looking for either error or warning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are the operations that return a final value to the driver program or write data to an external storage system. \n",
    "\n",
    "Actions force the evaluation of the transformations required for the RDD they were called on, since they need to actually produce output.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input had\", bad_lines_rdd.count(), \" concerning lines\")\n",
    "print(\"Here are 3 examples:\")\n",
    "for line in bad_lines_rdd.take(3):\n",
    "    print(line) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* take() is used to retrieve a small number of elements in the RDD at the driver program\n",
    "* collect() to retrieve the entire RDD\n",
    "  * hence collect() shouldn't be used on large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases RDDs can’t just be collect() ed to the driver because they are too large.  \n",
    "\n",
    "In these cases, it’s common to write data out to a distributed storage system such as HDFS or Amazon S3.   \n",
    "\n",
    "You can save the contents of an RDD using the \n",
    "* *saveAsTextFile()* action\n",
    "* *saveAsSequenceFile()* action\n",
    "* or any of a number of actions for various built-in formats\n",
    "\n",
    "We will cover the different options for exporting data in Chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark uses lazy evaluation to reduce the number of passes it has to take over our data by grouping operations together.   \n",
    "\n",
    "In systems like Hadoop MapReduce, developers often have to spend a lot of time considering how to group together operations to minimize the number of MapReduce passes.   \n",
    "\n",
    "\n",
    "In Spark, there is no substantial benefit to writing a single complex map instead of chaining together many simple operations. \n",
    "\n",
    "Thus, users are free to organize their program into smaller, more manageable operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing functions to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 ways to doing so in python  \n",
    "* as lambdas\n",
    "* as top-level functions\n",
    "* as locally defined functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rdd = input_rdd.filter(lambda line: \"error\" in lines)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_error(line):\n",
    "    return \"error\" in line\n",
    "\n",
    "error_rdd = input_rdd.filter(contains_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue to watch out for when passing functions is inadvertently serializing the object containing the function.  \n",
    "\n",
    "When you pass a function that is the member of an object, or contains references to fields in an object (e.g., self.field ), Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need. \n",
    "\n",
    "Sometimes this can also cause your program to fail, if your class contains objects that Python can’t figure out how to pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong way"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing a function with field references (don’t do this!)\n",
    "\n",
    "class SearchFunctions(object):\n",
    "    def __init__(self, query):\n",
    "        self.query = query\n",
    "        \n",
    "    def isMatch(self, s):\n",
    "        return self.query in s\n",
    "\n",
    "    def getMatchesFunctionReference(self, rdd):\n",
    "        # Problem: references all of \"self\" in \"self.isMatch\"\n",
    "        return rdd.filter(self.isMatch)\n",
    "\n",
    "    def getMatchesMemberReference(self, rdd):\n",
    "        # Problem: references all of \"self\" in \"self.query\"\n",
    "        return rdd.filter(lambda x: self.query in x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, just extract the fields you need from your object into a local variable and pass that in.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordFunctions(object):\n",
    "    ##\n",
    "    #\n",
    "    \n",
    "    def getMatchesNoReference(self, rdd):\n",
    "        # Safe: extract only the field we need into a local variable\n",
    "        query = self.query\n",
    "        return rdd.filter(lambda x: query in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know it is weird, but what can you do about it. Shove it up and live with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map, filter\n",
    "\n",
    "It is useful to note that map() ’s return type does not have to be the same as its input type.  \n",
    "\n",
    "So if we had an RDD String and our map() function were to parse the strings and return a Double, our input RDD type would be RDD[String] and the resulting RDD type would be RDD[Double] ."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize(range(1,5))\n",
    "print(\"type of nums:\", type(nums))\n",
    "squared_rdd = nums.map(lambda x: x*x)\n",
    "print(\"type of rdd:\", type(squared_rdd))\n",
    "for num in squared_rdd.collect():\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatmap"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize([\"hello world\", \"hi\"])\n",
    "\n",
    "words = lines.map(lambda line: line.split(\" \"))\n",
    "print(\"using map:\", words.first())\n",
    "\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(\"using flatmap:\", words.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to\n",
    "```ruby\n",
    "lines.map {|line| line.split(\" \")}.flatten.first()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo set operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct, union, intersection, subtract(aka setminus)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_1 = sc.parallelize([\"coffee\", \"coffee\", \"panda\", \"monkey\", \"tea\"])\n",
    "rdd_2 = sc.parallelize([\"coffee\", \"monkey\", \"kitty\"])\n",
    "\n",
    "print(\"rdd_1.distinct():\", rdd_1.distinct().collect())\n",
    "print(\"rdd_1.union(rdd_2):\", rdd_1.union(rdd_2).collect())\n",
    "print(\"rdd_1.intersection(rdd_2):\", rdd_1.intersection(rdd_2).collect())\n",
    "print(\"rdd_1.subtract(rdd_2):\", rdd_1.subtract(rdd_2).collect())\n",
    "print(\"rdd_1.cartesian(rdd_2):\", rdd_1.cartesian(rdd_2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that distinct() is expensive, however, as it requires shuffling all the data over the network to ensure that we receive only one copy of each element.   \n",
    "\n",
    "Shuffling, and how to avoid it, is discussed in more detail in Chapter 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While intersection() and union() are two similar concepts, the performance of intersection() is much worse since it requires a shuffle over the network to identify common elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*subtract(other)*, like intersection() , performs a shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be warned, however, that the Cartesian product is very expensive for large RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_3 = sc.parallelize(range(1,11))\n",
    "print('sum:', rdd_3.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "print('sum:', rdd_3.fold(0, operator.add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold (not clear)\n",
    "\n",
    "You can minimize object creation in fold() by modifying and returning the first of the two parameters in place. However, you should not modify the second parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[reduce vs fold](http://stackoverflow.com/questions/25158780/difference-between-reduce-and-foldleft-fold-in-functional-programming-particula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate() function frees us from the constraint of having the return be the same type as the RDD we are working on.  \n",
    "\n",
    "With aggregate(), like fold(), we supply an initial zero value of the type we want to return. \n",
    "\n",
    "We then supply a function to combine the elements from our RDD with the accumulator.  \n",
    "\n",
    "Finally, we need to supply a second function to merge two accumulators, given that each node accumulates its own results locally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize(range(20))\n",
    "sum_count = nums.aggregate((0,0),\n",
    "                           (lambda acc,val: (acc[0]+val, acc[1]+1)),\n",
    "                           (lambda acc1,acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1]))\n",
    "                          )\n",
    "print(sum_count)\n",
    "print(sum_count[0] / float(sum_count[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect\n",
    "\n",
    "returns the entire RDD's contents.   \n",
    "all the data must fit on a single m/c else fuck off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take(n)\n",
    "\n",
    "return n elements  \n",
    "attempts to minimize the number of partitions it accesses.  \n",
    "hence, it may represent a biased collection.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top(n)\n",
    "\n",
    "returns the top n elements based on a (Default) comparison function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "nums = range(20)\n",
    "random.shuffle(nums)\n",
    "print(nums)\n",
    "\n",
    "nums = sc.parallelize(nums)\n",
    "print(nums.top(0))\n",
    "print(nums.top(1))\n",
    "print(nums.top(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeSample\n",
    "\n",
    "Sometimes we need a sample of our data in our driver program.   \n",
    "\n",
    "The takeSample(withReplacement, num, seed) function allows us to take a sample of our data\n",
    "either with or without replacement.  \n",
    "\n",
    "Sometimes it is useful to perform an action on all of the elements in the RDD, but"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach\n",
    "\n",
    "Sometimes it is useful to perform an action on all of the elements in the RDD, but without returning any result to the driver program.   \n",
    "\n",
    "A good example of this would be posting JSON to a webserver or inserting records into a database. \n",
    "\n",
    "In either case, the foreach() action lets us perform computations on each element in the RDD without bringing it back locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count, countByValue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_1 = sc.parallelize([\"coffee\", \"coffee\", \"panda\", \"monkey\", \"tea\"])\n",
    "\n",
    "print('rdd_1.count():', rdd_1.count())\n",
    "print('rdd_1.countByValue():', rdd_1.countByValue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2 = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print('rdd_2.countByKey():', rdd_2.countByKey())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "?rdd_1.countByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeOrdered\n",
    "\n",
    "takeOrdered(num)(ordering) \n",
    "\n",
    "Return num elements based on\n",
    "provided ordering\n",
    "\n",
    "```python\n",
    "rdd.takeOrdered(2)\n",
    "(myOrdering) \n",
    "```\n",
    "\n",
    "```python\n",
    "{3, 3}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting between RDD Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence (Caching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, Spark RDDs are lazily evaluated, and sometimes we may wish to\n",
    "use the same RDD multiple times.  \n",
    "\n",
    "If we do this naively, Spark will recompute the RDD and all of its dependencies each time we call an action on the RDD.\n",
    "\n",
    "This can be especially expensive for iterative algorithms, which look at the data many times."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(20)).map(lambda x: str(x**2))\n",
    "print(rdd.count())\n",
    "print(\",\".join(rdd.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Space used CPU time In memory On disk Comments\n",
    "MEMORY_ONLY High Low Y N  Low High Y N  High Medium Some Some Spills to disk if there is too much data to fit in\n",
    "memory.\n",
    " Low High Some Some Spills to disk if there is too much data to fit in\n",
    "memory. Stores serialized representation in\n",
    "memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Level  | Space Use | CPU Time | In memory | On Disk | Comments |\n",
    "|---|---|---|---|---|---|\n",
    "| MEMORY_ONLY | High  | Low  | Y  | N  | |\n",
    "|  MEMORY_ONLY_SER | Low  | High  | Y  | N  | |\n",
    "| MEMORY_AND_DISK  | High  | Medium  | Some  | Some  | Spills to disk if too much d in mem|\n",
    "| MEMORY_AND_DISK_SER | Low | High | SOme | Some | Spills to disk if too much d in mem |\n",
    "| DISK_ONLY | Low | High | N | Y | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "val result = input.map(x => x * x)\n",
    "result.persist(StorageLevel.DISK_ONLY)\n",
    "println(result.count())\n",
    "println(result.collect().mkString(\",\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "\n",
    "* Notice that we called persist() on the RDD before the first action.   \n",
    "* The persist() call on its own doesn’t force evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you attempt to cache too much data to fit in memory, Spark will automatically evict old partitions using a Least Recently Used (LRU) cache policy. \n",
    "\n",
    "For the memory-only storage levels, it will recompute these partitions the next time they are accessed.\n",
    "\n",
    "While for the memory-and-disk ones, it will write them out to disk. \n",
    "\n",
    "In either case, this means that you don’t have to worry about your job breaking if you ask Spark to cache too much data. \n",
    "\n",
    "However, caching unnecessary data can lead to eviction of useful data and more recomputation time.\n",
    "\n",
    "Finally, RDDs come with a method called unpersist() that lets you manually remove them from the cache."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.0.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
