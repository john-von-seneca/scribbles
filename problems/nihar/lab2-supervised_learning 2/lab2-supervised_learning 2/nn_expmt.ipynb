{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">2. Single layer network </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a single neuron to discriminate between the apples and oranges data in apples.npy and oranges.npy. Try this for both a linear neuron and one with a sigmoidal output nonlinearity. (Use $+1/-1$ as the category assignments in the linear case, and $1/0$ in the non-linear case.) Use the code below to visualize the convergence of the solution during learning. You must fill in the code for simulating network itself and learning of the weights. Comment on the differences you observe between the sigmoid and linear case."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.lab2_utils import HyperPlanePlotter\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "apples  = np.load('data/apples.npy')\n",
    "oranges = np.load('data/oranges.npy')\n",
    "\n",
    "# initialize data array\n",
    "data = np.hstack((apples,oranges))\n",
    "dimensions, numSamples = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize teachers\n",
    "halfNumSamples = int(numSamples/2)\n",
    "teacherLinear = np.ones(numSamples)\n",
    "teacherLinear[halfNumSamples:] *= -1\n",
    "teacherSigmoid = np.ones(numSamples)\n",
    "teacherSigmoid[halfNumSamples:] *= 0\n",
    "\n",
    "# number of trials - ## Modify these so your learning converges\n",
    "numTrials = 2000\n",
    "\n",
    "# learning rates - ## Modify these so your learning converges by the end\n",
    "etaLinear  = 1e-2\n",
    "etaSigmoid = 8e-2\n",
    "\n",
    "# intialize plotter\n",
    "plotter = HyperPlanePlotter(data, apples, oranges, numTrials)\n",
    "plotEvery = numTrials // 10"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    return 1/(1+np.e**-u)\n",
    "\n",
    "def sigmoidDeriv(u):\n",
    "    return sigmoid(u)*(1-sigmoid(u))\n",
    "\n",
    "def identity(u): # For the linear case\n",
    "    return u\n",
    "\n",
    "def identityDeriv(u): # For the linear case\n",
    "    return 1\n",
    "\n",
    "def get_parameters(name):\n",
    "    if name == \"Linear\":\n",
    "        return identity, identityDeriv, teacherLinear, etaLinear\n",
    "    return sigmoid, sigmoidDeriv, teacherSigmoid, etaSigmoid"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeSingle(name):\n",
    "    func, funcDeriv, teacher, eta = get_parameters(name)\n",
    "    \n",
    "    # initialize weights and bias\n",
    "    weights = np.random.randn(2,1)\n",
    "    bias    = np.random.randn(1,1)\n",
    "    # initialize plots\n",
    "    plotter.setupPlotProb2(name, weights, bias)\n",
    "\n",
    "    # loop over trials\n",
    "    for t in range(numTrials):\n",
    "        errorT = 0\n",
    "        # initialize weight and bias derivatives\n",
    "        grad_W=0\n",
    "        grad_b=0\n",
    "        \n",
    "        # loop over training set\n",
    "        for i in range(numSamples):\n",
    "            # compute neuron output\n",
    "            x=data[:,i:i+1]\n",
    "            input_to_neuron=bias+np.dot(weights.T,x)\n",
    "            y=func(input_to_neuron)\n",
    "            # compute error \n",
    "            error=0.5* (y-teacher[i])**2\n",
    "            # accumulate weight derivative using func & funcDeriv\n",
    "            grad_W += (funcDeriv(input_to_neuron) * (teacher[i]-y) * x)\n",
    "            # accumulate bias derivative func & funcDeriv\n",
    "            grad_b+= funcDeriv(input_to_neuron)*(teacher[i]-y)\n",
    "            # accumulate the error according the objective function into errorT\n",
    "            errorT+=error\n",
    "\n",
    "        # update weights and bias\n",
    "        weights+= (1/numSamples)*eta*grad_W\n",
    "        bias+=(1/numSamples)*eta*grad_b\n",
    "\n",
    "        # update display of separating hyperplane at plotEvery intervals\n",
    "        if t % plotEvery == 0:\n",
    "            plotter.updatePlotProb2(weights, bias)\n",
    "        plotter.plotErrorProb2(name, t, errorT)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.initPlotProb2()\n",
    "optimizeSingle(\"Linear\")\n",
    "optimizeSingle(\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TEXT HERE - Comment on the differences between the sigmoid and linear case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">3. Multilayer network </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the data from question 2 with the additional datasets apples2.npy and oranges2.npy. As you can see from plotting out the combined data, the problem of discriminating the apples from the oranges is no longer linearly separable, so we must use a multilayer network for this problem. Start by deriving the learning rules for a two layer network. Then, train a two-layer network (using backprop) to learn to discriminate between apples and oranges. Use the code below to get started. Experiment with adding a momentum term to see if it helps with convergence.\n",
    "\n",
    "To make sure your solution works, we have provided you with a good initialization of the weights (goodInit=True). After you get this solution working you should experiment with random initializations (goodInit=False). In the description of your solution you should comment on the following:\n",
    "\n",
    "a) From your learned solution, describe in words how the two layers work together to discriminate between apples and oranges. <br/>\n",
    "b) The effect momentum has on the learning <br/>\n",
    "c) The solutions learned when goodInit=False and why they happen <br/>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.lab2_utils import HyperPlanePlotter\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally load 'data/apples2.mat' and 'data/oranges2.mat'\n",
    "apples  = np.load('data/apples.npy')\n",
    "oranges = np.load('data/oranges.npy')\n",
    "apples2 = np.load('data/apples2.npy')                                                                                                                                      \n",
    "oranges2 = np.load('data/oranges2.npy')\n",
    "\n",
    "# initialize data array\n",
    "apples = np.hstack((apples, apples2))\n",
    "oranges = np.hstack((oranges, oranges2))\n",
    "data = np.hstack((apples, oranges))\n",
    "dimensions,numSamples = data.shape\n",
    "halfNumSamples = int(numSamples/2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize teacher\n",
    "teacher = np.ones(numSamples)\n",
    "teacher[halfNumSamples:] *= 0\n",
    "\n",
    "# learning rate\n",
    "eta=1e-2\n",
    "\n",
    "# number of trials - you may want to make this smaller or larger\n",
    "numTrials = 4000\n",
    "\n",
    "# plotting\n",
    "plotter = HyperPlanePlotter(data, apples, oranges, numTrials, halfNumSamples)\n",
    "plotEvery  = numTrials // 50\n",
    "plotErrorEvery = numTrials // 100"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    return 1/(1+np.e**-u)\n",
    "\n",
    "def sigmoidDeriv(u):\n",
    "    return sigmoid(u)*(1-sigmoid(u))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeMulti(goodInit=True, momentum=False):\n",
    "    # initialize weights and biases\n",
    "    weightsOne = np.load('init/weightsOne.npy') if goodInit else np.random.randn(2,2) # first layer weights                                                                                                                                        \n",
    "    biasOne    = np.load('init/biasOne.npy') if goodInit else np.random.randn(2,1)                                                                                                                                                                 \n",
    "    weightsTwo = np.load('init/weightsTwo.npy') if goodInit else np.random.randn(2,1) # second layer weights                                                                                                                                       \n",
    "    biasTwo    = np.load('init/biasTwo.npy') if goodInit else np.random.randn(1)                                                                                                                                                                   \n",
    "    #biasTwo = np.random.randn(1,1)\n",
    "    biasTwo = biasTwo.reshape(1,1)\n",
    "    # setup plots\n",
    "    plotter.setupPlotProb3(weightsOne, biasOne, weightsTwo, biasTwo)\n",
    "\n",
    "    # initialize variables for momentum\n",
    "    ## YOUR CODE HERE\n",
    "    m=1e-2\n",
    "    \n",
    "    grad_W_One_prev=0\n",
    "    grad_W_Two_prev=0\n",
    "\n",
    "    # loop over trials\n",
    "    for t in range(numTrials):\n",
    "        errorT = 0 \n",
    "        # initialize derivative of weights, biases\n",
    "        ## YOUR CODE HERE\n",
    "        grad_W_One=0\n",
    "        grad_b_One=0\n",
    "        grad_W_Two=0\n",
    "        grad_b_Two=0\n",
    "        \n",
    "        # loop over training set\n",
    "        for i in range(numSamples):\n",
    "            # forward pass - compute y layer\n",
    "            x=data[:,i:i+1] #2*1\n",
    "            assert(x.shape == (2,1))\n",
    "            \n",
    "            input_to_l1= biasOne + weightsOne.T @ x #2*1\n",
    "            assert(input_to_l1.shape == (2,1))\n",
    "            \n",
    "            y=sigmoid(input_to_l1) #2*1\n",
    "            assert(y.shape == (2,1))\n",
    "            \n",
    "            # forward pass - compute z layer\n",
    "            input_to_l2=biasTwo+ weightsTwo.T @ y  #1*1\n",
    "            assert(input_to_l2.shape == (1,1))\n",
    "            z=sigmoid(input_to_l2) #1*1\n",
    "            assert(z.shape == (1,1))\n",
    "            \n",
    "            # compute error\n",
    "            error=0.5*(teacher[i]-z)**2\n",
    "            # accumulate second layer derivatives\n",
    "            delta_two = (teacher[i]-z) * sigmoidDeriv(input_to_l2) \n",
    "            \n",
    "            grad_W_Two += delta_two * y #2*1\n",
    "            assert(grad_W_Two.shape == (2,1))\n",
    "            \n",
    "            grad_b_Two += delta_two #1*1\n",
    "            assert(grad_b_Two.shape == (1,1))\n",
    "                        \n",
    "            \n",
    "            # accumulate first layer derivatives\n",
    "            ##Needs checking\n",
    "            delta_one = sigmoidDeriv(input_to_l1) * weightsTwo * delta_two\n",
    "            assert(delta_one.shape == (2,1))\n",
    "            \n",
    "            grad_W_One += x @ delta_one.T\n",
    "            assert(grad_W_One.shape == (2,2))\n",
    "            grad_b_One += delta_one\n",
    "            \n",
    "            # accumulate the error according the objective function into errorT\n",
    "            errorT += error\n",
    "                \n",
    "        # update weights and bias\n",
    "        print(grad_W_One.shape)\n",
    "        print(weightsOne.shape)\n",
    "        m_current = m if momentum else 0\n",
    "        weightsOne += (1/numSamples) * (eta * grad_W_One + m_current*grad_W_One_prev)\n",
    "        biasOne += (1/numSamples) * eta * grad_b_One\n",
    "        weightsTwo += (1/numSamples) * (eta * grad_W_Two+ m_current*grad_W_Two_prev)\n",
    "        biasTwo += (1/numSamples) * eta * grad_b_Two\n",
    "        \n",
    "        \n",
    "        # track previous weight derivatives to use momentum\n",
    "        ## YOUR CODE HERE\n",
    "        if momentum:\n",
    "            grad_W_One_prev=grad_W_One\n",
    "            grad_W_Two_prev=grad_W_Two\n",
    "\n",
    "        # update display of separating hyperplane at plotEvery intervals\n",
    "        if t % plotEvery == 0:\n",
    "            plotter.updatePlotProb3(weightsOne, biasOne, weightsTwo, biasTwo)\n",
    "        if t % plotErrorEvery == 0:\n",
    "            plotter.plotErrorProb3(t, errorT)\n",
    "eta=3.\n",
    "numTrials = 4000\n",
    "optimizeMulti(goodInit=True, momentum=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizeMulti(goodInit=False, momentum=True) # Run this many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">4. Pattern Discrimination Task </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following pattern discrimination task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/lab2.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will train a two-layer network to discriminate between these two patterns. First, make a hypothesis about what representation the first layer will learn in order to allow the second layer to discriminate between these two patterns. Then, train a two-layer neural network to discriminate between these patterns. How many hidden units are needed? What representation is learned by the hidden units in order to solve this problem? Comment on the differences between how you thought to discriminate between the patterns and how the network learned to."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.lab2_utils import FilterPlotter\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data array\n",
    "S = np.load('data/S.npy')\n",
    "T = np.load('data/T.npy')\n",
    "data = np.hstack((S,T))\n",
    "numInputUnits,numSamples = data.shape\n",
    "halfNumSamples = int(numSamples/2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize teacher\n",
    "teacher = np.ones(numSamples)\n",
    "teacher[halfNumSamples:] *= 0\n",
    "\n",
    "# learning rate\n",
    "eta=4e-1\n",
    "\n",
    "# number of trials - you may want to make this smaller or larger\n",
    "numTrials = 2000\n",
    "\n",
    "# plotting\n",
    "plotter = FilterPlotter(numTrials)\n",
    "plotHiddenUnitsEvery  = numTrials // 20\n",
    "plotErrorEvery = numTrials // 50"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(u):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def sigmoidDeriv(u):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeMultiPattern(numHiddenUnits, momentum=False):\n",
    "    # initialize weights and biases\n",
    "    weightsOne = np.random.randn(numInputUnits, numHiddenUnits) # first layer weights                                                                                                                                        \n",
    "    biasOne    = np.random.randn(numHiddenUnits,1)                                                                                                                                                                 \n",
    "    weightsTwo = np.random.randn(numHiddenUnits,1) # second layer weights                                                                                                                                       \n",
    "    biasTwo    = np.random.randn(1)                                                                                                                                                                \n",
    "    \n",
    "    plotter.setupPlots(weightsOne, numHiddenUnits)\n",
    "        \n",
    "    # initialize variables for momentum\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    weightsOneDerivLast = 0; biasOneDerivLast = 0; weightsTwoDerivLast = 0; biasTwoDerivLast = 0\n",
    "    # loop over trials\n",
    "    for t in range(numTrials):\n",
    "        # initialize derivative of weights, biases, and error array for each trial                                                                                                                                                                    \n",
    "        errorT = 0  \n",
    "        # loop over training set\n",
    "        for i in range(numSamples):\n",
    "            # forward pass\n",
    "            # compute error\n",
    "            # second layer derivatives \n",
    "            # first layer derivatives              \n",
    "            # accumulate the error according the objective function into errorT\n",
    "            pass ## YOUR CODE HERE\n",
    "            \n",
    "        # update weights and bias\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "        # track previous weight derivatives to use momentum\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "        # update display after plot*Every intervals\n",
    "        if t % plotHiddenUnitsEvery == 0:\n",
    "            plotter.updatePlots(weightsOne)\n",
    "        if t % plotErrorEvery == 0:\n",
    "            plotter.plotError(t, errorT)\n",
    "    print (\"Final Error: %.2f\" % errorT)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizeMultiPattern(numHiddenUnits=1, momentum=True) ## YOUR CODE HERE - Try different numbers of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizeMulti(goodInit=False, momentum=True) # Run this many times"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
